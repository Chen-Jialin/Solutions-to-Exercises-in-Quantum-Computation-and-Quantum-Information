% !Tex program = pdflatex
% Chapter 2 - Introduction to quantum mechanics
\ifx\allfiles\undefined
\documentclass[en]{sol-man}
\begin{document}
\fi
\chapter{Introduction to quantum mechanics}

\section{Linear algebra}

\begin{exe}[Linear dependence: example]
    Show that $(1,-1)$, $(1,2)$ and $(2,1)$ are Linearly dependent.
\end{exe}
\begin{pf}
    Since
    \begin{align}
        (1,-1)+(1,2)-(2,1)=0,
    \end{align}
    these three vectors are linearly dependent.
\end{pf}

\begin{exe}[Matrix representations: example]
    Suppose $V$ is a vector space with basis vectors $\lvert 0\rangle$ and $\lvert 1\rangle$, and $A$ is a linear operator from $V$ to $V$ such that $A\lvert 0\rangle=\lvert 1\rangle$ and $A\lvert 1\rangle=\lvert 0\rangle$. Give a matrix representation for $A$, with respect to the input basis $\lvert 0\rangle$, $\lvert 1\rangle$, and the output basis $\lvert 0\rangle$ and $\lvert 1\rangle$. Find input and output bases which give rise to a different matrix representation of $A$.
\end{exe}
\begin{sol}
    The matrix representation for $A$ with respect to the input basis $\lvert 0\rangle$, $\lvert 1\rangle$ and the output basis $\lvert 0\rangle$ and $\lvert 1\rangle$ is
    \begin{align}
        \left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right].
    \end{align}

    Keep $\lvert 0\rangle$ and $\lvert 1\rangle$ as the input basis and choose $\lvert+\rangle=(\lvert 0\rangle+\lvert 1\rangle)/\sqrt{2}$ and $\lvert-\rangle=(\lvert 0\rangle-\lvert 1\rangle)/\sqrt{2}$ as the output basis, then $A$ can be regarded as a linear operator from $V$ to $V$ such that $A\lvert 0\rangle=(\lvert+\rangle-\lvert-\rangle)/\sqrt{2}$ and $A\lvert 1\rangle=(\lvert 0\rangle+\lvert 1\rangle)/\sqrt{2}$. In this way, the matrix representation for $A$ is
    \begin{align}
        \frac{1}{\sqrt{2}}\left[\begin{matrix}
            1&-1\\
            1&1
        \end{matrix}\right].
    \end{align}
\end{sol}

\begin{exe}[Matrix representation for operator products]
    Suppose $A$ is a linear operator from vector space $V$ to vector space $W$, and $B$ is a linear operator from vector spaces $W$ to vector space $X$. Let $\lvert v_i\rangle$, $\lvert w_j\rangle$, and $\lvert x_k\rangle$ be bases for the vector spaces $V$, $W$, and $X$, respectively. Show that the matrix representation for the linear transformation $BA$ is the matrix product of the matrix representations for $B$ and $A$, with respect to the appropriate bases.
\end{exe}
\begin{pf}
    Suppose the dimension of vector spaces $V$, $W$, and $W$ are $l$, $m$, $n$, respectively. Since $A$ is a linear operator from vector space $V$ to vector space $W$, for each $i$ in the range $1,\cdots,l$, there exist complex numbers $A_{1i}$ through $A_{mi}$ such that
    \begin{align}
        A\lvert v_i\rangle=\sum_jA_{ji}\lvert w_j\rangle,
    \end{align}
    where $A_{ji}$ is the entries of the matrix representation of the operator $A$.
    Since $B$ is a linear operator from vector space $W$ to vector space $X$, for each $j$ in the range $1,\cdots,m$, there exist complex numbers $A_{1j}$ through $A_{nj}$ such that
    \begin{align}
        B\lvert w_j\rangle=\sum_kB_{kj}\lvert x_k\rangle,
    \end{align}
    where $B_{kj}$ is the entries of the matrix representation of the operator $B$.
    Putting the above two equations together, we have
    \begin{align}
        BA\lvert v_i\rangle=B\sum_jA_{ji}\lvert w_j\rangle=\sum_jA_{ji}\sum_kB_{kj}\lvert x_k\rangle=\sum_k\left(\sum_jB_{kj}A_{ji}\right)\lvert x_k\rangle=\sum_k(BA)_{ki}\lvert x_k\rangle.
    \end{align}
    where $(AB)_{ki}$ is the entries of the matrix representation of the operator $BA$. Therefore, the matrix representation for the linear transformation $BA$ is the matrix product of the matrix representations for $B$ and $A$, with respect to the appropriate bases.
\end{pf}

\begin{exe}[Matrix representation for identity]
    Show that the identity operator on a vector space $V$ has a matrix representation which is one along the diagonal and zero everywhere else, if the matrix representation is taken with respect to the same input and output bases. The matrix is known as the \emph{identity matrix}.
\end{exe}
\begin{pf}
    Suppose $I$ is the identity operator on the vector space $V$ and choose $\lvert v_1\rangle,\cdots,\lvert v_m\rangle$ as both the input basis and the output basis for $V$. Then for each $j$ in the range $1,\cdots,m$, there exist complex numbers $I_{1j}$ through $I_{mj}$ such that
    \begin{gather}
        I\lvert v_j\rangle=\sum_iA_{ij}\lvert v_i\rangle=\lvert v_j\rangle,\\
        \Longrightarrow(A_{jj}-1)\lvert v_j\rangle+\sum_{i\neq j}A_{ij}\lvert v_i\rangle=0,
    \end{gather}
    where $A_{ij}$ is the entries of the matrix representation of the operator $I$. Due to linear independence of the basis, there must be
    \begin{align}
        v_{ij}=\left\{\begin{array}{ll}
            1,&i=j;\\
            0,&i\neq j,
        \end{array}\right.=\delta_{ij},\quad\forall i,j=1,\cdots,m.
    \end{align}
    i.e., the identity operator on the vector space $V$ has a matrix representation which is one along the diagonal and zero everywhere else.
\end{pf}

\begin{exe}
    Verify that $(\cdot,\cdot)$ just defined is an inner product on $\mathbb{C}^n$.
\end{exe}
\begin{pf}
    $(\cdot,\cdot)$ just defined satisfies the requirements that:
    \begin{itemize}
        \item[(1)] $(\cdot,\cdot)$ is linear in the second argument,
        \begin{align}
            \notag\left((y_1,\cdots,y_n),\sum_j\lambda_j(z_1^{(j)},\cdots,z_n^{(j)})\right)=&\left((y_1,\cdots,y_n),(\sum_j\lambda_jz_1^{(j)},\cdots,\sum_j\lambda_jz_n^{(j)})\right)=\sum_iy_i^*\sum_j\lambda_jz_i^{(j)}\\
            =&\sum_j\lambda_j\sum_iy_i^*z_i^{(j)}=\sum_j\lambda_j((y_1,\cdots,y_n),(z_1^{(j)},\cdots,z_n^{(j)})).
        \end{align}
        \item[(2)] 
        \begin{align}
            ((y_1,\cdots,y_n),(z_1,\cdots,z_n))=\sum_iy_i^*z_i=\left(\sum_iz_i^*y_i\right)^*=((z_1,\cdots,z_n),(y_1,\cdots,y_n))^*.
        \end{align}
        \item[(3)] 
        \begin{align}
            ((y_1,\cdots,y_n),(y_1,\cdots,y_n))=\sum_iy_i^*y_i=\sum_i\abs{y_i}^2\geq 0,
        \end{align}
        with equality if and only if $(y_1,\cdots,y_n)=0$.
    \end{itemize}
    Therefore, $(\cdot,\cdot)$ is an inner product on $\mathbb{C}^n$.
\end{pf}

\begin{exe}
    Show that any inner product $(\cdot,\cdot)$ is conjugate-linear in the first argument,
    \begin{align}
        \left(\sum_i\lambda_i\lvert w_i\rangle,\lvert v\rangle\right)=\sum_i\lambda_i^*(\lvert w_i\rangle,\lvert v\rangle).
    \end{align}
\end{exe}
\begin{pf}
    \begin{align}
        \left(\sum_i\lambda_i\lvert w_i\rangle,\lvert v\rangle\right)=\left(\lvert v\rangle,\sum_i\lambda_i\lvert w_i\rangle\right)^*=\left(\sum_i\lambda_i(\lvert v\rangle,\lvert w_i\rangle)\right)^*=\sum_i\lambda_i^*(\lvert w_i\rangle,\lvert v\rangle).
    \end{align}
    Therefore, any inner product $(\cdot,\cdot)$ is conjugate-linear in the first argument.
\end{pf}

\begin{exe}
    Verify that $\lvert w\rangle\equiv(1,1)$ and $\lvert v\rangle\equiv(1,-1)$ are orthogonal. What are the normalized forms of these vectors?
\end{exe}
\begin{sol}
    Since
    \begin{align}
        (\lvert w\rangle,\lvert v\rangle)=1\times 1+1\times(-1)=0,
    \end{align}
    these two vectors are orthogonal. The normalized form of $\lvert w\rangle$ and $\lvert v\rangle$ are
    \begin{align}
        \frac{\lvert w\rangle}{\norm{\lvert w\rangle}}=\frac{1}{\sqrt{2}}(1,1),
    \end{align}
    and
    \begin{align}
        \frac{\lvert v\rangle}{\norm{\lvert v\rangle}}=\frac{1}{\sqrt{2}}(1,-1),
    \end{align}
    respectively.
\end{sol}

\begin{exe}
    Prove that the Gram-Schmidt procedure produces an orthonormal basis for $V$.
\end{exe}
\begin{pf}
    Obviously, $\lvert v_1\rangle,\cdots,\lvert v_n\rangle$ are normalized, so we first prove that $\lvert v_1\rangle,\cdots,\lvert v_d\rangle$ are orthogonal with induction.
    For $k=1$,
    \begin{align}
        (\lvert v_1\rangle,\lvert v_{k+1}\rangle)=&(\lvert v_1,\lvert v_2\rangle\rangle)=\left(\lvert v_1\rangle,\frac{\lvert w_2\rangle-\langle v_1\vert w_2\rangle\lvert v_1\rangle}{\norm{\lvert v_1\rangle-\langle v_1\vert w_2\rangle\lvert v_1\rangle}}\right)=\frac{\langle v_1\vert w_2\rangle-\langle v_1\vert w_2\rangle\langle v_1\vert v_1\rangle}{\norm{\lvert v_1\rangle-\langle v_1\vert w_2\rangle\lvert v_1\rangle}}=\frac{\langle v_1\vert w_2\rangle-\langle v_1\vert w_2\rangle}{\norm{\lvert v_1\rangle-\langle v_1\vert w_2\rangle\lvert v_1\rangle}}=0,\\
    \end{align}
    so $\lvert v_1\rangle,\cdots,\lvert v_{k+1}\rangle$ are orthogonal for $k=1$.
    For $k\geq 2$, if $\lvert v_1\rangle,\cdots,\lvert v_k\rangle$ are orthogonal, then
    \begin{align}
        \notag(\lvert v_j\rangle,\lvert v_{k+1}\rangle)=&\left(\lvert v_j\rangle,\frac{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}{\norm{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}}\right)=\frac{\langle v_j\vert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\langle v_j\vert v_i\rangle}{\norm{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}}\\
        =&\frac{\langle v_j\vert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\delta_{ji}}{\norm{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}}=\frac{\langle v_j\vert w_{k+1}\rangle-\langle v_j\vert w_{k+1}\rangle}{\norm{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}}=0,\quad\forall j=1,\cdots,k,
    \end{align}
    so $\lvert v_1\rangle,\cdots,\lvert v_{k+1}\rangle$ are orthogonal for $k\geq 2$, such as $k=d-1$.
    Till now, we proved that $\lvert v_1\rangle,\cdots,\lvert v_d\rangle$ are orthonormal.

    We then prove that $\lvert v_1\rangle,\cdots,\lvert v_n\rangle$ are a basis for $V$.
    As proved above, $\lvert v_1\rangle,\cdots,\lvert v_d\rangle$ are orthonormal, and thus linear independent. Since $\lvert w_1\rangle,\cdots,\lvert w_d\rangle$ are a basis for $V$. Any vector $\lvert v\rangle$ in $V$ can be written as a linear combination of $\lvert w_1\rangle,\cdots,\lvert w_d\rangle$:
    \begin{align}
        \lvert v\rangle=\sum_{i=1}^da_i\lvert w_i\rangle.
    \end{align}
    Using
    \begin{align}
        \lvert v_1\rangle=\frac{\lvert w_1\rangle}{\norm{\lvert w_1\rangle}}\Longrightarrow \lvert w_1\rangle=\norm{\lvert w_1\rangle}\lvert v_1\rangle,\\
    \end{align}
    and
    \begin{align}
        \notag\lvert v_{k+1}\rangle=&\frac{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}{\norm{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}}\\
        \Longrightarrow&\lvert w_{k+1}\rangle=\norm{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}\lvert v_{k+1}\rangle+\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle,\quad\forall k=1,\cdots,d-1
    \end{align}
    we can rewrite $\lvert v\rangle$ as a linear combination of $\lvert v_1\rangle,\cdots,\lvert v_d\rangle$:
    \begin{align}
        \lvert v\rangle=a_1\norm{\lvert w_1\rangle}\lvert v_1\rangle+\sum_{k=1}^{d-1}a_{k+1}\left(\norm{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}\lvert v_{k+1}\rangle+\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle\right),
    \end{align}
    so $\lvert v_1\rangle,\cdots,\lvert v_d\rangle$ span and form a basis for $V$.

    Therefore, the Gram-Schmidt procedure produces an orthonormal basis for $V$.
\end{pf}

\begin{exe}[Pauli operators and the outer product]
    The Pauli matrices (Figure 2.2 on page 65) can be considered as operators with respect to an orthonormal basis $\lvert 0\rangle$, $\lvert 1\rangle$ for a two-dimensional Hilbert space. Express each of the Pauli operators in the outer product notion.
\end{exe}
\begin{sol}
    The Pauli operators in the outer product notion:
    \begin{align}
        \sigma_0=&\sum_{m,n=1}^2\langle m\rvert\sigma_0\lvert n\rangle\lvert m\rangle\langle n\rvert=\lvert 0\rangle\langle 0\rvert+\lvert 1\rangle\langle 1\rvert,\\
        \sigma_1=&\sum_{m,n=1}^2\langle m\rvert\sigma_1\lvert n\rangle\lvert m\rangle\langle n\rvert=\lvert 0\rangle\langle 1\rvert+\lvert 1\rangle\langle 0\rvert,\\
        \sigma_2=&\sum_{m,n=1}^2\langle m\rvert\sigma_2\lvert n\rangle\lvert m\rangle\langle n\rvert=-i\lvert 0\rangle\langle 1\rvert+i\lvert 1\rangle\langle 0\rvert,\\
        \sigma_3=&\sum_{m,n=1}^2\langle m\rvert\sigma_3\lvert n\rangle\lvert m\rangle\langle n\rvert=\lvert 0\rangle\langle 0\rvert-\lvert 1\rangle\langle 1\rvert.
    \end{align}
\end{sol}

\begin{exe}
    Suppose $\lvert v_i\rangle$ is an orthonormal basis for and inner product space $V$. What is the matrix representation for the operator $\lvert v_j\rangle\langle v_k\rvert$, with respect to the $\lvert v_i\rangle$ basis?
\end{exe}
\begin{sol}
    The matrix representation for the operator $\lvert v_j\rangle\langle v_k\rvert$ with respect to the $\lvert v_i\rangle$ basis:
    \begin{align}
        \lvert v_j\rangle\langle v_k\rvert=\begin{array}{cc}
            \begin{array}{ccccccc}
                \multicolumn{7}{c}{k\text{th column}} \\
                 &  &  & \downarrow &  &  & 
                \end{array} & \begin{array}{c}
                    \\
                    \\
                   \end{array} \\
            \left[\begin{array}{ccccccc}
                0 &  &  &  & \multicolumn{3}{c}{\multirow{3}{*}{\Huge0}} \\
                 & \ddots &  &  & \multicolumn{3}{c}{} \\
                 &  & 0 &  & \multicolumn{3}{c}{} \\
                 &  &  & 1 &  &  &  \\
                \multicolumn{3}{c}{\multirow{3}{*}{\Huge0}} &  & 0 &  &  \\
                \multicolumn{3}{c}{} &  &  & \ddots &  \\
                \multicolumn{3}{c}{} &  &  &  & 0
                \end{array}\right] & \begin{array}{c}
                    \\
                    \\
                    \\
                   \leftarrow\,j\text{th row} \\
                    \\
                    \\
                    \\
                   \end{array}
        \end{array}
    \end{align}
    (a matrix with all zeros expect a one at the $j$th row and $k$th column).
\end{sol}

\begin{exe}[Eigendecomposition of the Pauli matrices]
    Find the eigenvectors, eigenvalues, and diagonal representations of the Pauli matrices $X$, $Y$, and $Z$.
\end{exe}
\begin{sol}
    The characteristic equation of $X$
    \begin{align}
        \det\abs{X-\lambda I}=\abs{\begin{matrix}
            -\lambda&1\\
            1&-\lambda
        \end{matrix}}=\lambda^2-1=0,
    \end{align}
    gives the eigenvalues
    \begin{align}
        \lambda_1=1,\quad\lambda_2=-1.
    \end{align}
    The eigenequations of $X$
    \begin{align}
        X\lvert v_1\rangle=\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right]=\left[\begin{matrix}
            b_1\\
            a_1
        \end{matrix}\right]=\lambda_1\lvert v_1\rangle=\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right],\quad X\lvert v_2\rangle=\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]\left[\begin{matrix}
            a_2\\
            b_2
        \end{matrix}\right]=\left[\begin{matrix}
            b_2\\
            a_2
        \end{matrix}\right]=\lambda_2\lvert v_2\rangle=\left[\begin{matrix}
            -a_2\\
            -b_2
        \end{matrix}\right],
    \end{align}
    give the corresponding eigenvectors
    \begin{align}
        \lvert v_1\rangle=\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            1
        \end{matrix}\right]=\frac{1}{\sqrt{2}}(\lvert 0\rangle+\lvert 1\rangle),\quad\lvert v_2\rangle=\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            -1
        \end{matrix}\right]=\frac{1}{\sqrt{2}}(\lvert 0\rangle-\lvert 1\rangle).
    \end{align}
    The diagonal representation of $X$ is
    \begin{align}
        X=\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]=\frac{1}{\sqrt{2}}(\lvert 0\rangle+\lvert 1\rangle)\frac{1}{\sqrt{2}}(\langle 0\rvert+\langle 1\rvert)-\frac{1}{\sqrt{2}}(\lvert 0\rangle-\lvert 1\rangle)\frac{1}{\sqrt{2}}(\langle 0\rvert-\langle 1\rvert).
    \end{align}
    The characteristic equation of $Y$
    \begin{align}
        \det\abs{Y-\lambda I}=\abs{\begin{matrix}
            -\lambda&-i\\
            i&-\lambda
        \end{matrix}}=\lambda^2-1=0,
    \end{align}
    gives the eigenvalues
    \begin{align}
        \lambda_1=1,\quad\lambda_2=-1.
    \end{align}
    The eigenequations of $Y$
    \begin{align}
        Y\lvert v_1\rangle=\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right]=\left[\begin{matrix}
            -ib_1\\
            ia_1
        \end{matrix}\right]=\lambda_1\lvert v_1\rangle=\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right],\quad Y\lvert v_2\rangle=\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]\left[\begin{matrix}
            a_2\\
            b_2
        \end{matrix}\right]=\left[\begin{matrix}
            -ib_2\\
            ia_2
        \end{matrix}\right]=\lambda_2\lvert v_2\rangle=\left[\begin{matrix}
            -a_2\\
            -b_2
        \end{matrix}\right],
    \end{align}
    give the corresponding eigenvectors
    \begin{align}
        \lvert v_1\rangle=\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            i
        \end{matrix}\right]=\frac{1}{\sqrt{2}}(\lvert 0\rangle+i\lvert 1\rangle),\quad\lvert v_2\rangle=\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            -i
        \end{matrix}\right]=\frac{1}{\sqrt{2}}(\lvert 0\rangle-i\lvert 1\rangle).
    \end{align}
    The diagonal representation of $Y$ is
    \begin{align}
        Y=\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]=\frac{1}{\sqrt{2}}(\lvert 0\rangle-i\lvert 1\rangle)\frac{1}{\sqrt{2}}(\langle 0\rvert+i\langle 1\rvert)-\frac{1}{\sqrt{2}}(\lvert 0\rangle+i\lvert 1\rangle)\frac{1}{\sqrt{2}}(\langle 0\rvert-i\langle 1\rvert).
    \end{align}
    The characteristic equation of $Z$
    \begin{align}
        \det\abs{Z-\lambda I}=\abs{\begin{matrix}
            1-\lambda&0\\
            0&-1-\lambda
        \end{matrix}}=\lambda^2-1=0,
    \end{align}
    gives the eigenvalues
    \begin{align}
        \lambda_1=1,\quad\lambda_2=-1.
    \end{align}
    The eigenequations of $Z$
    \begin{align}
        Z\lvert v_1\rangle=\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right]=\left[\begin{matrix}
            a_1\\
            -b_1
        \end{matrix}\right]=\lambda_1\lvert v_1\rangle=\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right],\quad Z\lvert v_2\rangle=\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]\left[\begin{matrix}
            a_2\\
            -b_2
        \end{matrix}\right]=\lambda_2\lvert v_2\rangle=\left[\begin{matrix}
            -a_2\\
            -b_2
        \end{matrix}\right],
    \end{align}
    give the corresponding eigenvectors
    \begin{align}
        \lvert v_1\rangle=\left[\begin{matrix}
            1\\
            0
        \end{matrix}\right]=\lvert 0\rangle,\quad\lvert v_2\rangle=\left[\begin{matrix}
            0\\
            1
        \end{matrix}\right].
    \end{align}
    The diagonal representation of $Z$ is
    \begin{align}
        Z=\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]=\lvert 0\rangle\langle 0\rvert-\lvert 1\rangle\langle 1\rvert.
    \end{align}
\end{sol}

\begin{exe}
    Prove that the matrix
    \begin{align}
        \left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]
    \end{align}
    is not diagonalizable.
\end{exe}
\begin{pf}
    The matrix is not normal,
    \begin{align}
        \left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]\left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]^{\dagger}=\left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]\left[\begin{matrix}
            1&1\\
            0&1
        \end{matrix}\right]=\left[\begin{matrix}
            1&1\\
            1&2
        \end{matrix}\right]\neq\left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]^{\dagger}\left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]=\left[\begin{matrix}
            1&1\\
            0&1
        \end{matrix}\right]\left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]=\left[\begin{matrix}
            2&1\\
            1&1
        \end{matrix}\right],
    \end{align}
    so it is not diagonalizable.
\end{pf}

\begin{exe}
    If $\lvert w\rangle$ and $\lvert v\rangle$ are any two vectors, show that $(\lvert w\rangle\langle v\rvert)^{\dagger}=\lvert v\rangle\langle w\rvert$.
\end{exe}
\begin{pf}
    \begin{align}
        (\lvert w\rangle\langle v\rvert)^{\dagger}=\langle v\rvert^{\dagger}\lvert w\rangle^{\dagger}=\lvert v\rangle\langle w\rvert.
    \end{align}
\end{pf}

\begin{exe}[Anti-linearity of the adjoint]
    Show tha the adjoint operation is anti-linear,
    \begin{align}
        \left(\sum_ia_iA_i\right)^{\dagger}=\sum_ia_i^*A_i^{\dagger}.
    \end{align}
\end{exe}
\begin{pf}
    For two arbitrary vectors $\lvert v\rangle$ and $\lvert w\rangle$,
    \begin{align}
        \left(\left(\sum_ia_iA_i\right)^{\dagger}\lvert v\rangle,\lvert w\rangle\right)=\left(\lvert v\rangle,\sum_ia_iA_i\lvert w\rangle\right)=\sum_ia_i(\lvert v\rangle,A_i\lvert w\rangle)=\sum_ia_i(A_i^{\dagger}\lvert v\rangle,\lvert w\rangle)=\left(\sum_ia_i^*A_i^{\dagger}\lvert v\rangle,\lvert w\rangle\right).
    \end{align}
    Due to the arbitrariness of $\lvert v\rangle$ and $\lvert w\rangle$,
    \begin{align}
        \left(\sum_ia_iA_i\right)^{\dagger}=\sum_ia_i^*A_i^{\dagger}.
    \end{align}
\end{pf}

\begin{exe}
    Show that $(A^{\dagger})^{\dagger}=A$.
\end{exe}
\begin{pf}
    For two arbitrary vectors $\lvert v\rangle$ and $\lvert w\rangle$,
    \begin{align}
        ((A^{\dagger})^{\dagger}\lvert v\rangle,\lvert w\rangle)=(\lvert v\rangle,A^{\dagger}\lvert w\rangle)=(A^{\dagger}\lvert w\rangle,\lvert v\rangle)^*=(\lvert w\rangle,A\lvert v\rangle)^*=[(A\lvert v\rangle,\lvert w\rangle)^*]^*=(A\lvert v\rangle,\lvert w\rangle).
    \end{align}
    Due to the arbitrariness of $\lvert v\rangle$ and $\lvert w\rangle$,
    \begin{align}
        (A^{\dagger})^{\dagger}=A.
    \end{align}
\end{pf}

\begin{exe}
    Show that any projector $P$ satisfies the equation $P^2=P$.
\end{exe}
\begin{pf}
    For any orthonormal basis $\lvert 1\rangle,\cdots,\lvert k\rangle$ for $W$,
    \begin{align}
        P=\sum_{i=1}^k\lvert i\rangle\langle i\rvert,
    \end{align}
    and then
    \begin{align}
        P^2=\sum_{i=1}^k\lvert i\rangle\langle i\rvert\sum_{j=1}^k\lvert j\rangle\langle j\rvert=\sum_{i=1}^k\sum_{j=1}^k\lvert i\rangle\langle i\vert j\rangle\langle j\rvert=\sum_{i=1}^k\sum_{j=1}^k\lvert i\rangle\delta_{ij}\langle j\rvert=\sum_{i=1}^k\lvert i\rangle\langle i\rvert=P.
    \end{align}
\end{pf}

\begin{exe}
    Show that a normal matrix is Hermitian if and only if it has real eigenvalues.
\end{exe}
\begin{pf}
    \emph{Sufficiency}: If normal matrix $A$ has real eigenvalues $\lambda_1,\cdots,\lambda_n$ with corresponding eigenvectors $\lvert 1\rangle,\cdots,\lvert n\rangle$. It can be written as
    \begin{align}
        A=\sum_{i=1}^n\lambda_i\lvert i\rangle\langle i\rvert.
    \end{align}
    Since $\lambda_1,\cdots,\lambda_n$ are real,
    \begin{align}
        A^{\dagger}=\sum_{i=1}^n\lambda_i^{\dagger}\lvert i\rangle\langle i\rvert=\sum_{i=1}^n\lambda_i\lvert i\rangle\langle i\rvert=A.
    \end{align}
    Therefore, $A$ is Hermitian.

    \emph{Necessity}: Suppose normal and Hermitian matrix $A$ has eigenvalues $\lambda_1,\cdots,\lambda_n$ with corresponding eigenvectors $\lvert 1\rangle,\cdots,\lvert n\rangle$. For any $i=1,\cdots,n$,
    \begin{align}
        \notag\lambda_i=\lambda_i(\lvert i\rangle,\lvert i\rangle)=(\lvert i\rangle,\lambda_i\lvert i\rangle)=&(\lvert i\rangle,A\lvert i\rangle)\\
        =&(A^{\dagger}\lvert i\rangle,\lvert i\rangle)=(\lvert i\rangle,A^{\dagger}\lvert i\rangle)^*=((A^{\dagger})^{\dagger}\lvert i\rangle,\lvert i\rangle)=(A\lvert i\rangle,\lvert i\rangle)=(\lambda_i\lvert i\rangle,\lvert i\rangle)=\lambda_i^*(\lvert i\rangle,\lvert i\rangle)=\lambda_i^*.
    \end{align}
    Therefore, all the eigenvalues $\lambda_1,\cdots,\lambda_n$ are real.
\end{pf}

\begin{exe}
    Show that all eigenvalues of a unitary matrix has modulus $1$, that is can be written in the form $e^{i\theta}$ for some real $\theta$.
\end{exe}
\begin{pf}
    Suppose unitary matrix $A$ has eigenvalues $\lambda_1,\cdots,\lambda_n$ with corresponding eigenvectors $\lvert 1\rangle,\cdots,\lvert n\rangle$. The eigenequations of $A$ are
    \begin{align}
        A\lvert i\rangle=\lambda_i\lvert i\rangle,\quad\forall i=1,\cdots,n.
    \end{align}
    Taking Hermitian conjugate of the above equations,
    \begin{align}
        \langle i\rvert A^{\dagger}=(A\lvert i\rangle)^{\dagger}=\lambda_i^*\langle i\rvert,\quad\forall i=1,\cdots,n.
    \end{align}
    Since $A$ is unitary, for any $i=1,\cdots,n$,
    \begin{gather}
        1=\langle i\vert i\rangle=\langle i\rvert I\lvert i\rangle=\langle i\rvert A^{\dagger}A\lvert i\rangle=\abs{\lambda_i}^2\langle i\vert i\rangle=\abs{\lambda_i}^2,\\
        \Longrightarrow\abs{\lambda_i}=1.
    \end{gather}
    Therefore, all eigenvalues of a unitary matrix has modulus $1$.
\end{pf}

\begin{exe}[Pauli matrices: Hermitian and unitary]
    Show that the Pauli matrices are Hermitian and unitary.
\end{exe}
\begin{pf}
    Since
    \begin{align}
        \sigma_0^{\dagger}=I^{\dagger}=I=\sigma_0,
    \end{align}
    and
    \begin{align}
        \sigma_0^{\dagger}\sigma_0=I^{\dagger}I=II=I,
    \end{align}
    $\sigma_0$ is Hermitian and unitary.
    Since
    \begin{align}
        \sigma_1^{\dagger}=\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]=\sigma_1,
    \end{align}
    and
    \begin{align}
        \sigma_1^{\dagger}\sigma_1=\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]=\left[\begin{matrix}
            1&0\\
            0&1
        \end{matrix}\right]=I,
    \end{align}
    $\sigma_1$ is Hermitian and unitary.
    Since
    \begin{align}
        \sigma_2^{\dagger}=\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]=\sigma_2,
    \end{align}
    and
    \begin{align}
        \sigma_2^{\dagger}\sigma_2=\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]=\left[\begin{matrix}
            1&0\\
            0&1
        \end{matrix}\right]=I,
    \end{align}
    $\sigma_2$ is Hermitian and unitary.
    Since
    \begin{align}
        \sigma_3^{\dagger}=\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]=\sigma_3,
    \end{align}
    and
    \begin{align}
        \sigma_3^{\dagger}\sigma_3=\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]=\left[\begin{matrix}
            1&0\\
            0&1
        \end{matrix}\right]=I,
    \end{align}
    $\sigma_3$ is Hermitian and unitary.
\end{pf}

\begin{exe}[Basis changes]
    Suppose $A'$ and $A''$ are matrix representations of an operator $A$ on a vector space $V$ with respect to two different orthonormal bases, $\lvert v_i\rangle$ and $\lvert w_i\rangle$. Then the elements of $A'$ and $A''$ are $A_{ij}'=\langle v_i\rvert A\lvert v_j\rangle$ and $A_{ij}''=\langle w_i\rvert A\lvert w_j\rangle$. characterize the relationship between $A'$ and $A''$.
\end{exe}
\begin{sol}
    Define $U=\sum_i\lvert w_i\rangle\langle v_i\rvert$ so that $\lvert w_i\rangle=U\lvert v_i\rangle$ and $\langle w_i\rvert=\langle v_i\rvert U^{\dagger}$ $\forall i$. Since $\lvert v_i\rangle$ and $\lvert w_i\rangle$ are two bases for $V$ and
    \begin{align}
        A_{ij}''=\langle w_i\rvert A''\lvert w_j\rangle=\langle v_i\rvert U^{\dagger}A'U\lvert v_j\rangle,
    \end{align}
    the relationship between $A'$ and $A''$ is
    \begin{align}
        A''=U^{\dagger}A'U.
    \end{align}
\end{sol}

\begin{exe}
    Repeat the proof of the spectral decomposition in Box 2.2 for the case when $M$ is Hermitian, simplifying the proof wherever possible.
\end{exe}
\begin{pf}
    \emph{Forward}: Suppose vector space $V$ is $d$-dimensional.
    The case $d=1$ is trivial.
    For the case $d\geq 2$, let $\lambda$ be an eigenvalue of $M$, $P$ the projector onto the $\lambda$ eigenspace, and $Q$ the projector onto the orthogonal component. The $M=IMI=(P+Q)M(P+Q)=PMP+PMQ+QMP+QMQ=PMP+QMQ$, where $PMP=\lambda P$, i.e., $PMP$ is diagonal, and $QMQQM^{\dagger}Q=QM^{\dagger}QQMQ$, i.e., $QMQ$ is normal. By induction, $QMQ$ is diagonal with respect to some orthonormal basis for the subspace $Q$. It follows that $M=PMP+QMQ$ is diagonal with respect to some orthonormal basis for the total vector space.

    \emph{Converse}: holds only if all the eigenvalues of $M$ are real. Suppose $M$ is diagonalizable with respect to an orthonormal basis $\lvert i\rangle$ for $V$, i.e.,
    \begin{align}
        M=\sum_i\lambda_i\lvert i\rangle\langle i\rvert,
    \end{align}
    where $\lambda_i$ are the eigenvalues of $M$.
    Since $M$ is Hermitian, all $\lambda_i$ are real,
    \begin{align}
        M^{\dagger}=\left(\sum_i\lambda_i\lvert i\rangle\langle i\rvert\right)^{\dagger}=\sum_i\lambda_i^*\lvert i\rangle\langle i\rvert=\sum_i\lambda_i\lvert i\rangle\langle i\rvert,
    \end{align}
    so $M$ is Hermitian.
\end{pf}

\begin{exe}
    Prove that two eigenvectors of a Hermitian operators with different eigenvalues are necessarily orthogonal.
\end{exe}
\begin{pf}
    Suppose Hermitian operator $A$ has two eigenvectors $\lvert v_1\rangle$ and $\lvert v_2\rangle$ corresponding to two different eigenvalues $\lambda_1$ and $\lambda_2$, i.e.,
    \begin{align}
        A\lvert v_1\rangle=\lambda_1\lvert v_1\rangle,\\
        A\lvert v_2\rangle=\lambda_2\lvert v_2\rangle.
    \end{align}
    Then,
    \begin{align}
        \langle v_1\rvert A\lvert v_2\rangle=\lambda_1\langle v_1\vert v_2\rangle=\lambda_2\langle v_1\vert v_2\rangle.
    \end{align}
    Since $\lambda_1\neq\lambda_2$, the above equation holds only if
    \begin{align}
        \langle v_1\vert v_2\rangle=0,
    \end{align}
    i.e., $\lvert v_1\rangle$ and $\lvert v_2\rangle$ are orthogonal.
\end{pf}

\begin{exe}
    Show that the eigenvalues of a projector $P$ are all either $0$ or $1$.
\end{exe}
\begin{pf}
    Suppose $\lvert 1\rangle,\cdots,\lvert k\rangle$ is an orthonormal basis for the subspace $P$ and $\lvert 1\rangle,\cdots,\lvert n\rangle$ is an orthonormal basis for the total vector space, where $k\leq n$. It is easy to see that $\lvert 1\rangle,\cdots,\lvert n\rangle$ are eigenvectors of projector $P=\sum_{i=1}^k\lvert i\rangle\langle i\rvert$:
    \begin{align}
        P\lvert j\rangle=\sum_{i=1}^k\lvert i\rangle\langle i\vert j\rangle=\sum_{i=1}^k\lvert i\rangle\delta_{ij}=\left\{\begin{array}{ll}
            \lvert j\rangle,&\text{if }j=1,\cdots,k;\\
            0,&\text{if }j=k+1,\cdots,n.
        \end{array}\right.
    \end{align}
    Therefore, the eigenvalues of a projector $P$ are all either $0$ or $1$.
\end{pf}

\begin{exe}[Hermiticity of positive operators]
    Show that a positive operator is necessarily Hermitian. (\emph{Hint}: Show that an arbitrary operator $A$ can be written $A=B+iC$ where $B$ and $C$ are Hermitian.)
\end{exe}
\begin{pf}
    An arbitrary positive operator $A$ can be written $A=B+iC$ where all the entries of $B=\frac{A+A^{\dagger}}{2}$ and $C=\frac{A-A^{\dagger}}{2i}$ are Hermitian. Since $A$ is a positive operator, for any vector $\lvert v\rangle$,
    \begin{align}
        \langle v\rvert A\lvert v\rangle=\langle v\rvert(B+iC)\lvert v\rangle=\langle v\rvert B\lvert v\rangle+i\langle v\rvert C\lvert v\rangle\geq 0,
    \end{align}
    and is real. In this way, $\langle v\rvert C\lvert v\rangle$ can only be either purely imaginary or zero. Since $C$ is Hermitian, it is diagonalizable and has a diagonal representation
    \begin{align}
        C=\sum_i\lambda_i\lvert i\rangle\langle i\rvert,
    \end{align}
    where $\lambda_i$ are the eigenvalues of $C$ and real, and $\lvert i\rangle$ is an orthonormal basis. Any vector $\lvert v\rangle$ can be written as a linear combination of $\lvert i\rangle$,
    \begin{align}
        \lvert v\rangle=\sum_ia_i\lvert i\rangle.
    \end{align}
    Hence
    \begin{align}
        \langle v\rvert C\lvert v\rangle=\sum_ia_i^*\langle i\rvert \sum_j\lambda_j\lvert j\rangle\langle j\rvert\sum_ka_k\lvert k\rangle=\sum_{i,j,k}\lambda_ja_i^*a_k\langle i\vert j\rangle\langle j\vert k\rangle=\sum_{ijk}\lambda_ja_i^*a_k\delta_{ij}\delta_{jk}=\sum_i\lambda_i\abs{a_i}^2
    \end{align}
    can not be purely imaginary and only be zero. Therefore, $A=B$ and is Hermitian.
\end{pf}

\begin{exe}
    Show that for any operator $A$, $A^{\dagger}A$ is positive.
\end{exe}
\begin{pf}
    For any vector $\lvert v\rangle$,
    \begin{align}
        (\lvert v\rangle,A^{\dagger}A\lvert v\rangle)=((A^{\dagger}A)^{\dagger}\lvert v\rangle,\lvert v\rangle)=(A^{\dagger}A\lvert v\rangle,\lvert v\rangle)=(\lvert v\rangle,A^{\dagger}A\lvert v\rangle)^*,
    \end{align}
    so $(\lvert v\rangle,A^{\dagger}A\lvert v\rangle)$ is real. Besides,
    \begin{align}
        (\lvert v\rangle,A^{\dagger}A\lvert v\rangle)=((A^{\dagger})^{\dagger}\lvert v\rangle,A\lvert v\rangle)=(A\lvert v\rangle,A\lvert v\rangle)\geq 0.
    \end{align}
    Therefore, for any operator $A$, $A^{\dagger}A$ is positive.
\end{pf}

\begin{exe}
    Let $\lvert\psi\rangle=(\lvert 0\rangle+\lvert 1\rangle)/\sqrt{2}$. Write out $\lvert\psi\rangle^{\otimes 2}$ and $\lvert\psi\rangle^{\otimes 3}$ explicitly, both in terms of tensor products like $\lvert 0\rangle\lvert 1\rangle$, and using the Kronecker product.
\end{exe}
\begin{sol}
    \begin{align}
        \lvert\psi\rangle^{\otimes 2}=&\frac{1}{\sqrt{2}}(\lvert 0\rangle+\lvert 1\rangle)\otimes\frac{1}{\sqrt{2}}(\lvert 0\rangle+\lvert 1\rangle)=\frac{1}{2}(\lvert 0\rangle\lvert 0\rangle+\lvert 0\rangle\lvert 1\rangle+\lvert 1\rangle\lvert 0\rangle+\lvert 1\rangle\lvert 1\rangle)\\
        =&\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            1
        \end{matrix}\right]\otimes\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            1
        \end{matrix}\right]=\frac{1}{2}\left[\begin{matrix}
            1\\
            1\\
            1\\
            1
        \end{matrix}\right],\\
        \notag\lvert\psi\rangle^{\otimes 3}=&\frac{1}{\sqrt{2}}(\lvert 0\rangle+\lvert 1\rangle)\otimes\frac{1}{\sqrt{2}}(\lvert 0\rangle+\lvert 1\rangle)\otimes\frac{1}{\sqrt{2}}(\lvert 0\rangle+\lvert 1\rangle)=\frac{1}{2\sqrt{2}}(\lvert 0\rangle\lvert 0\rangle\lvert 0\rangle+\lvert 0\rangle\lvert 0\rangle\lvert 1\rangle+\lvert 0\rangle\lvert 1\rangle\lvert 0\rangle+\lvert 0\rangle\lvert 1\rangle\lvert 1\rangle\\
        &+\lvert 1\rangle\lvert 0\rangle\lvert 0\rangle+\lvert 1\rangle\lvert 0\rangle\lvert 1\rangle+\lvert 1\rangle\lvert 1\rangle\lvert 0\rangle+\lvert 1\rangle\lvert 1\rangle\lvert 1\rangle)\\
        =&\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            1
        \end{matrix}\right]\otimes\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            1
        \end{matrix}\right]\otimes\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            1
        \end{matrix}\right]=\frac{1}{2\sqrt{2}}\left[\begin{matrix}
            1\\
            1\\
            1\\
            1\\
            1\\
            1\\
            1\\
            1
        \end{matrix}\right].
    \end{align}
\end{sol}

\begin{exe}
    Calculate the matrix representation of the tensor products of the Pauli operators (a) $X$ and $Z$; (b) $I$ and $X$; (c) $X$ and $I$. Is the tensor product commutative?
\end{exe}
\begin{sol}
    \begin{itemize}
        \item[(a)] The matrix representation of the tensor product of $X$ and $Z$:
        \begin{align}
            X\otimes Z=\left[\begin{matrix}
                0&1\\
                1&0
            \end{matrix}\right]\otimes\left[\begin{matrix}
                1&0\\
                0&-1
            \end{matrix}\right]=\left[\begin{matrix}
                0&0&1&0\\
                0&0&0&-1\\
                1&0&0&0\\
                0&-1&0&0
            \end{matrix}\right].
        \end{align}
        The matrix representation of the tensor product of $Z$ and $X$:
        \begin{align}
            Z\otimes X=\left[\begin{matrix}
                1&0\\
                0&-1
            \end{matrix}\right]\otimes\left[\begin{matrix}
                0&1\\
                1&0
            \end{matrix}\right]=\left[\begin{matrix}
                0&1&0&0\\
                1&0&0&0\\
                0&0&0&-1\\
                0&0&-1&0
            \end{matrix}\right].
        \end{align}
        Therefore, the tensor product $X\otimes Z$ and $Z\otimes X$ are not commutative.
        \item[(b)] The matrix representation of the tensor product of $I$ and $X$:
        \begin{align}
            I\otimes X=\left[\begin{matrix}
                1&0\\
                0&1
            \end{matrix}\right]\otimes\left[\begin{matrix}
                0&1\\
                1&0
            \end{matrix}\right]=\left[\begin{matrix}
                0&1&0&0\\
                1&0&0&0\\
                0&0&0&1\\
                0&0&1&0
            \end{matrix}\right].
        \end{align}
        \item[(c)] The matrix representation of the tensor product of $X$ and $I$:
        \begin{align}
            X\otimes I=\left[\begin{matrix}
                0&1\\
                1&0
            \end{matrix}\right]\otimes\left[\begin{matrix}
                1&0\\
                0&1
            \end{matrix}\right]=\left[\begin{matrix}
                0&0&1&0\\
                0&0&0&1\\
                1&0&0&0\\
                0&1&0&0
            \end{matrix}\right].
        \end{align}
        Therefore, the tensor product $I\otimes X$ and $X\otimes I$ are not commutative.
    \end{itemize}
\end{sol}

\begin{exe}
    Show that the transpose, complex conjugate, and joint operation distribute over the tensor product,
    \begin{align}
        (A\otimes B)^*=A^*\otimes B^*;\quad(A\otimes B)^T=A^T\otimes B^T;\quad(A\otimes B)^{\dagger}=A^{\dagger}\otimes B^{\dagger}.
    \end{align}
\end{exe}
\begin{pf}
    Suppose $A$ is a $m$ by $n$ matrix, and $B$ is a $p$ by $q$ matrix.
    \begin{itemize}
        \item[(a)] 
        \begin{align}
            (A\otimes B)^*=\left[\begin{matrix}
                A_{11}B&A_{12}B&\cdots&A_{1n}B\\
                A_{21}B&A_{22}B&\cdots&A_{2n}B\\
                \vdots&\vdots&\vdots&\vdots\\
                A_{m1}B&A_{m2}B&\cdots&A_{mn}B
            \end{matrix}\right]^*=\left[\begin{matrix}
                A_{11}^*B^*&A_{12}^*B^*&\cdots&A_{1n}^*B^*\\
                A_{21}^*B^*&A_{22}^*B^*&\cdots&A_{2n}^*B^*\\
                \vdots&\vdots&\vdots&\vdots\\
                A_{m1}^*B^*&A_{m2}^*B^*&\cdots&A_{mn}^*B^*
            \end{matrix}\right]=A^*\otimes B^*.
        \end{align}
        \item[(b)] 
        \begin{align}
            (A\otimes B)^T=\left[\begin{matrix}
                A_{11}B&A_{12}B&\cdots&A_{1n}B\\
                A_{21}B&A_{22}B&\cdots&A_{2n}B\\
                \vdots&\vdots&\vdots&\vdots\\
                A_{m1}B&A_{m2}B&\cdots&A_{mn}B
            \end{matrix}\right]^T=\left[\begin{matrix}
                A_{11}B^T&A_{21}B^T&\cdots&A_{m1}B^T\\
                A_{12}B^T&A_{22}B^T&\cdots&A_{m2}B^T\\
                \vdots&\vdots&\vdots&\vdots\\
                A_{1n}B^T&A_{2n}B^T&\cdots&A_{mn}B^T
            \end{matrix}\right]=A^T\otimes B^T.
        \end{align}
        \item[(c)] 
        \begin{align}
            (A\otimes B)^{\dagger}=[(A\otimes B)^*]^T=(A^*\otimes B^*)^T=(A^*)^T\otimes(B^*)^T=A^{\dagger}\otimes B^{\dagger}.
        \end{align}
    \end{itemize}
\end{pf}

\begin{exe}
    Show that the tensor product of two unitary operators is unitary.
\end{exe}
\begin{pf}
    Suppose $A$ and $B$ are two unitary operators,
    \begin{align}
        A^{\dagger}A=&I,\\
        B^{\dagger}B=&I.
    \end{align}
    Using the conclusion obtained in the previous exercise,
    \begin{align}
        (A\otimes B)^{\dagger}(A\otimes B)=(A^{\dagger}\otimes B^{\dagger})(A\otimes B)=(A^{\dagger}A)\otimes(B^{\dagger}B)=I\otimes I=I,
    \end{align}
    so the tensor product of two unitary operators is unitary.
\end{pf}

\begin{exe}
    Show that the tensor product of two Hermitian operators is Hermitian.
\end{exe}
\begin{pf}
    Suppose $A$ and $B$ are two Hermitian operators,
    \begin{align}
        A^{\dagger}=&A,\\
        B^{\dagger}=&B.
    \end{align}
    Using the conclusion obtained in Exercise 2.28,
    \begin{align}
        (A\otimes B)^{\dagger}=A^{\dagger}\otimes B^{\dagger}=A\otimes B,
    \end{align}
    so the tensor product of two Hermitian operators is Hermitian.
\end{pf}

\begin{exe}
    Show that the tensor product of two positive operators is positive.
\end{exe}
\begin{pf}
    Suppose $A$ and $B$ are positive operators on vector spaces $V$ and $W$ respectively. For any vector $\lvert v\rangle\in V$ and $\lvert w\rangle\in W$,
    \begin{align}
        (\lvert v\rangle,A\lvert v\rangle)\geq&0,\\
        (\lvert w\rangle,B\lvert w\rangle)\geq&0,
    \end{align}
    so $A\otimes B$ is a positive operator,
    \begin{align}
        (\lvert v\rangle\otimes\lvert w\rangle,(A\otimes B)(\lvert v\rangle\otimes\lvert w\rangle))=(\lvert v\rangle\otimes\lvert w\rangle,(A\lvert v\rangle)\otimes(B\lvert w\rangle))=(\lvert v\rangle,A\lvert v\rangle)(\lvert w\rangle,B\lvert w\rangle)\geq 0.
    \end{align}
    Therefore, the tensor product of two positive operators is positive.
\end{pf}

\begin{exe}
    Show that the tensor product of two projectors is a projector.
\end{exe}
\begin{pf}
    Suppose $\lvert i\rangle_V$ is an orthonormal basis for vector space $V$, and $\lvert j\rangle_W$ is an orthonormal basis for vector space $W$. The projector onto $V$ is
    \begin{align}
        P_V=\sum_i\lvert i\rangle_V\langle i\lvert_V,
    \end{align}
    and the projector onto $W$ is
    \begin{align}
        P_W=\sum_j\lvert j\rangle_W\langle j\lvert_W.
    \end{align}
    Their tensor product is
    \begin{align}
        P_V\otimes P_W=\left(\sum_i\lvert i\rangle_V\langle i\lvert_V\right)\otimes\left(\sum_j\lvert j\rangle_W\langle j\lvert_W\right)=\sum_{i,j}(\lvert i\rangle_V\otimes\lvert j\rangle_W)(\langle i\rvert_V\otimes\langle j\rvert_W).
    \end{align}
    Since both $\lvert i\rangle_V$ and $\lvert j\rangle_W$ are independent, $\lvert i\rangle\otimes\lvert j\rangle$ are independent. Since both $\lvert i\rangle_V$ and $\lvert j\rangle_W$ are orthonormal, $\lvert i\rangle\otimes\lvert j\rangle$ are orthonormal,
    \begin{align}
        (\vert i\rangle_V\otimes\lvert j\rangle_W,\lvert k\rangle_V\otimes\lvert l\rangle_W)=(\lvert i\rangle_V,\lvert k\rangle_V)(\lvert j\rangle_W,\lvert l\rangle_W)=\delta_{ik}\delta_{jl}.
    \end{align}
    Since any vector $\lvert v\rangle\in V$ can be written as a linear combination of $\lvert i\rangle_V$,
    \begin{align}
        \lvert v\rangle=\sum_ia_i\lvert i\rangle_V,
    \end{align}
    and any vector $\lvert w\rangle\in W$ can be written as a linear combination of $\lvert j\rangle_W$,
    \begin{align}
        \lvert w\rangle=\sum_jb_j\lvert j\rangle_W,
    \end{align}
    the tensor product $\lvert v\otimes\lvert w\rangle\in V\otimes W$ can be written as a linear combination of $\lvert i\rangle_V\otimes\lvert j\rangle_W$,
    \begin{align}
        \lvert v\rangle\otimes\lvert w\rangle=\left(\sum_ia_i\lvert i\rangle_V\right)\otimes\left(\sum_jb_j\lvert j\rangle_W\right)=\sum_{i,j}a_ib_j\lvert i\rangle_V\otimes\lvert j\rangle_W.
    \end{align}
    Hence $\lvert i\rangle_V\otimes\lvert j\rangle_W$ is an orthonormal basis for $V\otimes W$ and $P_V\otimes P_W$ is a projector onto $V\otimes W$. Therefore, the tensor product of two projectors is a projector.
\end{pf}

\begin{exe}
    The Hadamard operator on one qubit may be written as
    \begin{align}
        H=\frac{1}{\sqrt{2}}\left[(\lvert 0\rangle+\lvert 1\rangle)\langle 0\rvert+(\lvert 0\rangle-\lvert 1\rangle)\langle 1\rvert\right].
    \end{align}
    Show explicitly that the Hadamard transform on $n$ qubits, $H^{\otimes n}$, may be written as
    \begin{align}
        H^{\otimes n}=\frac{1}{\sqrt{2^n}}\sum_{x,y}(-1)^{x\cdot y}\lvert x\rangle\langle y\rvert.
    \end{align}
    Write out an explicit matrix representation for $H^{\otimes 2}$.
\end{exe}
\begin{sol}
    Hadamard transform on one qubit may be written as
    \begin{align}
        H\lvert 0\rangle=\frac{1}{\sqrt{2}}(\lvert 0\rangle\langle 0\rvert+\lvert 1\rangle\langle 0\rvert+\lvert 0\rangle\langle 1\rvert-\lvert 1\rangle\langle 1\rvert)=\frac{1}{\sqrt{2}}\sum_{x,y=0}^1\lvert x\rangle\langle y\rvert.
    \end{align}
    Thus Hadamard transform on $n$ qubits may be written as
    \begin{align}
        \notag H^{\otimes n}=&\left(\frac{1}{\sqrt{2}}\sum_{x_1,y_1=0}^1\lvert x_1\rangle\langle y_1\rvert\right)\otimes\cdots\otimes\left(\frac{1}{\sqrt{2}}\sum_{x_n,y_n=0}^1\lvert x_n\rangle\langle y_n\rvert\right)=\frac{1}{\sqrt{2^n}}\sum_{\substack{x_1,\cdots,x_n\\y_1,\cdots,y_n}=0}^1\lvert x_1,\cdots,x_n\rangle\langle y_1,\cdots,y_n\rvert\\
        =&\frac{1}{\sqrt{2^n}}\sum_{x,y}(-1)^{x\cdot y}\lvert x\rangle\langle y\rvert.
    \end{align}
    The matrix representation for $H^{\otimes 2}$ is
    \begin{align}
        H^{\otimes 2}=\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1&1\\
            1&-1
        \end{matrix}\right]\otimes\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1&1\\
            1&-1
        \end{matrix}\right]=\frac{1}{2}\left[\begin{matrix}
            1&1&1&1\\
            1&-1&1&-1\\
            1&1&-1&-1\\
            1&-1&-1&1
        \end{matrix}\right].
    \end{align}
\end{sol}

\begin{exe}
    Find the square root and logarithm of the matrix
    \begin{align}
        \left[\begin{matrix}
            4&3\\
            3&4
        \end{matrix}\right].
    \end{align}
\end{exe}
\begin{sol}
    The square root of the matrix is
    \begin{align}
        \left[\begin{matrix}
            2&\sqrt{3}\\
            \sqrt{3}&2
        \end{matrix}\right].
    \end{align}
    The logarithm of the matrix is
    \begin{align}
        \left[\begin{matrix}
            \ln 4&\ln 3\\
            \ln 3&\ln 4
        \end{matrix}\right].
    \end{align}
\end{sol}

\begin{exe}[Exponential of the Pauli matrices]
    Let $\vec{v}$ be any real, three dimensional unit vector and $\theta$ a real number. Prove that
    \begin{align}
        \label{E2.35}
        \exp(i\theta\vec{v}\cdot\vec{\sigma})=\cos(\theta)I+i\sin(\theta)\vec{v}\cdot\vec{\sigma},
    \end{align}
    where $\vec{v}\cdot\vec{\sigma}=\sum_{i=1}^3v_i\sigma_i$. This exercise is generalized in Problem 2.1 on page 117.
\end{exe}
\begin{pf}
    The left side of equation \eqref{E2.35} is
    \begin{align}
        \exp(i\theta\vec{v}\cdot\vec{\sigma})=\sum_{n=0}^{\infty}\frac{1}{n!}(i\theta\vec{v}\cdot\vec{\sigma})^k=\sum_{n=1}^{\infty}\frac{(-1)^n}{(2n)!}(\theta\vec{v}\cdot\vec{\sigma})^{2n}+\sum_{n=0}^{\infty}\frac{i(-1)^n}{(2n+1)!}(\theta\vec{v}\cdot\vec{\sigma})^{2n+1}.
    \end{align}
    Note that
    \begin{align}
        (\vec{v}\cdot\vec{\sigma})^2=\left(\sum_{i=1}^3v_i\sigma_i\right)^2=\sum_{i,j=1}^3v_iv_j\sigma_i\sigma_j.
    \end{align}
    Due to the anti-commutation relation between the Pauli matrices,
    \begin{align}
        \{\sigma_i,\sigma_j\}=\sigma_i\sigma_j+\sigma_j\sigma_i=2\delta_{ij}I=\left\{\begin{array}{ll}
            2I,&i=j;\\
            0,&i\neq j,
        \end{array}\right.
    \end{align}
    we have
    \begin{align}
        (\vec{v}\cdot\vec{\sigma})^2=\sum_iv_i^2I=I.
    \end{align}
    Hence the left side of equation \eqref{E2.35} can be written as
    \begin{align}
        \exp(i\theta\vec{v}\cdot\vec{\sigma})=\sum_{n=1}^{\infty}\frac{(-1)^n}{(2n)!}\theta^{2n}+\sum_{n=0}^{\infty}\frac{i(-1)^n}{(2n+1)}\theta^{2n+1}\vec{\sigma}=\cos(\theta)I+i\sin(\theta)\vec{v}\cdot\vec{\sigma},
    \end{align}
    which equals the right side of equation \eqref{E2.35}. Therefore, equation \eqref{E2.35} holds.
\end{pf}

\begin{exe}
    Show that the Pauli matrices except for $I$ have trace zero.
\end{exe}
\begin{pf}
    The trace of $I$ is
    \begin{align}
        \tr(I)=1+1=2.
    \end{align}
    The trace of $X$ is
    \begin{align}
        \tr(X)=0+0=0.
    \end{align}
    The trace of $Y$ is
    \begin{align}
        \tr(Y)=0+0=0.
    \end{align}
    The trace of $Z$ is
    \begin{align}
        \tr(Z)=1+(-1)=0.
    \end{align}
    Therefore, the Pauli matrices except for $I$ have trace zero.
\end{pf}

\begin{exe}[Cyclic property of the trace]
    If $A$ and $B$ are two linear operators show that
    \begin{align}
        \tr(AB)=\tr(BA).
    \end{align}
\end{exe}
\begin{pf}
    \begin{align}
        \tr(AB)=\sum_i(AB)_{ii}=\sum_i\left(\sum_jA_{ij}B_{ji}\right)=\sum_j\left(\sum_iB_{ji}A_{ij}\right)=\sum_j(BA)_{jj}=\tr(BA).
    \end{align}
\end{pf}

\begin{exe}[Linearity of the trace]
    If $A$ and $B$ are two linear operators. show that
    \begin{align}
        \tr(A+B)=\tr(A)+\tr(B)
    \end{align}
    and if $z$ is an arbitrary complex number show that
    \begin{align}
        \tr(zA)=z\tr(A).
    \end{align}
\end{exe}
\begin{pf}
    \begin{align}
        \tr(A+B)=&\sum_i(A+B)_{ii}=\sum_iA_{ii}+\sum_iB_{ii}=\tr(A)+\tr(B).\\
        \tr(zA)=&\sum_i(zA)_{ii}=z\sum_iA_{ii}=z\tr(A).
    \end{align}
\end{pf}

\begin{exe}[The Hilbert-Schmidt inner product on operators]
    The set $L_V$ of linear operators on a Hilbert space $V$ is obviously a vector space -- the sum of two linear operators is a Linear operator, $zA$ is a linear operator if $A$ is a linear operator and $z$ is a complex number, and there is a zero element $0$. An important additional result is that the vector space $L_V$ can be given a natural inner product structure, turning it into a Hilbert space.
    \begin{itemize}
        \item[(1)] Show that the function $(\cdot,\cdot)$ on $L_V\times L_V$ defined by
        \begin{align}
            (A,B)\equiv\tr(A^{\dagger}B)
        \end{align}
        is an inner product function. This inner product is known as the \emph{Hilbert-Schmidt} or \emph{trace} inner product.
        \item[(2)] If $V$ has $d$ dimensions show that $L_V$ has dimension $d^2$.
        \item[(3)] Find an orthonormal basis of Hermitian matrices for the Hilbert space $L_V$.
    \end{itemize}
\end{exe}
\begin{sol}
    \begin{itemize}
        \item[(1)] The function $(\cdot,\cdot)$ on $L_V\times L_V$ satisfies the requirements that:
        \begin{itemize}
            \item[(a)] $(\cdot,\cdot)$ is linear in the second argument,
            \begin{align}
                \left(A,\sum_j\lambda_jB^{(j)}\right)=\tr\left(A^{\dagger}\sum_j\lambda_jB^{(j)}\right)=\tr\left(\sum_j\lambda_jA^{\dagger}B^{(j)}\right)=\sum_j\lambda_j\tr(A^{\dagger}B^{(j)})=\sum_j\lambda(A,B^{\dagger}).
            \end{align}
            \item[(b)] 
            \begin{align}
                \notag(A,B)=&\tr(A^{\dagger}B)=\sum_i(A^{\dagger}B)_{ii}=\sum_i\left[\sum_j(A^{\dagger})_{ij}B_{ji}\right]=\sum_i\left(\sum_jA_{ji}^*B_{ji}\right)=\left[\sum_i\left(\sum_jA_{ji}B_{ji}^*\right)\right]^*\\
                =&\left\{\sum_i\left[\sum_j(B^{\dagger})_{ij}A_{ji}\right]\right\}^*=\left[\sum_i(B^{\dagger}A)_{ii}\right]^*=[\tr(B^{\dagger}A)]^*=(B,A)^*.
            \end{align}
            \item[(c)] 
            \begin{align}
                (A,A)=\tr(A^{\dagger}A)=\sum_i(A^{\dagger}A)_{ii}=\sum_i\left[\sum_j(A^{\dagger})_{ij}A_{ji}\right]=\sum_i\left(\sum_jA_{ji}^*A_{ji}\right)=\sum_{i,j}\abs{A_{ji}}^2\geq 0,
            \end{align}
            with equality if and only if $A=0$.
        \end{itemize}
        Therefore, the function $(\cdot,\cdot)$ defined on $L_V\times L_V$ is an inner product function.
        \item[(2)] Suppose $\lvert 1\rangle,\cdots,\lvert d\rangle$ form an orthonormal basis for $V$. Since
        \begin{itemize}
            \item[(a)] any operator $A$ in $V$ can be written as
            \begin{align}
                A=\sum_{i,j}A_{ij}\lvert i\rangle\langle j\rvert,
            \end{align}
            and
            \item[(b)] $\lvert i\rangle\langle j\rvert$ are orthonormal,
            \begin{align}
                (\lvert i\rangle\langle j\rvert,\lvert m\rangle\langle n\rvert)=\tr[(\lvert i\rangle\langle j\rvert)^{\dagger}\lvert m\rangle\langle n\rvert]=\tr(\lvert j\rangle\langle i\vert m\rangle\langle n\rvert)=\sum_k\langle k\rvert j\rangle\langle i\vert m\rangle\langle n\vert k\rangle=\sum_k\delta_{kj}\delta_{im}\delta_{nk}=\delta_{im}\delta_{jn},
            \end{align}
            and thus linearly independent, $\lvert i\rangle\langle j\rvert$ is an orthonormal basis for $L_V$. Since this basis has $d^2$ elements, $L_V$ has dimension $d^2$.
        \end{itemize}
        \item[(3)] $\left\{\lvert i\rangle\langle i\rvert,\frac{\lvert i\rangle\langle j\rvert+\lvert j\rangle\langle i\rvert}{\sqrt{2}},\frac{\lvert i\rangle\langle j\rvert-\lvert j\rangle\langle i\rvert}{i\sqrt{2}};\forall 1\leq i<j\leq d\right\}$ is an orthonormal basis of Hermitian matrices for the Hilbert space $L_V$. Here is the reason:
        \begin{itemize}
            \item[(a)] For any Hermitian matrices $A$, since
            \begin{align}
                A^{\dagger}=A\Longrightarrow A_{ji}^*=A_{ji},\quad\forall i,j=1,\cdots,d,
            \end{align}
            and $A_{ii}$ is real $\forall i$, $A$ can be written as
            \begin{align}
                \notag A=&\sum_{i,j}A_{ij}\lvert i\rangle\langle j\rvert=\sum_iA_{ii}\lvert i\rangle\langle i\rvert+\sum_{i=2}^d\sum_{j=1}^{i-1}A_{ij}\lvert i\rangle\langle j\rvert+\sum_{j=2}^d\sum_i^{j-1}A_{ij}\lvert i\rangle\langle j\rvert\\
                \notag=&\sum_iA_{ii}\lvert i\rangle\langle i\rvert+\sum_{i=2}^d\sum_{j=1}^{i-1}A_{ji}^*\lvert i\rangle\langle j\rvert+\sum_{j=2}^d\sum_i^{j-1}A_{ij}\lvert i\rangle\langle j\rvert\\
                \notag=&\sum_iA_{ii}\lvert i\rangle\langle i\rvert+\sum_{j=2}^d\sum_{i=1}^{j-1}A_{ij}^*\lvert j\rangle\langle i\rvert+\sum_{j=2}^d\sum_i^{j-1}A_{ij}\lvert i\rangle\langle j\rvert\\
                \notag=&\sum_iA_{ii}\lvert i\rangle\langle i\rvert+\sum_{j=2}^d\sum_{i=1}^{j-1}\frac{A_{ij}+A_{ij}^*}{\sqrt{2}}\frac{\lvert i\rangle\langle j\rvert+\lvert j\rangle\langle i\rvert}{\sqrt{2}}+\sum_{j=2}^d\sum_i^{j-1}\frac{A_{ij}-A_{ij}^*}{-i\sqrt{2}}\frac{\lvert i\rangle\langle j\rvert-\lvert j\rangle\langle i\rvert}{i\sqrt{2}}.
            \end{align}
            \item[(b)] Elements in $\left\{\lvert i\rangle\langle i\rvert,\frac{\lvert i\rangle\langle j\rvert+\lvert j\rangle\langle i\rvert}{\sqrt{2}},\frac{\lvert i\rangle\langle j\rvert-\lvert j\rangle\langle i\rvert}{i\sqrt{2}};\forall 1\leq i<j\leq d\right\}$ are orthonormal and thus linearly independent,
            \begin{align}
                \notag\left(\lvert i\rangle\langle i\rvert,\frac{\lvert m\rangle\langle n\rvert+\lvert n\rangle\langle m\rvert}{\sqrt{2}}\right)=&\tr\left[(\lvert i\rangle\langle i\rvert)^{\dagger}\left(\frac{\lvert m\rangle\langle n\rvert+\lvert n\rangle\langle m\rvert}{\sqrt{2}}\right)\right]=\frac{1}{\sqrt{2}}\tr\left[\lvert i\rangle\langle i\rvert(\lvert m\rangle\langle n\rvert+\lvert n\rangle\langle m\rvert)\right]\\
                \notag=&\frac{1}{\sqrt{2}}\tr(\lvert i\rangle\langle i\vert m\rangle\langle n\rvert+\lvert i\rangle\langle i\vert n\rangle\langle m\rvert)\\
                \notag=&\frac{1}{\sqrt{2}}\sum_k(\langle k\vert i\rangle\langle i\vert m\rangle\langle n\vert k\rangle+\langle k\vert i\rangle\langle i\vert n\rangle\langle m\vert k\rangle)\\
                \notag=&\frac{1}{\sqrt{2}}\sum_k(\delta_{ki}\delta_{im}\delta_{nk}+\delta_{ki}\delta_{in}\delta_{mk})\\
                =&\sqrt{2}\delta_{im}\delta_{in}=0,\quad\forall i=1,\cdots,d,1\leq m<n\leq d,\\
                \notag\left(\lvert i\rangle\langle i\rvert,\frac{\lvert m\rangle\langle n\rvert-\lvert n\rangle\langle m\rvert}{i\sqrt{2}}\right)=&\tr\left[(\lvert i\rangle\langle i\rvert)^{\dagger}\left(\frac{\lvert m\rangle\langle n\rvert+\lvert n\rangle\langle m\rvert}{i\sqrt{2}}\right)\right]=\frac{1}{i\sqrt{2}}\tr\left[\lvert i\rangle\langle i\rvert(\lvert m\rangle\langle n\rvert+\lvert n\rangle\langle m\rvert)\right]\\
                \notag=&\frac{1}{i\sqrt{2}}\tr(\lvert i\rangle\langle i\vert m\rangle\langle n\rvert-\lvert i\rangle\langle i\vert n\rangle\langle m\rvert)\\
                \notag=&\frac{1}{i\sqrt{2}}\sum_k(\langle k\vert i\rangle\langle i\vert m\rangle\langle n\vert k\rangle-\langle k\vert i\rangle\langle i\vert n\rangle\langle m\vert k\rangle)\\
                \notag=&\frac{1}{i\sqrt{2}}\sum_k(\delta_{ki}\delta_{im}\delta_{nk}-\delta_{ki}\delta_{in}\delta_{mk})\\
                =&0,\quad\forall i=1,\cdots,d,1\leq m<n\leq d,\\
                \notag\left(\frac{\lvert i\rangle\langle j\rvert+\lvert j\rangle\langle i\rvert}{\sqrt{2}},\frac{\lvert m\rangle\langle n\rvert-\lvert n\rangle\langle m\rvert}{i\sqrt{2}}\right)=&\tr\left[\left(\frac{\lvert i\rangle\langle j\rvert+\lvert j\rangle\langle i\rvert}{\sqrt{2}}\right)^{\dagger}\left(\frac{\lvert m\rangle\langle n\rvert-\lvert n\rangle\langle m\rvert}{i\sqrt{2}}\right)\right]\\
                \notag=&\frac{1}{2i}\tr\left[(\lvert j\rangle\langle i\rvert+\lvert i\rangle\langle j\rvert)(\lvert m\rangle\langle n\rvert-\lvert n\rangle\langle m\rvert)\right]\\
                \notag=&\frac{1}{2i}\tr(\lvert j\rangle\langle i\vert m\rangle\langle n\rvert-\lvert j\rangle\langle i\vert n\rangle\langle m\rvert+\lvert i\rangle\langle j\vert m\rangle\langle n\rvert-\lvert i\rangle\langle j\vert n\rangle\langle m\rvert)\\
                \notag=&\frac{1}{2i}\sum_k(\langle k\vert j\rangle\langle i\vert m\rangle\langle n\vert k\rangle-\langle k\vert j\rangle\langle i\vert n\rangle\langle m\vert k\rangle\\
                \notag&+\langle k\vert i\rangle\langle j\vert m\rangle\langle n\vert k\rangle-\langle k\vert i\rangle\langle j\vert n\rangle\langle m\vert k\rangle)\\
                \notag=&\frac{1}{2i}\sum_k(\delta_{kj}\delta_{im}\delta_{nk}-\delta_{kj}\delta_{in}\delta_{mk}+\delta_{ki}\delta_{jm}\delta_{nk}-\delta_{ki}\delta_{jn}\delta_{mk})\\
                =&0,\quad\forall 1\leq i<j\leq d,\cdots,d,1\leq m<n\leq d,\\
                \notag(\lvert i\rangle\langle i\rvert,\lvert j\rangle\langle j\rvert)=&\tr[(\lvert i\rangle\langle i\rvert)^{\dagger}\lvert j\rangle\langle j\rvert]=\tr(\lvert i\rangle\langle i\vert j\rangle\langle j\rvert)=\sum_k\langle k\vert i\rangle\langle i\vert j\rangle\langle j\vert k\rangle=\sum_k\delta_{ki}\delta_{ij}\delta_{jk}\\
                =&\delta_{ij},\quad\forall i,j=1,\cdots,d,\\
                \notag\left(\frac{\lvert i\rangle\langle j\rvert+\lvert i\rangle\langle j\rvert}{\sqrt{2}},\frac{\lvert m\rangle\langle n\rvert+\lvert n\rangle\langle m\rvert}{\sqrt{2}}\right)=&\tr\left[\left(\frac{\lvert i\rangle\langle j\rvert+\lvert j\rangle\langle i\rvert}{\sqrt{2}}\right)^{\dagger}\frac{\lvert m\rangle\langle n\rvert+\lvert n\rangle\langle m\rvert}{\sqrt{2}}\right]\\
                \notag=&\frac{1}{2}\tr[(\lvert j\rangle\langle i\rvert+\lvert i\rangle\langle j\rvert)(\lvert m\rangle\langle m\rvert+\lvert n\rangle\langle m\rvert)]\\
                \notag=&\frac{1}{2}\tr(\lvert j\rangle\langle i\vert m\rangle\langle n\rvert+\lvert j\rangle\langle i\vert n\rangle\langle m\rvert+\lvert i\rangle\langle j\vert m\rangle\langle n\rvert+\lvert i\rangle\langle j\vert n\rangle\langle m\rvert)\\
                \notag=&\frac{1}{2}\sum_k(\langle k\vert j\rangle\langle i\vert m\rangle\langle n\vert k\rangle+\langle k\vert j\rangle\langle i\vert n\rangle\langle m\vert k\rangle\\
                \notag&+\langle k\vert i\rangle\langle j\vert m\rangle\langle n\vert k\rangle+\langle k\vert i\rangle\langle j\vert n\rangle\langle m\vert k\rangle)\\
                \notag=&\frac{1}{2}\sum_k(\delta_{kj}\delta_{im}\delta_{nk}+\delta_{kj}\delta_{in}\delta_{mk}+\delta_{ki}\delta_{jm}\delta_{nk}+\delta_{ki}\delta_{jn}\delta_{mk})\\
                =&\delta_{im}\delta_{jn}+\delta_{in}\delta_{jm},\quad\forall 1\leq i<j\leq d,1\leq m<n\leq d,\\
                \notag\left(\frac{\lvert i\rangle\langle j\rvert-\lvert j\rangle\langle i\rvert}{i\sqrt{2}},\frac{\lvert m\rangle\langle n\rvert-\lvert n\rangle\langle m\rvert}{i\sqrt{2}}\right)=&\tr\left[\left(\frac{\lvert i\rangle\langle j\rvert-\lvert j\rangle\langle i\rvert}{i\sqrt{2}}\right)^{\dagger}\frac{\lvert m\rangle\langle n\rvert-\lvert n\rangle\langle m\rvert}{i\sqrt{2}}\right]\\
                \notag=&\frac{1}{2}\tr\left[(\lvert j\rangle\langle i\rvert-\lvert i\rangle\langle j\rvert)(\lvert m\rangle\langle n\rvert-\lvert n\rangle\langle m\rvert)\right]\\
                \notag=&\frac{1}{2}\tr(\lvert j\rangle\langle i\vert m\rangle\langle n\rvert-\lvert j\rangle\langle i\vert n\rangle\langle m\rvert-\lvert i\rangle\langle j\vert m\rangle\langle n\rvert+\lvert j\rangle\langle i\vert n\rangle\langle m\rvert)\\
                \notag=&\frac{1}{2}\sum_k(\langle k\vert j\rangle\langle i\vert m\rangle\langle n\vert k\rangle-\langle k\vert j\rangle\langle i\vert n\rangle\langle m\vert k\rangle\\
                \notag&-\langle k\vert i\rangle\langle j\vert m\rangle\langle n\vert k\rangle+\langle k\vert i\rangle\langle j\vert n\rangle\langle m\vert k\rangle)\\
                \notag=&\frac{1}{2}\sum_k(\delta_{kj}\delta_{im}\delta_{nk}-\delta_{kj}\delta_{in}\delta_{mk}-\delta_{ki}\delta_{jm}\delta_{nk}+\delta_{ki}\delta_{jn}\delta_{mk})\\
                =&\delta_{im}\delta_{jn}-\delta_{in}\delta_{jm}=\delta_{im}\delta_{jn},\quad\forall 1\leq i<j\leq d,1\leq m<n\leq d.
            \end{align}
            and thus linearly independent.
        \end{itemize}
        Therefore, $\left\{\lvert i\rangle\langle i\rvert,\frac{\lvert i\rangle\langle j\rvert+\lvert j\rangle\langle i\rvert}{\sqrt{2}},\frac{\lvert i\rangle\langle j\rvert-\lvert j\rangle\langle i\rvert}{i\sqrt{2}};\forall 1\leq i<j\leq d\right\}$ is an orthonormal basis of Hermitian matrices for the Hilbert space $L_V$.
    \end{itemize}
\end{sol}

\begin{exe}[commutation relation for the Pauli matrices]
    Verify the commutation relations
    \begin{align}
        [X,Y]=2iZ;\quad[Y,Z]=2iX;\quad[Z,X]=2iY.
    \end{align}
    There is an elegant way of writing this using $\epsilon_{jkl}$, the alternative antisymmetric tensor on three indices for which $\epsilon_{jkl}=0$ except for $\epsilon_{123}=\epsilon_{231}=\epsilon_{312}=1$, and $\epsilon_{321}=\epsilon_{213}=\epsilon_{132}=-1$:
    \begin{align}
        [\sigma_j,\sigma_k]=2i\sum_{l=1}^3\epsilon_{jkl}\sigma_l.
    \end{align}
\end{exe}
\begin{pf}
    The commutation relations for the Pauli matrices are
    \begin{align}
        [X,Y]=&XY-YX=\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]-\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]=\left[\begin{matrix}
            i&0\\
            0&-i
        \end{matrix}\right]-\left[\begin{matrix}
            -i&0\\
            0&i
        \end{matrix}\right]=2i\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]=2iZ,\\
        [Y,Z]=&YZ-ZY=\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]-\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]=\left[\begin{matrix}
            0&i\\
            i&0
        \end{matrix}\right]-\left[\begin{matrix}
            0&-i\\
            -i&0
        \end{matrix}\right]=2i\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]=2iX,\\
        [Z,X]=&ZX-XZ=\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]-\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]=\left[\begin{matrix}
            0&1\\
            -1&0
        \end{matrix}\right]-\left[\begin{matrix}
            0&-1\\
            1&0
        \end{matrix}\right]=2i\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]=2iY,
    \end{align}
    and
    \begin{align}
        [\sigma_j,\sigma_j]=&\sigma_l\sigma_l-\sigma_l\sigma_l=0,\quad\forall j=1,2,3,\\
        [\sigma_j,\sigma_k]=&\sigma_j\sigma_k-\sigma_k\sigma_j=-[\sigma_k,\sigma_j],\quad\forall j,k=1,2,3.
    \end{align}
    Therefore, in general,
    \begin{align}
        [\sigma_j,\sigma_k]=2i\sum_{l=1}^3\epsilon_{jkl}\sigma_l.
    \end{align}
\end{pf}

\begin{exe}[Anti-commutation relation for the Pauli matrices]
    Verify the anti-commutation relations
    \begin{align}
        \{\sigma_i,\sigma_j\}=0
    \end{align}
    where $i\neq j$ are both chosen from the set $1,2,3$. Also verify that ($i=0,1,2,3$)
    \begin{align}
        \sigma_i^2=I.
    \end{align}
\end{exe}
\begin{pf}
    \begin{align}
        \{\sigma_1,\sigma_2\}=&\sigma_1\sigma_2+\sigma_2\sigma_1=\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]+\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]=\left[\begin{matrix}
            i&0\\
            0&-i
        \end{matrix}\right]+\left[\begin{matrix}
            -i&0\\
            0&i
        \end{matrix}\right]=0,\\
        \{\sigma_2,\sigma_3\}=&\sigma_2\sigma_3+\sigma_3\sigma_2=\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]+\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]=\left[\begin{matrix}
            0&i\\
            i&0
        \end{matrix}\right]+\left[\begin{matrix}
            0&-i\\
            -i&0
        \end{matrix}\right]=0,\\
        \{\sigma_3,\sigma_1\}=&\sigma_3\sigma_1+\sigma_1\sigma_3=\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]+\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]=\left[\begin{matrix}
            0&1\\
            -1&0
        \end{matrix}\right]+\left[\begin{matrix}
            0&-1\\
            1&0
        \end{matrix}\right]=0,\\
        \sigma_0^2=&I^2=I,\\
        \sigma_1^2=&\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]=\left[\begin{matrix}
            1&0\\
            0&1
        \end{matrix}\right]=I,\\
        \sigma_2^2=&\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]=\left[\begin{matrix}
            1&0\\
            0&1
        \end{matrix}\right]=I,\\
        \sigma_3^2=&\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]=\left[\begin{matrix}
            1&0\\
            0&1
        \end{matrix}\right]=I.
    \end{align}
    Therefore, in general,
    \begin{align}
        \{\sigma_i,\sigma_j\}=0,\quad\forall i,j=1,2,3,\text{ and }i\neq j,
    \end{align}
    and
    \begin{align}
        \sigma_i^2=I,\quad\forall i=0,1,2,3.
    \end{align}
\end{pf}

\begin{exe}
    Verify that
    \begin{align}
        AB=\frac{[A,B]+\{A,B\}}{2}.
    \end{align}
\end{exe}
\begin{pf}
    \begin{align}
        \frac{[A,B]+\{A,B\}}{2}=\frac{AB-BA+AB+BA}{2}=AB.
    \end{align}
\end{pf}

\begin{exe}
    Show that for $j,k=1,2,3$,
    \begin{align}
        \sigma_j\sigma_k=\delta_{jk}I+i\sum_{l=1}^3\epsilon_{jkl}\sigma_l.
    \end{align}
\end{exe}
\begin{pf}
    As obtained in exercise 2.40, 2.41 and 2.42,
    \begin{align}
        [\sigma_j,\sigma_k]=&2i\sum_{l=1}^3\epsilon_{jkl}\sigma_l,\quad\forall j,k=1,2,3,\\
        \{\sigma_j,\sigma_k\}=&2\delta_{jk}I,\quad\forall j,k=1,2,3,\\
        AB=&\frac{[A,B]+\{A,B\}}{2},
    \end{align}
    so
    \begin{align}
        \sigma_j\sigma_k=\frac{[\sigma_j,\sigma_k]+\{\sigma_j,\sigma_k\}}{2}=\frac{2i\sum_{l=1}^3\epsilon_{jkl}\sigma_l+2\delta_{jk}I}{2}=\delta_{jk}I+i\sum_{l=1}^3\epsilon_{jkl}\sigma_l,\quad\forall j,k=1,2,3.
    \end{align}
\end{pf}

\begin{exe}
    Suppose $[A,B]=0$, $\{A,B\}=0$, and $A$ is invertible. Show that $B$ must be 0.
\end{exe}
\begin{pf}
    \begin{align}
        AB=\frac{[A,B]+\{A,B\}}{2}=0.
    \end{align}
    $A$ is invertible and thus can not be $0$, so $B$ must be $0$.
\end{pf}

\begin{exe}
    Show that $[A,B]^{\dagger}=[B^{\dagger},A^{\dagger}]$.
\end{exe}
\begin{pf}
    \begin{align}
        [A,B]^{\dagger}=(AB-BA)^{\dagger}=B^{\dagger}A^{\dagger}-A^{\dagger}B^{\dagger}=[B^{\dagger},A^{\dagger}].
    \end{align}
\end{pf}

\begin{exe}
    Show that $[A,B]=-[B,A]$.
\end{exe}
\begin{pf}
    \begin{align}
        [A,B]=AB-BA=-(BA-AB)=-[B,A].
    \end{align}
\end{pf}

\begin{exe}
    Suppose $A$ and $B$ are Hermitian. Show that $i[A,B]$ is Hermitian.
\end{exe}
\begin{pf}
    Since $A$ and $B$ are Hermitian,
    \begin{align}
        A^{\dagger}=&A,\\
        B^{\dagger}=&B.
    \end{align}
    Since
    \begin{align}
        (i[A,B])^{\dagger}=[i(AB-BA)]^{\dagger}=-i(B^{\dagger}A^{\dagger}-A^{\dagger}B^{\dagger})=i(AB-BA)=i[A,B],
    \end{align}
    $i[A,B]$ is Hermitian.
\end{pf}

\begin{exe}
    What is the polar decomposition of a positive matrix $P$? Of a unitary matrix $U$? Of a Hermitian matrix, $H$?
\end{exe}
\begin{sol}
    \emph{Polar decomposition of positive matrix $P$}: There exists unitary $U$ and positive operators $J$ and $K$ such that
    \begin{align}
        P=UJ=KU,
    \end{align}
    where the unique positive operators $J=\sqrt{P^{\dagger}P}$ and $K=\sqrt{PP^{\dagger}}$. Since $P$ is positive, it can be given a spectral decomposition, $P=\sum_i\lambda_i\lvert i\rangle\langle i\rvert$, where $\lambda_i$ are real and $\lambda_i\geq 0$ $\forall i$. In this way,
    \begin{align}
        J=&\sqrt{P^{\dagger}P}=\sum_i\sqrt{\lambda_i^*\lambda_i}\lvert i\rangle\langle i\rvert=\sum_i\abs{\lambda_i}\lvert i\rangle\langle i\rvert=\sum_i\lambda_i\lvert i\rangle\langle i\rvert=P,\\
        K=&\sqrt{PP^{\dagger}}=\sum_i\sqrt{\lambda_i\lambda_i^*}\lvert i\rangle\langle i\rvert=\sum_i\abs{\lambda_i}\lvert i\rangle\langle i\rvert=\sum_i\lambda_i\lvert i\rangle\langle i\rvert=P.
    \end{align}
    If $P$ is invertible, i.e., $P$ is positive definite, then so is $J$, so $U=PJ^{-1}=I$ is unique.\\
    \emph{Polar decomposition of unitary matrix $U$}: There exists unitary $A$ and positive operators $J$ and $K$ such that
    \begin{align}
        U=AJ=KA,
    \end{align}
    where the unique positive operators $J=\sqrt{U^{\dagger}U}=I$ and $K=\sqrt{KK^{\dagger}}=I$. Hence $A=UJ^{-1}=U$ is unique.\\
    \emph{Polar decomposition of Hermitian matrix $H$}: There exists unitary $U$ and positive operators $J$ and $K$ such that
    \begin{align}
        H=UJ=KU,
    \end{align}
    where the unique positive operators $J=\sqrt{H^{\dagger}H}$ and $K=\sqrt{HH^{\dagger}}$. Since $H$ is Hermitian, it is normal. According to spectral decomposition theorem, $H$ is diagonal with respect to some orthonormal basis, i.e.,
    \begin{align}
        H=A\Lambda A^{\dagger},
    \end{align}
    where $\Lambda$ is the matrix of eigenvalues of $H$ and $A$ is unitary. In this way,
    \begin{align}
        J=&\sqrt{H^{\dagger}H}=\sqrt{HH}=A\sqrt{\Lambda^2}A^{\dagger},\\
        K=&\sqrt{HH^{\dagger}}=\sqrt{HH}=A\sqrt{\Lambda^2}A^{\dagger}.
    \end{align}
    If $H$ is invertible, then $U=HJ^{-1}$ is unique.
\end{sol}

\begin{exe}
    Express the polar decomposition of a normal matrix in the outer product representation.
\end{exe}
\begin{sol}
    \emph{Polar decomposition of normal matrix in the outer products representation}: For a normal matrix $M$, there exists unitary $U$ and positive operators $J$ and $K$ such that
    \begin{align}
        M=UJ=KU,
    \end{align}
    where the unique operators $J=\sqrt{M^{\dagger}M}$ and $K=\sqrt{MM^{\dagger}}$. In the outer product representation,
    \begin{align}
        M=\sum_i\lambda_i\lvert i\rangle\langle i\rvert,
    \end{align}
    so
    \begin{align}
        J=&\sum_i\sqrt{\lambda_i^*\lambda_i}\lvert i\rangle\langle i\rvert=\sum_i\abs{\lambda_i}\lvert i\rangle\langle i\rvert,\\
        K=&\sum_i\sqrt{\lambda_i\lambda_i^*}\lvert i\rangle\langle i\rvert=\sum_i\abs{\lambda_i}\lvert i\rangle\langle i\rvert.
    \end{align}
    If $M$ is invertible, then
    \begin{align}
        U=MJ^{-1}=\left(\sum_i\lambda_i\lvert i\rangle\langle i\rvert\right)\left(\sum_j\abs{\lambda_j}^{-1}\lvert j\rangle\langle j\rvert\right)=\sum_i\frac{\lambda_i}{\abs{\lambda_i}}\lvert i\rangle\langle i\rvert
    \end{align}
    is also unique.
\end{sol}

\begin{exe}
    Find the left and right polar decomposition of the matrix
    \begin{align}
        \left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right].
    \end{align}
\end{exe}
\begin{sol}
    Left polar decomposition:
    \begin{align}
        \left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]=UJ,
    \end{align}
    where
    \begin{align}
        J=\sqrt{\left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]^{\dagger}\left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]}.
    \end{align}
    Since
    \begin{align}
        \left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]^{\dagger}\left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]=\left[\begin{matrix}
            1&1\\
            0&1
        \end{matrix}\right]\left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]=\left[\begin{matrix}
            2&1\\
            1&1
        \end{matrix}\right],
    \end{align}
    whose characteristic equation
    \begin{align}
        \left\lvert\begin{matrix}
            2-\lambda&1\\
            1&1-\lambda
        \end{matrix}\right\rvert=\lambda^2-3\lambda+1=0
    \end{align}
    gives eigenvalues
    \begin{align}
        \lambda_{1,2}=\frac{3\pm\sqrt{5}}{2},
    \end{align}
    and eigenequations
    \begin{align}
        \left[\begin{matrix}
            2&1\\
            1&1
        \end{matrix}\right]\lvert v_1\rangle=&\left[\begin{matrix}
            2&1\\
            1&1
        \end{matrix}\right]\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right]=\left[\begin{matrix}
            2a_1+b_1\\
            a_1+b_1
        \end{matrix}\right]=\lambda_1\lvert v_1\rangle=\frac{3+\sqrt{5}}{2}\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right],\\
        \left[\begin{matrix}
            2&1\\
            1&1
        \end{matrix}\right]\lvert v_2\rangle=&\left[\begin{matrix}
            2&1\\
            1&1
        \end{matrix}\right]\left[\begin{matrix}
            a_2\\
            b_2
        \end{matrix}\right]=\left[\begin{matrix}
            2a_2+b_2\\
            a_2+b_2
        \end{matrix}\right]=\lambda_2\lvert v_2\rangle=\frac{3-\sqrt{5}}{2}\left[\begin{matrix}
            a_2\\
            b_2
        \end{matrix}\right]
    \end{align}
    give the corresponding eigenvectors
    \begin{align}
        \lvert v_1\rangle=\left[\begin{matrix}
            \sqrt{\frac{5+\sqrt{5}}{10}}\\
            \sqrt{\frac{5-\sqrt{5}}{10}}
        \end{matrix}\right],\quad\lvert v_2\rangle=\left[\begin{matrix}
            \sqrt{\frac{5-\sqrt{5}}{10}}\\
            -\sqrt{\frac{5+\sqrt{5}}{10}}
        \end{matrix}\right],
    \end{align}
    we have
    \begin{align}
        J=\sqrt{\frac{3+\sqrt{5}}{2}}\lvert v_1\rangle\langle v_1\rvert+\sqrt{\frac{3-\sqrt{5}}{2}}\lvert v_2\rangle\langle v_2\rvert\approx\left[\begin{matrix}
            1.3416&0.4472\\
            0.4472&0.8944
        \end{matrix}\right],
    \end{align}
    and
    \begin{align}
        U=\left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]J^{-1}\approx\left[\begin{matrix}
            0.8944&-0.4472\\
            0.4472&0.8944
        \end{matrix}\right].
    \end{align}
    Right polar decomposition:
    \begin{align}
        \left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]=KU,
    \end{align}
    where
    \begin{align}
        K=\sqrt{\left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]\left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]^{\dagger}}.
    \end{align}
    Since
    \begin{align}
        \left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]\left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]^{\dagger}=\left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]\left[\begin{matrix}
            1&1\\
            0&1
        \end{matrix}\right]=\left[\begin{matrix}
            1&1\\
            1&2
        \end{matrix}\right],
    \end{align}
    whose characteristic equation
    \begin{align}
        \left\lvert\begin{matrix}
            1-\lambda&1\\
            1&2-\lambda
        \end{matrix}\right\rvert=\lambda^2-3\lambda+1=0
    \end{align}
    gives eigenvalues
    \begin{align}
        \lambda_{1,2}=\frac{3\pm\sqrt{5}}{2},
    \end{align}
    and eigenequations
    \begin{align}
        \left[\begin{matrix}
            1&1\\
            1&2
        \end{matrix}\right]\lvert v_1\rangle=&\left[\begin{matrix}
            1&1\\
            1&2
        \end{matrix}\right]\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right]=\left[\begin{matrix}
            a_1+b_1\\
            a_1+2b_1
        \end{matrix}\right]=\lambda_1\lvert v_1\rangle=\frac{3+\sqrt{5}}{2}\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right],\\
        \left[\begin{matrix}
            1&1\\
            1&2
        \end{matrix}\right]\lvert v_2\rangle=&\left[\begin{matrix}
            1&1\\
            1&2
        \end{matrix}\right]\left[\begin{matrix}
            a_2\\
            b_2
        \end{matrix}\right]=\left[\begin{matrix}
            a_2+b_2\\
            a_2+2b_2
        \end{matrix}\right]=\lambda_2\lvert v_2\rangle=\frac{3-\sqrt{5}}{2}\left[\begin{matrix}
            a_2\\
            b_2
        \end{matrix}\right],
    \end{align}
    give the corresponding eigenvectors
    \begin{align}
        \lvert v_1\rangle=\left[\begin{matrix}
            \sqrt{\frac{5-\sqrt{5}}{10}}\\
            \sqrt{\frac{5+\sqrt{5}}{10}}
        \end{matrix}\right],\quad\lvert v_2\rangle=\left[\begin{matrix}
            \sqrt{\frac{5+\sqrt{5}}{10}}\\
            -\sqrt{\frac{5-\sqrt{5}}{10}}
        \end{matrix}\right],
    \end{align}
    we have
    \begin{align}
        K=\sqrt{\frac{3+\sqrt{5}}{2}}\lvert v_1\rangle\langle v_1\rvert+\sqrt{\frac{3-\sqrt{5}}{2}}\lvert v_2\rangle\langle v_2\rvert\approx\left[\begin{matrix}
            0.8944&0.4472\\
            0.4472&1.3416
        \end{matrix}\right].
    \end{align}
\end{sol}

\section{The postulates of quantum mechanics}

\begin{exe}
    Verify that the Hadamard gate $H$ is unitary.
\end{exe}
\begin{pf}
    Since
    \begin{align}
        H^{\dagger}H=\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1&1\\
            1&-1
        \end{matrix}\right]^{\dagger}\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1&1\\
            1&-1
        \end{matrix}\right]=\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1&1\\
            1&-1
        \end{matrix}\right]\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1&1\\
            1&-1
        \end{matrix}\right]=\left[\begin{matrix}
            1&0\\
            0&1
        \end{matrix}\right]=I,
    \end{align}
    the Hadamard gate $H$ is unitary.
\end{pf}

\begin{exe}
    Verify that $H^2=I$.
\end{exe}
\begin{pf}
    \begin{align}
        H^2=\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1&1\\
            1&-1
        \end{matrix}\right]\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1&1\\
            1&-1
        \end{matrix}\right]=\left[\begin{matrix}
            1&0\\
            0&1
        \end{matrix}\right]=I.
    \end{align}
\end{pf}

\begin{exe}
    What are the eigenvalues and eigenvectors of $H$?
\end{exe}
\begin{sol}
    The characteristic equation of $H$
    \begin{align}
        \left\lvert\begin{matrix}
            \frac{1}{\sqrt{2}}-\lambda&\frac{1}{\sqrt{2}}\\
            \frac{1}{\sqrt{2}}&-\frac{1}{\sqrt{2}}-\lambda
        \end{matrix}\right\rvert=\lambda^2-1=0
    \end{align}
    gives the eigenvalues
    \begin{align}
        \lambda_{1,2}=\pm 1.
    \end{align}
    The eigenequations of $H$
    \begin{align}
        H\lvert v_1\rangle=&\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1&1\\
            1&-1
        \end{matrix}\right]\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right]=\frac{1}{\sqrt{2}}\left[\begin{matrix}
            a_1+b_1\\
            a_1-b_1
        \end{matrix}\right]=\lambda_1\lvert v_1\rangle=\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right],\\
        H\lvert v_2\rangle=&\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1&1\\
            1&-1
        \end{matrix}\right]\left[\begin{matrix}
            a_2\\
            b_2
        \end{matrix}\right]=\frac{1}{\sqrt{2}}\left[\begin{matrix}
            a_2+b_2\\
            a_2-b_2
        \end{matrix}\right]=\lambda_2\lvert v_2\rangle=\left[\begin{matrix}
            -a_2\\
            -b_2
        \end{matrix}\right],
    \end{align}
    give the corresponding eigenvectors
    \begin{align}
        \lvert v_1\rangle=\left[\begin{matrix}
            \sqrt{\frac{2+\sqrt{2}}{2}}\\
            \sqrt{\frac{2-\sqrt{2}}{2}}
        \end{matrix}\right],\quad\lvert v_2\rangle=\left[\begin{matrix}
            \sqrt{\frac{2-\sqrt{2}}{4}}\\
            -\sqrt{\frac{2+\sqrt{2}}{4}}
        \end{matrix}\right].
    \end{align}
\end{sol}

\begin{exe}
    Suppose $A$ and $B$ are commuting Hermitian operators. Prove that $\exp(A)\exp(B)=\exp(A+B)$. (\emph{Hint}: Use the results of Section 2.1.9.)
\end{exe}
\begin{pf}
    Since $A$ and $B$ commute,
    \begin{align}
        [A,B]=AB-BA=0\Longrightarrow AB=BA.
    \end{align}
    Hence
    \begin{align}
        \notag\exp(A+B)=&\sum_{n=0}^{\infty}\frac{(X+Y)^n}{n!}=\sum_{n=0}^{\infty}\sum_{m=0}^n\left(\begin{matrix}
            n\\
            m
        \end{matrix}\right)\frac{A^mB^{n-m}}{n!}=\sum_{n=0}^{\infty}\sum_{m=0}^n\frac{A^mB^{n-m}}{m!(n-m)!}=\sum_{j=0}^{\infty}\sum_{k=0}^{\infty}\frac{A^jB^k}{j!k!}\\
        =&\left(\sum_{j=0}^{\infty}\frac{A^j}{j!}\right)\left(\sum_{k=0}^{\infty}\frac{B^k}{k!}\right)=\exp(A)\exp(B).
    \end{align}
\end{pf}

\begin{exe}
    Prove that $U(t_1,t_2)$ defined in Equation (2.91) is unitary.
\end{exe}
\begin{pf}
    \begin{align}
        U(t_1,t_2)=&\exp\left[\frac{-iH(t_2-t_1)}{\hbar}\right]=\sum_{n=0}^{\infty}\frac{1}{n!}\left[\frac{-iH(t_2-t_1)}{\hbar}\right]^n,\\
        \notag[U(t_1,t_2)]^{\dagger}=&\sum_{n=0}^{\infty}\frac{1}{n!}\left[\frac{iH^{\dagger}(t_2-t_1)}{\hbar}\right]^n=\sum_{n=0}^{\infty}\frac{1}{n!}\left[\frac{iH(t_2-t_1)}{\hbar}\right]^n=\exp\left[\frac{iH(t_2-t_1)}{\hbar}\right]=\left\{\exp\left[\frac{-iH(t_2-t_1)}{\hbar}\right]\right\}^{-1}\\
        =&[U(t_1,t_2)]^{-1}.
    \end{align}
    Therefore, $U(t_1,t_2)$ is unitary.
\end{pf}

\begin{exe}
    Use the spectral decomposition to show that $K=-i\log(U)$ is Hermitian for any unitary $U$, and thus $U=\exp(iK)$ for some Hermitian $K$.
\end{exe}
\begin{pf}
    The spectral decomposition of $U$ in terms of outer product representation is
    \begin{align}
        U=\sum_n\lambda_n\lvert n\rangle\langle n\rvert.
    \end{align}
    Since $U$ is unitary,
    \begin{align}
        U^{\dagger}=\sum_n\lambda_n^*\lvert n\rangle\langle n\rvert=\sum_n\lambda_n^{-1}\lvert n\rangle\langle n\rvert=U^{-1},
    \end{align}
    we have
    \begin{align}
        \lambda_n^*=\lambda_n^{-1},\quad\forall n.
    \end{align}
    The spectral decomposition of $K$ is
    \begin{align}
        K=-i\log U=\sum_n-i\log\lambda_i\lvert n\rangle\langle n\rvert.
    \end{align}
    Since
    \begin{align}
        K^{\dagger}=\sum_n(-i\log\lambda_n)^*\lvert n\rangle\langle n\rvert=\sum_ni\log(\lambda_n^*)\lvert n\rangle\langle n\rvert=\sum_ni\log(\lambda_n^{-1})\lvert n\rangle\langle n\rvert=\sum_n-i\log\lambda_n\lvert n\rangle\langle n\rvert=K,
    \end{align}
    $K$ is Hermitian. Hence $U=\exp(iK)$ for some Hermitian $K$.
\end{pf}

\begin{exe}[Cascaded measurements are single measurements]
    Suppose $\{L_l\}$ and $\{M_m\}$ are two sets of measurement operators. Show that a measurement defined by the measurement operators $\{L_l\}$ followed by a measurement defined by the measurement operators $\{M_m\}$ is physically equivalent to a single measurement defined by measurement operators $\{N_{lm}\}$ with the representation $N_{ml}=M_mL_l$.
\end{exe}
\begin{pf}
    Suppose the state of the quantum state is $\lvert\psi\rangle$ immediately before the measurement. Then the probability that the result $m$ occurs in the first measurement is
    \begin{align}
        p(l)=\langle\psi\rvert L_l^{\dagger}L_l\lvert\psi\rangle,
    \end{align}
    and the state of the system after the measurement is
    \begin{align}
        \frac{L_l\lvert\psi\rangle}{\sqrt{\langle\psi\rvert L_l^{\dagger}L_l\lvert\psi\rangle}}.
    \end{align}
    The probability that the result $l$ occurs in the second measurement conditional on that the result $m$ occurs in the first measurement is
    \begin{align}
        p(m\vert l)=\frac{\langle\psi\rvert L_l^{\dagger}}{\sqrt{\langle\psi\rvert L_l^{\dagger}L_l\lvert\psi\rangle}}M_m^{\dagger}M_m\frac{L_l\lvert\psi\rangle}{\sqrt{\langle\psi\rvert L_l^{\dagger}L_l\lvert\psi\rangle}}=\frac{\langle\psi\rvert L_l^{\dagger}M_m^{\dagger}M_mL_l\lvert\psi\rangle}{\langle\psi\rvert L_l^{\dagger}L_l\lvert\psi\rangle},
    \end{align}
    and the state of the system after the two measurements is
    \begin{align}
        \frac{M_mL_l\lvert\psi\rangle}{\sqrt{\frac{\langle\psi\rvert L_l^{\dagger}M_m^{\dagger}M_mL_l\lvert\psi\rangle}{\langle\psi\rvert L_l^{\dagger}L_l\lvert\psi\rangle}}}
    \end{align}
    Hence the probability that the result $m$ occurs in the first measurement and the result $l$ occurs in the second measurement is
    \begin{align}
        p(l,m)=p(l)p(m\vert l)=\langle\psi\rvert L_l^{\dagger}M_m^{\dagger}M_mL_l\lvert\psi\rangle.
    \end{align}
    Therefore, a measurement defined by the measurement operators $\{L_l\}$ followed by a measurement defined by the measurement $\{M_m\}$ is physically equivalent to a single measurement defined by measurement operators $\{N_{lm}\}$ with the representation $N_{ml}=M_mL_l$.
\end{pf}

\begin{exe}
    Suppose we prepare a quantum system in an eigenstate $\lvert\psi\rangle$ of some observable $M$, with corresponding eigenvalue $m$. What is the average observed value of $M$, and the standard deviation?
\end{exe}
\begin{sol}
    The average observed value of $M$ is
    \begin{align}
        \langle M\rangle=\langle\psi\rvert M\lvert\psi\rangle=\langle\psi\rvert m\lvert\rangle=m.
    \end{align}
    The average observed value of $M^2$ is
    \begin{align}
        \langle M^2\rangle=\langle\psi\rvert M^2\lvert\psi\rangle=\langle\psi\rvert m^2\lvert\psi\rangle=m^2.
    \end{align}
    The standard deviation of $M$ is
    \begin{align}
        [\Delta(M)]^2=\langle M^2\rangle-\langle M\rangle^2=0.
    \end{align}
\end{sol}

\begin{exe}
    Suppose we have qubit in the state $\lvert 0\rangle$, and we measure the observable $X$. What is the average value of $X$? What is the standard deviation of $X$?
\end{exe}
\begin{sol}
    The average value of $X$ is
    \begin{align}
        \langle 0\rvert X\lvert 0\rangle=\left[\begin{matrix}
            1&0
        \end{matrix}\right]\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]\left[\begin{matrix}
            1\\
            0
        \end{matrix}\right]=0.
    \end{align}
    The average value of $X^2$ is
    \begin{align}
        \langle 0\rvert X^2\lvert 0\rangle=\left[\begin{matrix}
            1&0
        \end{matrix}\right]\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]^2\left[\begin{matrix}
            1\\
            0
        \end{matrix}\right]=1.
    \end{align}
    The standard deviation of $X$ is
    \begin{align}
        [\Delta(X)]^2=\langle X^2\rangle-\langle X\rangle^2=1.
    \end{align}
\end{sol}

\begin{exe}
    Show that $\vec{v}\cdot\vec{\sigma}$ has eigenvalues $\pm 1$, and that the projectors onto the corresponding eigenspaces are given by $P_{\pm}=(I\pm\vec{v}\cdot\vec{\sigma})/2$.
\end{exe}
\begin{pf}
    \begin{align}
        \vec{v}\cdot\vec{\sigma}=v_1\sigma_1+v_2\sigma_2+v_3\sigma_3=v_1\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]+v_2\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]+v_3\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]=\left[\begin{matrix}
            v_3&v_1-iv_2\\
            v_1+iv_2&-v_3
        \end{matrix}\right].
    \end{align}
    The characteristic equation of $\vec{v}\cdot\vec{\sigma}$
    \begin{align}
        \lvert\vec{v}\cdot\vec{\sigma}-\lambda I\rvert=\left\lvert\begin{matrix}
            v_3-\lambda&v_1-iv_2\\
            v_1+iv_2&-v_3-\lambda
        \end{matrix}\right\rvert=\lambda^2-v_1^2-v_2^2-v_3^2=\lambda^2-1=0
    \end{align}
    gives the eigenvalues
    \begin{align}
        \lambda_{1,2}=\pm 1.
    \end{align}
    The eigenequations
    \begin{align}
        (\vec{v}\cdot\vec{\sigma})\lvert v_1\rangle=&\left[\begin{matrix}
            v_3&v_1-iv_2\\
            v_1+iv_2&-v_3
        \end{matrix}\right]\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right]=\left[\begin{matrix}
            v_3a_1+(v_1-iv_2)b_1\\
            (v_1+iv_2)a_1-v_3b_1
        \end{matrix}\right]=\lambda_1\lvert v_1\rangle=\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right],\\
        (\vec{v}\cdot\vec{\sigma})\lvert v_2\rangle=&\left[\begin{matrix}
            v_3&v_1-iv_2\\
            v_1+iv_2&-v_3
        \end{matrix}\right]\left[\begin{matrix}
            a_2\\
            b_2
        \end{matrix}\right]=\left[\begin{matrix}
            v_3a_2+(v_1-iv_2)b_2\\
            (v_1+iv_2)a_2-v_3b_2
        \end{matrix}\right]=\lambda_2\lvert v_2\rangle=\left[\begin{matrix}
            -a_2\\
            -b_2
        \end{matrix}\right]
    \end{align}
    give the corresponding eigenvectors
    \begin{align}
        \lvert v_1\rangle=\frac{1}{\sqrt{2(1+v_3)}}\left[\begin{matrix}
            1+v_3\\
            v_1+iv_2
        \end{matrix}\right],\quad\lvert v_2\rangle=\frac{1}{\sqrt{2(1-v_3)}}\left[\begin{matrix}
            v_3-1\\
            v_1+iv_2
        \end{matrix}\right].
    \end{align}
    The projectors onto the corresponding eigensapces are
    \begin{align}
        \notag P_+=&\lvert v_1\rangle\langle v_1\rvert=\frac{1}{\sqrt{2(1+v_3)}}\left[\begin{matrix}
            1+v_3\\
            v_1+iv_2
        \end{matrix}\right]\frac{1}{\sqrt{2(1+v_3)}}\left[\begin{matrix}
            1+v_3&v_1-iv_2
        \end{matrix}\right]\\
        \notag=&\frac{1}{2(1+v_3)}\left[\begin{matrix}
            (1+v_3)^2&(1+v_3)(v_1-iv_2)\\
            (v_1+iv_2)(1+v_3)&(v_1+iv_2)(v_1-iv_2)
        \end{matrix}\right]=\frac{1}{2(1+v_3)}\left[\begin{matrix}
            (1+v_3)^2&(1+v_3)(v_1-iv_2)\\
            (v_1+iv_2)(1+v_3)&(1+v_3)(1-v_3)
        \end{matrix}\right]\\
        =&\frac{1}{2}\left[\begin{matrix}
            1+v_3&v_1-iv_2\\
            v_1+iv_2&1-v_3
        \end{matrix}\right]=(I+\vec{v}\cdot\vec{\sigma})/2,\\
        \notag P_-=&\lvert v_2\rangle\langle v_2\rvert=\frac{1}{\sqrt{2(1-v_3)}}\left[\begin{matrix}
            v_3-1\\
            v_1+iv_2
        \end{matrix}\right]\frac{1}{\sqrt{2(1-v_3)}}\left[\begin{matrix}
            v_3-1&v_1-iv_2
        \end{matrix}\right]\\
        \notag=&\frac{1}{2(1-v_3)}\left[\begin{matrix}
            (v_3-1)^2&(v_3-1)(v_1-iv_2)\\
            (v_1+iv_2)(v_3-1)&(v_1+iv_2)(v_1-iv_2)
        \end{matrix}\right]=\frac{1}{2(1-v_3)}\left[\begin{matrix}
            (1-v_3)^2&-(1-v_3)(v_1-iv_2)\\
            -(v_1+iv_2)(1-v_3)&(1+v_3)(1-v_3)
        \end{matrix}\right]\\
        =&\frac{1}{2}\left[\begin{matrix}
            1-v_3&-(v_1-iv_2)\\
            v_1+iv_2&1+v_3
        \end{matrix}\right]=(I-\vec{v}\cdot\vec{\sigma})/2.
    \end{align}
\end{pf}

\begin{exe}
    Calculate the probability of obtaining the result $+1$ for a measurement of $\vec{v}\cdot\vec{\sigma}$, given that the state priori to measurement is $\lvert 0\rangle$. What is the state of the system after the measurement if $+1$ is obtained?
\end{exe}
\begin{sol}
    The probability of obtaining $+1$ for a measurement of $\vec{v}\cdot\vec{\sigma}$ is
    \begin{align}
        p(1)=\langle 0\rvert P_+\lvert 0\rangle=\left[\begin{matrix}
            1&0
        \end{matrix}\right]\frac{1}{2}\left[\begin{matrix}
            1-v_3&-(v_1-iv_2)\\
            v_1+iv_2&1+v_3
        \end{matrix}\right]\left[\begin{matrix}
            1\\
            0
        \end{matrix}\right]=\frac{1-v_3}{2}.
    \end{align}
    The state of the system after the measurement if $+1$ is obtained is
    \begin{align}
        \lvert v_1\rangle=\frac{1}{\sqrt{2(1+v_3)}}\left[\begin{matrix}
            1+v_3\\
            v_1+iv_2
        \end{matrix}\right].
    \end{align}
\end{sol}

\begin{exe}
    Show that any measurement where the measurement operators and the POVM elements coincide is a projective measurement.
\end{exe}
\begin{pf}
    Consider a measurement whose measurement operators $\{M_m\}$ coincide with POVM elements $\{E_m=M_m^{\dagger}M_m\}$,
    \begin{align}
        E_m=M_m^{\dagger}M_m=M_m.
    \end{align}
    Since
    \begin{align}
        M_m=E_m^{\dagger}=M_m^{\dagger}M_m=E_m=M_m,
    \end{align}
    $\{M_m\}$ are Hermitian. Hence
    \begin{align}
        M_m^{\dagger}M_m=M_m^2=M_m,
    \end{align}
    $\{M_m\}$ are projectors and this measurement is a projective measurement.
\end{pf}

\begin{exe}
    Suppose a measurement is described by measurement operators $M_m$. Show that there exists unitary operators $U_m$ such that $M_m=U_m\sqrt{E_m}$, where $E_m$ is the POVM associated to the measurement.
\end{exe}
\begin{pf}
    Since $U_m$ is unitary and the POVM measurement operators $E_m$ is Hermitian,
    \begin{align}
        M_m^{\dagger}M_m=\sqrt{E_m^{\dagger}}U_m^{\dagger}U_m\sqrt{E_m}=\sqrt{E_m}U_m^{-1}U_m\sqrt{E_m}=E_m.
    \end{align}
    Therefore, there exists unitary operators $U_m$ such that $M_m=U_m\sqrt{E_m}$, where $E_m$ is the POVM associated to the measurement.
\end{pf}

\begin{exe}
    Suppose Bob is given a quantum state chosen from a set $\lvert\psi_1\rangle,\cdots,\lvert\psi_m\rangle$ of linearly independent states. Construct a POVM $\{E_1,E_2,\cdots,E_{m+1}\}$ such that if outcome $E_i$ occurs, $1\leq i\leq m$, then Bob knows with certainty that he was given the state $\lvert\psi_i\rangle$. (The POVM must be such that $\langle\psi_i\rvert E_i\lvert\psi_i\rangle>0$ for each $i$.)
\end{exe}
\begin{sol}
    For each $1\leq i\leq m$, using Gram-Schmidt procedure to produce the orthonormal basis set $\lvert\phi_1^{(i)}\rangle,\lvert\phi_2^{(i)}\rangle,\cdots,\lvert\phi_{i-1}^{(i)}\rangle,\lvert\phi_{i+1}^{(i)}\rangle,\cdots,\lvert\phi_m\rangle$ from $\lvert\psi_1\rangle,\lvert\psi_2\rangle,\cdots,\lvert\psi_{i-1}\rangle,\lvert\psi_{i+1}\rangle,\cdots,\lvert\psi_m\rangle$, and then
    \begin{align}
        E_m=\frac{\left(\lvert\psi_i\rangle-\sum_{j\neq i}\langle\psi_i\vert\phi_j^{(i)}\rangle\lvert\phi_j^{(i)}\rangle\right)\left(\langle\psi_i\rvert-\sum_{j\neq i}\langle\phi_j^{(i)}\vert\psi_i\rangle\langle\phi_j^{(i)}\rvert\right)}{\abs{\lvert\psi_i\rangle-\sum_{j\neq i}\langle\psi_i\vert\phi_j^{(i)}\rangle\lvert\phi_j^{(i)}\rangle}^2}.
    \end{align}
    For $i=m+1$,
    \begin{align}
        E_{m+1}=I-\sum_{i=1}^mE_m.
    \end{align}
\end{sol}

\begin{exe}
    Express the states $(\lvert 0\rangle+\lvert 1\rangle)/\sqrt{2}$ and $(\lvert 0\rangle-\lvert 1\rangle)/\sqrt{2}$ in a basis in which they are not the same up to a relative phase shift.
\end{exe}
\begin{sol}
    In the basis $\{\lvert 0\rangle,\lvert 1\rangle\}$,
    \begin{align}
        \frac{\lvert 0\rangle+\lvert 1\rangle}{\sqrt{2}}=&\frac{1}{\sqrt{2}}\lvert 0\rangle+\frac{1}{\sqrt{2}}\lvert 1\rangle,\\
        \frac{\lvert 0\rangle-\lvert 1\rangle}{\sqrt{2}}=&\frac{1}{\sqrt{2}}\lvert 0\rangle+\frac{1}{\sqrt{2}}e^{i\pi}\lvert 1\rangle,
    \end{align}
    which are not the same up to a relative phase shift.
\end{sol}

\begin{exe}
    Show that the average value of the observable $X_1Z_2$ for a two qubit system measured in the state $(\lvert 00\rangle+\lvert 11\rangle)/\sqrt{2}$ is zero.
\end{exe}
\begin{pf}
    The average value of the observable $X_1Z_2$ for a two qubit system measured in the state $(\lvert 00\rangle+\lvert 11\rangle)/\sqrt{2}$ is
    \begin{align}
        \frac{\langle 00\rvert+\langle 11\rvert}{\sqrt{2}}X_1Z_2\frac{\lvert 00\rangle+\lvert 11\rangle}{\sqrt{2}}=\frac{\langle 00\rvert+\langle 11\rvert}{\sqrt{2}}X_1\frac{\lvert 00\rangle-\lvert 11\rangle}{\sqrt{2}}=\frac{\langle 00\rvert+\langle 11\rvert}{\sqrt{2}}\frac{\lvert 10\rangle-\lvert 01\rangle}{\sqrt{2}}=0.
    \end{align}
\end{pf}

\begin{exe}
    Suppose $V$ is Hilbert space with a subspace $W$. Suppose $U:W\rightarrow V$ is linear operator which preserves inner products, that is, for any $\lvert w_1\rangle$ and $\lvert w_2\rangle$ in $W$,
    \begin{align}
        \langle w_1\rvert U^{\dagger}U\lvert w_2\rangle=\langle w_1\vert w_2\rangle.
    \end{align}
    Prove that there exists a unitary operator $U':V\rightarrow V$ which extends $U$. That is, $U'\lvert w\rangle=U\lvert w\rangle$ for all $\lvert w\rangle$ in $W$, but $U'$ is defined on the entire space $V$. Usually we omit the prime symbol $'$ and just write $U$ to decode the extension.
\end{exe}
\begin{pf}
    Define
    \begin{align}
        U'\equiv U\otimes A,
    \end{align}
    where the unitary operator $A:U-W\rightarrow\im(U)^{\perp}$.
    Since both $U$ and $A$ are unitary, $U'$ is unitary.
    For all $\lvert w\rangle$ in $V$, we can express it as
    \begin{align}
        \lvert w\rangle=\lvert w_1\rangle\otimes\lvert w_2\rangle,
    \end{align}
    where $\lvert w_1\rangle$ and $\lvert w_2\rangle$ are in the image of $U$, $\im(U)$, and the complement of the image of U, $\im(U)^{\perp}=W-\im(U)$, respectively, and then
    \begin{align}
        U'\lvert w\rangle=(U\otimes A)(\lvert w_1\rangle\otimes\lvert w_2\rangle)=(U\lvert w_1\rangle)\otimes(A\lvert w_2\rangle).
    \end{align}
    Specially, for all $\lvert w\rangle$ in $W$, we can extend it as
    \begin{align}
        \lvert w'\rangle=\lvert w\rangle\otimes\lvert 0\rangle\in V
    \end{align}
    where $\im(U)^{\perp}$ is the zero vector in $V-W$,
    and then
    \begin{align}
        U'\lvert w'\rangle=(U\otimes A)(\lvert w\rangle\otimes\lvert 0\rangle)=U\lvert w\rangle\otimes\lvert 0\rangle
    \end{align}
    Therefore, there exists a unitary $U':V\rightarrow V$ which extends $U$.
\end{pf}

\begin{exe}
    Prove that $\lvert\psi\rangle\neq\lvert a\rangle\lvert b\rangle$ for all single qubit states $\lvert a\rangle$ and $\lvert b\rangle$.
\end{exe}
\begin{pf}
    Suppose
    \begin{align}
        \lvert a\rangle=&a_0\lvert 0\rangle+a_1\lvert 1\rangle,\\
        \lvert b\rangle=&b_0\lvert 0\rangle+b_1\lvert 1\rangle,
    \end{align}
    where the normalization condition requires that
    \begin{align}
        \abs{a_0}^2+\abs{a_1}^2=&1,\\
        \abs{b_0}^2+\abs{b_1}^2=&1.
    \end{align}
    If
    \begin{align}
        \label{E2.68}
        \frac{\lvert 00\rangle+\lvert 11\rangle}{\sqrt{2}}=\lvert\psi\rangle=\lvert a\rangle\lvert b\rangle=a_0b_0\lvert 00\rangle+a_0b_1\lvert 01\rangle+a_1b_0\lvert 10\rangle+a_1b_1\lvert 11\rangle,
    \end{align}
    then
    \begin{align}
        \label{E2.68-1}a_0b_0=&\frac{1}{\sqrt{2}},\\
        \label{E2.68-2}a_0b_1=&0,\\
        \label{E2.68-3}a_1b_0=&0,\\
        \label{E2.68-4}a_1b_1=&\frac{1}{\sqrt{2}}.
    \end{align}
    Equation \eqref{E2.68-1} and \eqref{E2.68-4} means that none of $a_0$, $a_1$, $b_0$ and $b_1$ equals $0$, which conflicts with equation \eqref{E2.68-2} and \eqref{E2.68-3}.
    Therefore, equation \eqref{E2.68} is impossible and $\lvert\psi\rangle\neq\lvert a\rangle\lvert b\rangle$ for all single qubit states $\lvert a\rangle$ and $\lvert b\rangle$.
\end{pf}

\section{Application: superdense coding}

\begin{exe}
    Verify that the Bell basis forms an orthonormal basis for the two qubit state space.
\end{exe}
\begin{pf}
    The Bell basis satisfies that:
    \begin{itemize}
        \item[(a)] The Bell basis is normalized,
        \begin{align}
            \abs{\lvert\beta_{00}\rangle}=&\sqrt{\langle\beta_{00}\vert\beta_{00}\rangle}=\sqrt{\frac{\langle 00\rvert+\langle 00\rvert}{\sqrt{2}}\frac{\lvert 00\rangle+\lvert 11\rangle}{\sqrt{2}}}=1,\\
            \abs{\lvert\beta_{01}\rangle}=&\sqrt{\langle\beta_{01}\vert\beta_{01}\rangle}=\sqrt{\frac{\langle 01\rvert+\langle 10\rvert}{\sqrt{2}}\frac{\lvert 01\rangle+\lvert 10\rangle}{\sqrt{2}}}=1,\\
            \abs{\lvert\beta_{10}\rangle}=&\sqrt{\langle\beta_{10}\vert\beta_{10}\rangle}=\sqrt{\frac{\langle 00\rvert-\langle 11\rvert}{\sqrt{2}}\frac{\lvert 00\rangle-\lvert 11\rangle}{\sqrt{2}}}=1,\\
            \abs{\lvert\beta_{11}\rangle}=&\sqrt{\langle\beta_{11}\vert\beta_{11}\rangle}=\sqrt{\frac{\langle 01\rvert-\langle 10\rvert}{\sqrt{2}}\frac{\lvert 01\rangle-\lvert 10\rangle}{\sqrt{2}}}=1.
        \end{align}
        \item[(b)] The Bell basis is orthogonal,
        \begin{align}
            \langle\beta_{00}\vert\beta_{01}\rangle=&\frac{\langle 00\rvert+\langle 11\rvert}{\sqrt{2}}\frac{\lvert 01\rangle+\lvert 10\rangle}{\sqrt{2}}=0,\\
            \langle\beta_{00}\vert\beta_{10}\rangle=&\frac{\langle 00\rvert+\langle 11\rvert}{\sqrt{2}}\frac{\lvert 00\rangle-\lvert 11\rangle}{\sqrt{2}}=0,\\
            \langle\beta_{00}\vert\beta_{11}\rangle=&\frac{\langle 00\rvert+\langle 11\rvert}{\sqrt{2}}\frac{\lvert 01\rangle-\lvert 10\rangle}{\sqrt{2}}=0,\\
            \langle\beta_{01}\vert\beta_{10}\rangle=&\frac{\langle 01\rvert+\langle 10\rvert}{\sqrt{2}}\frac{\lvert 00\rangle-\lvert 11\rangle}{\sqrt{2}}=0,\\
            \langle\beta_{01}\vert\beta_{11}\rangle=&\frac{\langle 01\rvert+\langle 10\rvert}{\sqrt{2}}\frac{\lvert 01\rangle-\lvert 10\rangle}{\sqrt{2}}=0,\\
            \langle\beta_{10}\vert\beta_{11}\rangle=&\frac{\langle 00\rvert-\langle 11\rvert}{\sqrt{2}}\frac{\lvert 01\rangle-\lvert 10\rangle}{\sqrt{2}}=0,
        \end{align}
        and thus also linearly independent.
        \item[(c)] The dimension of the two qubit state space is $2\times 2=4$, the Bell basis also has $4$ independent elements, so the Bell basis can span the two qubit space.
    \end{itemize}
    Therefore, the Bell basis forms an orthonormal basis for the two qubit state space.
\end{pf}

\begin{exe}
    Suppose $E$ is any positive operator acting on Alice's qubit. Show that $\langle\psi\rvert E\otimes I\lvert\psi\rangle$ \emph{takes the same value} when $\lvert\psi\rangle$ is any of the four Bell states. Suppose some malevolent third party (`Eve') intercepts Alice's qubit on the way to Bob in the superdense coding protocol. Can Eve infer anything about which of the four possible bit strings $00$, $01$, $10$, $11$ Alice is trying to send? If so, how, or if not, why not?
\end{exe}
\begin{pf}
    $\langle\psi\rvert E\otimes I\lvert\psi\rangle$ takes the same value when $\lvert\psi\rangle$ is any of the four Bell states,
    \begin{align}
        \notag\langle\beta_{00}\rvert E\otimes I\lvert\beta_{00}\rangle=&\frac{\langle 00\rvert+\langle 11\rvert}{\sqrt{2}}\frac{\lvert 00\rangle+\lvert 11\rangle}{\sqrt{2}}=\frac{1}{2}(\langle 00\rvert E\otimes I\lvert 00\rangle+\langle 00\rvert E\otimes I\lvert 11\rangle+\langle 11\rvert E\otimes I\lvert 00\rangle+\langle 11\rvert E\otimes I\lvert 11\rangle)\\
        =&\frac{1}{2}(\langle 0\rvert E\lvert 0\rangle\langle 0\rvert I\lvert 0\rangle+\langle 0\rvert E\lvert 1\rangle\langle 0\rvert I\lvert 1\rangle+\langle 1\rvert E\lvert 0\rangle\langle 1\rvert I\lvert 0\rangle+\langle 1\rvert E\lvert 1\rangle\langle 1\rvert I\lvert 1\rangle)=\frac{1}{2}(\langle 0\rvert E\lvert 0\rangle+\langle 1\rvert E\lvert 1\rangle),\\
        \notag\langle\beta_{01}\rvert E\otimes I\lvert\beta_{01}\rangle=&\frac{\langle 01\rvert+\langle 10\rvert}{\sqrt{2}}\frac{\lvert 01\rangle+\lvert 10\rangle}{\sqrt{2}}=\frac{1}{2}(\langle 01\rvert E\otimes I\lvert 01\rangle+\langle 01\rvert E\otimes I\lvert 10\rangle+\langle 10\rvert E\otimes I\lvert 01\rangle+\langle 10\rvert E\otimes I\lvert 10\rangle)\\
        =&\frac{1}{2}(\langle 0\rvert E\lvert 0\rangle\langle 1\rvert I\lvert 1\rangle+\langle 0\rvert E\lvert 1\rangle\langle 1\rvert I\lvert 0\rangle+\langle 1\rvert E\lvert 0\rangle\langle 0\rvert I\lvert 1\rangle+\langle 1\rvert E\lvert 1\rangle\langle 0\rvert I\lvert 0\rangle)=\frac{1}{2}(\langle 0\rvert E\lvert 0\rangle+\langle 1\rvert E\lvert 1\rangle),\\
        \notag\langle\beta_{10}\rvert E\otimes I\lvert\beta_{10}\rangle=&\frac{\langle 00\rvert-\langle 11\rvert}{\sqrt{2}}\frac{\lvert 00\rangle-\lvert 11\rangle}{\sqrt{2}}=\frac{1}{2}(\langle 00\rvert E\otimes I\lvert 00\rangle-\langle 00\rvert E\otimes I\lvert 11\rangle-\langle 11\rvert E\otimes I\lvert 00\rangle+\langle 11\rvert E\otimes I\lvert 11\rangle)\\
        =&\frac{1}{2}(\langle 0\rvert E\lvert 0\rangle\langle 0\rvert I\lvert 0\rangle-\langle 0\rvert E\lvert 1\rangle\langle 0\rvert I\lvert 1\rangle-\langle 1\rvert E\lvert 0\rangle\langle 1\rvert I\lvert 0\rangle+\langle 1\rvert E\lvert 1\rangle\langle 1\rvert I\lvert 1\rangle)=\frac{1}{2}(\langle 0\rvert E\lvert 0\rangle+\langle 1\rvert E\lvert 1\rangle),\\
        \notag\langle\beta_{11}\rvert E\otimes I\lvert\beta_{11}\rangle=&\frac{\langle 01\rvert-\langle 10\rvert}{\sqrt{2}}\frac{\lvert 01\rangle-\lvert 10\rangle}{\sqrt{2}}=\frac{1}{2}(\langle 01\rvert E\otimes I\lvert 01\rangle-\langle 01\rvert E\otimes I\lvert 10\rangle-\langle 10\rvert E\otimes I\lvert 01\rangle+\langle 10\rvert E\otimes I\lvert 10\rangle)\\
        =&\frac{1}{2}(\langle 0\rvert E\lvert 0\rangle\langle 1\rvert I\lvert 1\rangle-\langle 0\rvert E\lvert 1\rangle\langle 1\rvert I\lvert 0\rangle-\langle 1\rvert E\lvert 0\rangle\langle 0\rvert I\lvert 1\rangle+\langle 1\rvert E\lvert 1\rangle\langle 0\rvert I\lvert 0\rangle)=\frac{1}{2}(\langle 0\rvert E\lvert 0\rangle+\langle 1\rvert E\lvert 1\rangle).
    \end{align}
    Eve can not infer anything about which of the four possible bit string Alice is trying to send. Here is the reason:
    Suppose Eve intercepts Alice's qubit and try to do some measurement on it. Whichever of the four possible bit string $00$, $01$, $10$, $11$ Alice is trying to send, Eve will get result $m$ with the same possibility
    \begin{align}
        p(m)=\frac{1}{2}(\langle 0\rvert E\lvert 0\rangle+\langle 1\rvert E\lvert 1\rangle).
    \end{align}
    In this way, Eve can not obtain any knowledge about the bit string Alice is trying to send from the measurement result.
\end{pf}

\section{The density operator}

\begin{exe}[Criterion of decide if a state is mixed or pure]
    Let $\rho$ be a density operator. Show that $\tr(\rho^2)\leq 1$, with equality if and only if $\rho$ is a pure state.
\end{exe}
\begin{pf}
    The density operator is
    \begin{align}
        \rho=\sum_ip_i\lvert\psi_i\rangle\langle\psi_i\rvert,
    \end{align}
    so
    \begin{align}
        \notag\tr(\rho^2)=&\tr\left[\left(\sum_ip_i\lvert\psi_i\rangle\langle\psi_i\rvert\right)\left(\sum_jp_j\lvert\psi_j\rangle\langle\psi_j\rvert\right)\right]=\sum_ip_i\left[\sum_jp_j\tr(\lvert\psi_i\rangle\langle\psi_i\vert\psi_j\rangle\langle\psi_j\rvert)\right]\\
        \notag=&\sum_ip_i\left[\sum_jp_j\tr(\langle\psi_i\vert\psi_j\rangle\langle\psi_j\vert\psi_i\rangle)\right]=\sum_ip_i\left[\sum_jp_j\tr(\abs{\langle\psi_i\vert\psi_j\rangle}^2)\right]=\sum_ip_i\left[\sum_jp_j\abs{\langle\psi_i\vert\psi_j\rangle}^2\right]\\
        \leq&\sum_ip_i\left(\sum_jp_j\right)=\sum_ip_i=1.
    \end{align}
    If $\rho$ is a pure state,
    \begin{align}
        \rho=\lvert\psi\rangle\langle\psi\rvert,
    \end{align}
    then
    \begin{align}
        \tr(\rho^2)=\tr(\lvert\psi\rangle\langle\psi\vert\psi\rangle\langle\psi\rvert)=\tr(\lvert\psi\rangle\langle\psi\rvert)=1.
    \end{align}
    If $\tr(\rho^2)=1$, then
    \begin{gather}
        \sum_ip_i\left[\sum_jp_j\abs{\langle\psi_i\vert\psi_j\rangle}^2\right]=\sum_ip_i\left(\sum_jp_j\right),\\
        \Longrightarrow\abs{\langle\psi_i\vert\psi_j\rangle}^2=\abs{\langle\psi_i\vert\psi_i\rangle}^2=1,
    \end{gather}
    which means that, for all $i$ and $j$, $\lvert\psi_i\rangle$ and $\lvert\psi_j\rangle$ is the same up to a phase difference
    \begin{align}
        \lvert\psi_j\rangle=e^{i\theta_{ij}}\lvert\psi_i\rangle,\quad\forall i,j,
    \end{align}
    i.e., $\rho$ must be a pure state
    \begin{align}
        \rho=\lvert\psi_1\rangle\langle\psi_1\rvert.
    \end{align}
    Therefore, $\tr(\rho^2)\leq 1$, with equality if and only if $\rho$ is a pure state.
\end{pf}

\begin{exe}[Bloch sphere for mixed states]
    The Bloch sphere picture for pure states of a single qubit was introduced in Section 1.2. This description has an important generalization to mixed states as follows.
    \begin{itemize}
        \item[(1)] Show that an arbitrary density matrix for a mixed state qubit may be written as
        \begin{align}
            \rho=\frac{I+\vec{r}\cdot\vec{\sigma}}{2},
        \end{align}
        where $\vec{r}$ is a real three-dimensional vector such that $\norm{\vec{r}}\leq 1$. This vector is known as the \emph{Bloch vector} for the state $\rho$.
        \item[(2)] What is the Bloch vector representation for the state $\rho=I/2$?
        \item[(3)] Show that a state $\rho$ is pure if and only if $\norm{\vec{r}}=1$.
        \item[(4)] Show that for pure states the description of the Bloch vector we have given coincide with that in Section 1.2.
    \end{itemize}
\end{exe}
\begin{sol}
    \item[(1)] An arbitrary $2\times 2$ matrix $\rho$ can be expressed as a linear combination of $I$, $\sigma_1$, $\sigma_2$, $\sigma_3$,
    \begin{align}
        \notag\rho=&\left[\begin{matrix}
            \rho_{11}&\rho_{12}\\
            \rho_{21}&\rho_{22}
        \end{matrix}\right]=\left[\begin{matrix}
            \frac{1}{2}(\rho_{11}+\rho_{22})+\frac{1}{2}(\rho_{11}-\rho_{22})&\frac{1}{2}(\rho_{12}+\rho_{21})+i(-i)\frac{1}{2}(\rho_{12}-\rho_{21})\\
            \frac{1}{2}(\rho_{12}+\rho_{21})+i\cdot i(\rho_{12}-\rho_{21})&\frac{1}{2}(\rho_{11}+\rho_{22})-\frac{1}{2}(\rho_{11}-\rho_{22})
        \end{matrix}\right]\\
        \notag=&\frac{1}{2}(\rho_{11}+\rho_{22})\left[\begin{matrix}
            1&0\\
            0&1
        \end{matrix}\right]+\frac{1}{2}(\rho_{12}+\rho_{21})\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]+\frac{i}{2}(\rho_{12}-\rho_{21})\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]+\frac{1}{2}(\rho_{11}-\rho_{22})\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]\\
        =&\frac{1}{2}(\rho_{11}+\rho_{22})I+\frac{1}{2}(\rho_{12}+\rho_{21})\sigma_1+\frac{i}{2}(\rho_{12}-\rho_{21})\sigma_2+\frac{1}{2}(\rho_{11}-\rho_{22})\sigma_3
    \end{align}
    If $\rho$ is an arbitrary density matrix, then it has trace of $1$,
    \begin{align}
        \tr(\rho)=\rho_{11}+\rho_{22}=1,
    \end{align}
    so
    \begin{align}
        \rho=\frac{1}{2}I+\frac{1}{2}(\rho_{12}+\rho_{21})\sigma_1+\frac{i}{2}(\rho_{12}-\rho_{21})\sigma_2+\frac{1}{2}(\rho_{11}-\rho_{22})\sigma_3=\frac{I+\vec{r}\cdot\vec{\sigma}}{2},
    \end{align}
    where
    \begin{align}
        \vec{r}=(r_1,r_2,r_3)=(\rho_{12}+\rho_{21},i(\rho_{12}-\rho_{21}),\rho_{11}-\rho_{22}).
    \end{align}
    \item[(2)] For the state
    \begin{align}
        \rho=\frac{I}{2}=\frac{I+\vec{r}\cdot\vec{\sigma}}{2},
    \end{align}
    the Bloch vector representation is
    \begin{align}
        \vec{r}=(0,0,0)=\vec{0}.
    \end{align}
    \item[(3)] \emph{Necessity}: If $\norm{\vec{r}}=1$, then
    \begin{align}
        \label{E2.72-1}
        \norm{\vec{r}}^2=r_1r_1^*+r_2r_2^*+r_3r_3^*=(\rho_{12}+\rho_{21})(\rho_{12}+\rho_{21})^*+(\rho_{12}-\rho_{21})(\rho_{12}-\rho_{21})^*+(\rho_{11}-\rho_{22})(\rho_{11}-\rho_{22})^*=1.
    \end{align}
    Since the density matrix $\rho$ is positive, it is necessarily Hermitian, i.e., $\rho_{11}$ and $\rho_{22}$ are real and $\rho_{12}$ and $\rho_{21}$ are complex conjugate,
    \begin{align}
        \rho_{12}=\rho_{21}^*.
    \end{align}
    Hence we can rewrite equation \eqref{E2.72-1} as
    \begin{align}
        \label{E2.72-2}
        (\rho_{12}+\rho_{21})(\rho_{21}+\rho_{12})+(\rho_{12}-\rho_{21})(\rho_{21}-\rho_{12})+(\rho_{11}-\rho_{22})(\rho_{11}-\rho_{22})^*=4\rho_{12}\rho_{21}+\rho_{11}^2+\rho_{22}^2-2\rho_{11}\rho_{22}=1.
    \end{align}
    Since the trace of the density matrix $\rho$ is $1$,
    \begin{align}
        \tr(\rho)=\rho_{11}+\rho_{22}=1,
    \end{align}
    we can rewrite equation \eqref{E2.72-2} as
    \begin{gather}
        4\rho_{12}\rho_{21}+2(\rho_{11}^2+\rho_{22}^2)-(\rho_{11}^2+\rho_{22}^2+2\rho_{11}\rho_{22})=4\rho_{12}\rho_{21}+2(\rho_{11}^2+\rho_{22}^2)-(\rho_{11}+\rho_{22})^2=4\rho_{12}\rho_{21}+2(\rho_{11}^2+\rho_{22}^2)-1=1,\\
        \Longrightarrow 2\rho_{12}\rho_{21}+\rho_{11}^2+\rho_{22}^2=1.
    \end{gather}
    In this way, the trace of the square of the density matrix equals $1$,
    \begin{align}
        \tr(\rho^2)=\tr\left(\left[\begin{matrix}
            \rho_{11}&\rho_{12}\\
            \rho_{21}&\rho_{22}
        \end{matrix}\right]\left[\begin{matrix}
            \rho_{11}&\rho_{12}\\
            \rho_{21}&\rho_{22}
        \end{matrix}\right]\right)=\tr\left[\begin{matrix}
            \rho_{11}^2+\rho_{12}\rho_{21}&\rho_{11}\rho_{12}+\rho_{12}\rho_{22}\\
            \rho_{21}\rho_{11}+\rho_{22}\rho_{21}&\rho_{21}\rho_{12}+\rho_{22}^2
        \end{matrix}\right]=\rho_{11}^2+\rho_{22}^2+2\rho_{12}\rho_{21}=1.
    \end{align}
    Therefore, the state $\rho$ is pure.\\
    \emph{Sufficiency}: If the state $\rho$ is pure, the trace of its square is $1$,
    \begin{align}
        \tr(\rho^2)=\rho_{11}^2+\rho_{22}^2+2\rho_{12}\rho_{21}=1.
    \end{align}
    Then
    \begin{align}
        \norm{\vec{r}}=\sqrt{4\rho_{12}\rho_{21}+2(\rho_{11}^2+\rho_{22}^2)-1}=1.
    \end{align}
    Therefore, a state $\rho$ is pure if and only if $\norm{\vec{r}}=1$.
    \item[(4)] According to Section 1.2, we write the state of the pure state as
    \begin{align}
        \lvert\psi\rangle=\cos\frac{\theta}{2}\lvert 0\rangle+e^{i\varphi}\sin\frac{\theta}{2}\lvert 1\rangle,
    \end{align}
    and the Bloch vector is
    \begin{align}
        \vec{r}=(\sin\theta\cos\varphi,\sin\theta\sin\varphi,\cos\theta).
    \end{align}
    The density matrix of the pure state is
    \begin{align}
        \notag\rho=&\left(\cos\frac{\theta}{2}\lvert 0\rangle+e^{i\varphi}\sin\frac{\theta}{2}\lvert 1\rangle\right)\left(\cos\frac{\theta}{2}\langle 0\rvert+e^{-i\varphi}\sin\frac{\theta}{2}\langle 1\rvert\right)=\left[\begin{matrix}
            \cos\frac{\theta}{2}\\
            e^{i\varphi}\sin\frac{\theta}{2}
        \end{matrix}\right]\left[\begin{matrix}
            \cos\frac{\theta}{2}&e^{-i\varphi}\sin\frac{\theta}{2}
        \end{matrix}\right]\\
        =&\left[\begin{matrix}
            \cos^2\frac{\theta}{2}&e^{-i\varphi}\sin\frac{\theta}{2}\cos\frac{\theta}{2}\\
            e^{i\varphi}\sin\frac{\theta}{2}\cos\frac{\theta}{2}&\sin^2\frac{\theta}{2}
        \end{matrix}\right].
    \end{align}
    According to the description we have given, the Bloch vector is
    \begin{align}
        \notag\vec{r}=&(\rho_{12}+\rho_{21},i(\rho_{12}-\rho_{21}),\rho_{11}-\rho_{22})\\
        \notag=&\left(e^{-i\varphi}\sin\frac{\theta}{2}\cos\frac{\theta}{2}+e^{i\varphi}\sin\frac{\theta}{2}\cos\frac{\theta}{2},i\left(e^{-i\varphi}\sin\frac{\theta}{2}\cos\frac{\theta}{2}-e^{i\varphi}\sin\frac{\theta}{2}\cos\frac{\theta}{2}\right),\cos^2\frac{\theta}{2}-\sin^2\frac{\theta}{2}\right)\\
        =&(\sin\theta\cos\varphi,\sin\theta\sin\sin\varphi,\cos\theta).
    \end{align}
    Therefore, for pure states, the description of the Bloch vector we have given coincide with that in Section 1.2.
\end{sol}

\begin{exe}
    Let $\rho$ be a density operator. A \emph{minimal ensemble} for $\rho$ is an ensemble $\{p_i,\lvert\psi_i\rangle\}$ containing a number of elements equal to the rank of $\rho$. Let $\lvert\psi_i\rangle$ be any state in the support of $\rho$. (The \emph{support} of a Hermitian operator $A$ is vector space spanned by the eigenvectors of $A$ with non-zero eigenvalues.) Show that there is a minimal ensemble for $\rho$ that contains $\lvert\psi\rangle$, and moreover that in any such ensemble $\lvert\psi_i\rangle$ must appear with probability
    \begin{align}
        p_i=\frac{1}{\langle\psi_i\rvert\rho^{-1}\lvert\psi_i\rangle},
    \end{align}
    where $\rho^{-1}$ is defined to be the inverse of $\rho$, where $\rho$ is considered as an operator acting only on the support of $\rho$. (This definition removes the problem that $\rho$ may not have an inverse.)
\end{exe}
\begin{pf}
    Suppose the spectral decomposition of the density matrix $\rho$ is
    \begin{align}
        \rho=\sum_{k=1}^Nq_k\lvert k\rangle\langle k\rvert=\sum_{k=1}^N\lvert\tilde{k}\rangle\langle\tilde{k}\rvert.
    \end{align}
    where $q_k$ are the non-zero eigenvalues and $\lvert k\rangle$ are the eigenvectors of $\rho$, $N=\text{rank}(\rho)$ and
    \begin{align}
        \lvert\tilde{k}\rangle=\sqrt{q_k}\lvert k\rangle.
    \end{align}
    Since $\lvert\psi_i\rangle$ is a state in the support of $\rho$, it can be written as a linear combination of the eigenvectors
    \begin{align}
        \lvert\psi_i\rangle=\sum_k\langle k\vert\psi_i\rangle\lvert k\rangle=\sum_kc_{ik}\lvert k\rangle,
    \end{align}
    where
    \begin{align}
        c_{ik}=\langle k\vert\psi_i\rangle,
    \end{align}
    and
    \begin{align}
        \sum_{k=1}^N\abs{c_{ik}}^2=1.
    \end{align}
    Define
    \begin{align}
        p_i=\frac{1}{\sum_{k=1}^N\frac{\abs{c_{ik}}^2}{p_k}}
    \end{align}
    and
    \begin{align}
        u_{ik}=\frac{\sqrt{p_i}c_{ik}}{\sqrt{q_k}}.
    \end{align}
    Since
    \begin{align}
        \sum_{k=1}^N\abs{u_{ik}}^2=\sum_{k=1}^N\frac{p_i\abs{c_{ik}}^2}{q_k}=p_i\sum_{k=1}^N\frac{\abs{c_{ik}}^2}{q_k}=1,
    \end{align}
    $u_{ik}$ is a unitary matrix.
    According to Theorem 2.6, the set
    \begin{align}
        \lvert\tilde{\psi}_i\rangle=\sum_{k=1}^Nu_{ik}\lvert\tilde{k}\rangle=\sum_{k=1}^N\frac{\sqrt{p_i}c_{ik}}{\sqrt{q_k}}\lvert\tilde{k}\rangle=\sqrt{p_i}\sum_{k=1}^Nc_{ik}\lvert k\rangle=\sqrt{p_i}\lvert\psi_i\rangle,\quad i=1,\cdots,N,
    \end{align}
    generate the same density matrix $\rho$,
    \begin{align}
        \rho=\sum_{i=1}^N\lvert\tilde{\psi}_i\rangle\langle\tilde{\psi}_i\rvert=\sum_{i=1}^Np_i\lvert\psi_i\rangle\langle\psi_i\rvert.
    \end{align}
    Moreover,
    \begin{align}
        \frac{1}{\langle\psi_i\rvert\rho^{-1}\lvert\psi_i\rangle}=\frac{1}{\langle\psi_i\rvert\sum_{k=1}^Nq_k^{-1}\lvert k\rangle\langle k\rvert\lvert\psi_i\rangle}=\frac{1}{\sum_{k=1}^Nq_k^{-1}\abs{\langle k\vert\psi_i\rangle}^2}=\frac{1}{\sum_{k=1}^N\frac{\abs{c_{ik}}^2}{q_k}}=p_i.
    \end{align}
    Therefore, there is a minimal ensemble for $\rho$ that contains $\lvert\psi_i\rangle$, and moreover that in any such ensemble $\lvert\psi_i\rangle$ must appear with probability
    \begin{align}
        p_i=\frac{1}{\langle\psi_i\rvert\rho^{-1}\lvert\psi_i\rangle}.
    \end{align}
\end{pf}

\begin{exe}
    Suppose a composite of system $A$ and $B$ is in the state $\lvert a\rangle\lvert b\rangle$, where $\lvert a\rangle$ is a pure state of system $A$, and $\lvert b\rangle$ is a pure state of system $B$. Show that the reduced density operator of system $A$ alone is a pure state.
\end{exe}
\begin{pf}
    The density operator of the composite system is
    \begin{align}
        \rho^{AB}=\lvert a\rangle\langle a\rvert\otimes\lvert b\rangle\langle b\rvert.
    \end{align}
    The reduced density operator of system $A$ is
    \begin{align}
        \rho^A=\tr_B(\rho^{AB})=\lvert a\rangle\langle a\rvert\tr_B(\lvert b\rangle\langle b\rvert)=\lvert a\rangle\langle a\rvert.
    \end{align}
    Since
    \begin{align}
        \tr[(\rho^A)^2]=\tr[(\lvert a\rangle\langle a\rvert)(\lvert a\rangle\langle a\rvert)]=\tr(\lvert a\rangle\langle a\rvert)=1,
    \end{align}
    the reduced density operator of system $A$ alone is a pure state.
\end{pf}

\begin{exe}
    For each of the four Bell states, find the reduced density operator for each qubit.
\end{exe}
\begin{sol}
    Suppose the first one of the two qubits described by the Bell states is $A$, and the other $B$.
    For the Bell state
    \begin{align}
        \lvert\beta_{00}\rangle=\frac{\lvert 00\rangle\langle 11\rvert}{\sqrt{2}},
    \end{align}
    its density operator is
    \begin{align}
        \rho_{00}^{AB}=\lvert\beta_{00}\rangle\langle\beta_{00}\rvert=\frac{\lvert 00\rangle\langle 11\rvert}{\sqrt{2}}\frac{\langle 00\rvert+\langle 11\rvert}{\sqrt{2}}=\frac{1}{2}(\lvert 00\rangle\langle 00\rvert+\lvert 00\rangle\langle 11\rvert+\lvert 11\rangle\langle 00\rvert+\lvert 11\rangle\langle 11\rvert),
    \end{align}
    the reduced density operator of qubit $A$ is
    \begin{align}
        \notag\rho_{00}^A=&\tr_B(\rho_{00}^{AB})=\frac{1}{2}[\tr_B(\lvert 00\rangle\langle 00\rvert)+\tr_B(\lvert 00\rangle\langle 11\rvert)+\tr_B(\lvert 11\rangle\langle 00\rvert)+\tr_B(\lvert 11\rangle\langle 11\rvert)]\\
        \notag=&\frac{1}{2}(\lvert 0\rangle\langle 0\rvert\langle 0\vert 0\rangle+\lvert 0\rangle\langle 1\rvert\langle 0\vert 1\rangle+\lvert 1\rangle\langle 0\rvert\langle 1\vert 0\rangle+\lvert 1\rangle\langle 1\rvert\langle 1\vert 1\rangle)=\frac{1}{2}(\lvert 0\rangle\langle 0\rvert+\lvert 1\rangle\langle 1\rvert),
    \end{align}
    and the reduced density operator of qubit $B$ is
    \begin{align}
        \notag\rho_{00}^B=&\tr_A(\rho_{00}^{AB})=\frac{1}{2}[\tr_A(\lvert 00\rangle\langle 00\rvert)+\tr_A(\lvert 00\rangle\langle 11\rvert)+\tr_A(\lvert 11\rangle\langle 00\rvert)+\tr_A(\lvert 11\rangle\langle 11\rvert)]\\
        =&\frac{1}{2}(\langle 0\vert 0\rangle\lvert 0\rangle\langle 0\rvert+\langle 0\vert 1\rangle\lvert 0\rangle\langle 1\rvert+\langle 1\vert 0\rangle\lvert 1\rangle\langle 0\rvert+\langle 1\vert 1\rangle\lvert 1\rangle\langle 1\rvert)=\frac{1}{2}(\lvert 0\rangle\langle 0\rvert+\lvert 1\rangle\langle 1\rvert),
    \end{align}
    For the Bell state
    \begin{align}
        \lvert\beta_{01}\rangle=\frac{\lvert 01\rangle+\lvert 10\rangle}{\sqrt{2}},
    \end{align}
    its density operator is
    \begin{align}
        \rho_{01}^{AB}=\lvert\beta_{01}\rangle\langle\beta_{01}\rvert=\frac{\lvert 01\rangle+\lvert 10\rangle}{\sqrt{2}}\frac{\langle 01\rvert+\langle 10\rvert}{\sqrt{2}}=\frac{1}{2}(\lvert 01\rangle\langle 01\rvert+\lvert 01\rangle\langle 10\rvert+\lvert 10\rangle\langle 01\rvert+\lvert 10\rangle\langle 10\rvert),
    \end{align}
    the reduced density operator of qubit $A$ is
    \begin{align}
        \notag\rho_{01}^A=&\tr_B(\rho_{01}^{AB})=\frac{1}{2}[\tr_B(\lvert 01\rangle\langle 01\rvert)+\tr_B(\lvert 01\rangle\langle 10\rvert)+\tr_B(\lvert 10\rangle\langle 01\rvert)+\tr_B(\lvert 10\rangle\langle 10\rvert)]\\
        =&\frac{1}{2}(\lvert 0\rangle\langle 0\rvert\langle 1\vert 1\rangle+\lvert 0\rangle\langle 1\rvert\langle 1\vert 0\rangle+\lvert 1\rangle\langle 0\rvert\langle 0\vert 1\rangle+\lvert 1\rangle\langle 1\rvert\langle 0\vert 0\rangle)=\frac{1}{2}(\lvert 0\rangle\langle 0\rvert+\lvert 1\rangle\langle 1\rvert),
    \end{align}
    and the reduced density operator of qubit $B$ is
    \begin{align}
        \notag\rho_{01}^B=&\tr_A(\rho_{01}^{AB})=\frac{1}{2}[\tr_A(\lvert 01\rangle\langle 01\rvert)+\tr_A(\lvert 01\rangle\langle 10\rvert)+\tr_A(\lvert 10\rangle\langle 01\rvert)+\tr_A(\lvert 10\rangle\langle 10\rvert)]\\
        =&\frac{1}{2}(\langle 0\vert 0\rangle\lvert 1\rangle\langle 1\rvert+\langle 0\vert 1\rangle\lvert 1\rangle\langle 0\rvert+\langle 1\vert 0\rangle\lvert 0\rangle\langle 1\rvert+\langle 1\vert 1\rangle\lvert 0\rangle\langle 0\rvert)=\frac{1}{2}(\lvert 1\rangle\langle 1\rvert+\lvert 0\rangle\langle 0\rvert).
    \end{align}
    For the Bell state
    \begin{align}
        \lvert\beta_{10}^{AB}\rangle=\frac{\lvert 00\rangle-\lvert 11\rangle}{\sqrt{2}},
    \end{align}
    \begin{align}
        \rho_{10}^{AB}=\lvert\beta_{10}^{AB}\rangle\langle\beta_{10}^{AB}\rvert=\frac{\lvert 00\rangle-\lvert 11\rangle}{\sqrt{2}}\frac{\langle 00\rvert-\langle 11\rvert}{\sqrt{2}}=\frac{1}{2}(\lvert 00\rangle\langle 00\rvert-\lvert 00\rangle\langle 11\rvert-\lvert 11\rangle\langle 00\rvert+\lvert 11\rangle\langle 11\rvert),
    \end{align}
    the reduced density operator of qubit $A$ is
    \begin{align}
        \notag\rho_{10}^A=&\tr_B(\rho_{10}^{AB})=\frac{1}{2}[\tr_B(\lvert 00\rangle\langle 00\rvert)-\tr_B(\lvert 00\rangle\langle 11\rvert)-\tr_B(\lvert 11\rangle\langle 00\rvert)+\tr_B(\lvert 11\rangle\langle 11\rvert)]\\
        =&\frac{1}{2}(\lvert 0\rangle\langle 0\rvert\langle 0\vert 0\rangle-\lvert 0\rangle\langle 1\rvert\langle 0\vert 1\rangle-\lvert 1\rangle\langle 0\rvert\langle 1\vert 0\rangle+\lvert 1\rangle\langle 1\rvert\langle 1\vert 1\rangle)=\frac{1}{2}(\lvert 0\rangle\langle 0\rvert+\lvert 1\rangle\langle 1\rvert),
    \end{align}
    and the reduced density operator of qubit $B$ is
    \begin{align}
        \notag\rho_{10}^B=&\tr_A(\rho_{10}^{AB})=\frac{1}{2}[\tr_A(\lvert 00\rangle\langle 00\rvert)-\tr_A(\lvert 00\rangle\langle 11\rvert)-\tr_A(\lvert 11\rangle\langle 00\rvert)+\tr_A(\lvert 11\rangle\langle 11\rvert)]\\
        =&\frac{1}{2}(\langle 0\vert 0\rangle\lvert 0\rangle\langle 0\rvert-\langle 0\vert 1\rangle\lvert 0\rangle\langle 1\rvert-\langle 1\vert 0\rangle\lvert 1\rangle\langle 0\rvert+\langle 1\vert 1\rangle\lvert 1\rangle\langle 1\rvert)=\frac{1}{2}(\lvert 0\rangle\langle 0\rvert+\lvert 1\rangle\langle 1\rvert).
    \end{align}
    For the Bell state
    \begin{align}
        \lvert\beta_{11}^{AB}\rangle=\frac{\lvert 01\rangle-\lvert 10\rangle}{\sqrt{2}},
    \end{align}
    its density operator is
    \begin{align}
        \rho_{11}^{AB}=\lvert\beta_{11}^{AB}\rangle\langle\beta_{11}^{AB}\rvert=\frac{\lvert 01\rangle-\lvert 10\rangle}{\sqrt{2}}\frac{\langle 01\rvert-\langle 10\rvert}{\sqrt{2}}=\frac{1}{2}(\lvert 01\rangle\langle 01\rvert-\lvert 01\rangle\langle 10\rvert-\lvert 10\rangle\langle 01\rvert+\lvert 10\rangle\langle 10\rvert),
    \end{align}
    the reduced density operator of qubit $A$ is
    \begin{align}
        \notag\rho_{11}^A=&\tr_B(\rho_{11}^{AB})=\frac{1}{2}[\tr_B(\lvert 01\rangle\langle 01\rvert)-\tr_B(\lvert 01\rangle\langle 10\rvert)-\tr_B(\lvert 10\rangle\langle 01\rvert)+\tr_B(\lvert 10\rangle\langle 10\rvert)]\\
        =&\frac{1}{2}(\lvert 0\rangle\langle 0\rvert\langle 1\vert 1\rangle-\lvert 0\rangle\langle 1\rvert\langle 1\vert 0\rangle-\lvert 1\rangle\langle 0\rvert\langle 0\vert 1\rangle+\lvert 1\rangle\langle 1\rvert\langle 0\vert 0\rangle)=\frac{1}{2}(\lvert 0\rangle\langle 0\rvert+\lvert 1\rangle\langle 1\rvert),
    \end{align}
    and the reduced density operator of qubit $B$ is
    \begin{align}
        \notag\rho_{11}^B=&\tr_A(\rho_{11}^{AB})=\frac{1}{2}[\tr_A(\lvert 01\rangle\langle 01\rvert)-\tr_A(\lvert 01\rangle\langle 10\rvert)-\tr_A(\lvert 10\rangle\langle 01\rvert)+\tr_A(\lvert 10\rangle\langle 10\rvert)]\\
        =&\frac{1}{2}(\langle 1\vert 1\rangle\lvert 0\rangle\langle 0\rvert-\langle 1\vert 0\rangle\lvert 0\rangle\langle 1\rvert-\langle 0\vert 1\rangle\lvert 1\rangle\langle 0\rvert+\langle 0\vert 0\rangle\lvert 1\rangle\langle 1\rvert)=\frac{1}{2}(\lvert 1\rangle\langle 1\rvert+\lvert 0\rangle\langle 0\rvert).
    \end{align}
\end{sol}

\begin{exe}
    Extend the proof the Schmidt decomposition to the case where $A$ and $B$ may have the state spaces of different dimensionality.
\end{exe}
\begin{pf}
    Suppose the dimension of the state space for systems $A$ and $B$ are $n$ and $m$, respectively, and let $\lvert j\rangle$ and $\lvert k\rangle$ be any fixed orthonormal bases for systems $A$ and $B$, respectively.
    Then $\lvert\psi\rangle$ can be written as
    \begin{align}
        \lvert\psi\rangle=\sum_{j=1}^n\sum_{k=1}^ma_{jk}\lvert j\rangle\lvert k\rangle,
    \end{align}
    for some matrix $a$ of complex numbers $a_{jk}$.
    Without loss of generality, set $n>m$. By the singular value decomposition,
    \begin{align}
        a=u\left[\begin{matrix}
            d\\
            0
        \end{matrix}\right]v,
    \end{align}
    where $u$ is an $n\times n$ unitary matrix, $v$ is an $m\times m$ unitary matrix, $d$ is an $m\times m$ diagonal matrix with non-negative real elements, and $\left[\begin{smallmatrix}
        d\\
        0
    \end{smallmatrix}\right]$ is a $n\times m$ matrix whose $(m+1)$th to $n$th row are all zero.
    Thus
    \begin{align}
        \lvert\psi\rangle=\sum_{j=1}^n\sum_{i=1}^m\sum_{k=1}^mu_{ji}d_{ii}v_{ik}\lvert j\rangle\lvert k\rangle.
    \end{align}
    Define
    \begin{align}
        \lvert i_A\rangle\equiv&\sum_{j=1}^nu_{ji}\lvert j\rangle,\\
        \lvert i_B\rangle\equiv&\sum_{k=1}^mu_{ik}\lvert k\rangle,
    \end{align}
    and
    \begin{align}
        \lambda_i\equiv d_{ii}.
    \end{align}
    We see that this gives
    \begin{align}
        \lvert\psi\rangle=\sum_{i=1}^m\lambda_i\lvert i_A\rangle\lvert i_B\rangle.
    \end{align}
    Due to the unitarity of $u$ and $v$ and the orthonormality of $\lvert j\rangle$ and $\lvert k\rangle$,
    \begin{align}
        \langle i_A\vert l_A\rangle=&\left(\sum_{j=1}^nu_{ji}\lvert j\rangle\right)^{\dagger}\left(\sum_{j'=1}^nu_{j'l}\lvert j'\rangle\right)=\sum_{j=1}^n\sum_{j'=1}^nu_{ij}^*u_{j'l}\langle j\vert j'\rangle=\sum_{j=1}^n\sum_{j'=1}^nu_{ij}^*u_{j'i}\delta_{jj'}=\sum_{j=1}^nu_{ij}^*u_{jl}=\delta_{il},\\
        \langle i_B\vert l_B\rangle=&\left(\sum_{k=1}^mv_{ik}\lvert k\rangle\right)^{\dagger}\left(\sum_{k'=1}^mv_{lk'}\lvert k'\rangle\right)=\sum_{k=1}^m\sum_{k'=1}^mv_{ki}^*v_{lk'}\langle k\vert k'\rangle=\sum_{k=1}^m\sum_{k'=1}^mv_{ki}^*v_{lk'}\delta_{kk'}=\sum_{k=1}^mv_{ki}^*u_{lk}=\delta_{il},
    \end{align}
    $\lvert i_A\rangle$ and $\lvert i_B\rangle$ form two orthonormal sets, respectively.
    Moreover, since $\lvert\psi\rangle$ is a normalized pure state,
    \begin{align}
        \langle\psi\vert\psi\rangle=\left(\sum_{i=1}^m\lambda_i\lvert i_A\rangle\lvert i_B\rangle\right)^{\dagger}\left(\sum_{l=1}^m\lambda_l\lvert l_A\rangle\lvert l_B\rangle\right)=\sum_{i=1}^m\sum_{l=1}^m\lambda_i^2\langle i_A\vert l_A\rangle\langle i_B\vert l_B\rangle=\sum_{i=1}^m\sum_{l=1}^m\lambda_i\lambda_l\delta_{il}=\sum_{i=1}^m\lambda_i^2=1.
    \end{align}
    Therefore, there exists orthonormal states $\lvert i_A\rangle$ for system $A$, and orthonormal states $\lvert i_B\rangle$ of system $B$ such that
    \begin{align}
        \lvert\psi\rangle=\sum_i\lambda_i\lvert i_A\rangle\lvert i_B\rangle,
    \end{align}
    where $\lambda_i$ are non-negative real number satisfying $\sum_i\lambda_i^2=1$.
\end{pf}

\begin{exe}
    Suppose $ABC$ is a three component quantum system. Show by example that there are quantum state $\lvert\psi\rangle$ of such systems which can not be written in the form
    \begin{align}
        \lvert\psi\rangle=\sum_i\lambda_i\lvert i_A\rangle\lvert i_B\rangle\lvert i_C\rangle,
    \end{align}
    where $\lambda_i$ are real numbers, and $\lvert i_A\rangle$, $\lvert i_B\rangle$, $\lvert i_C\rangle$ are orthonormal bases of the respective systems.
\end{exe}
\begin{sol}
    Prove this according to \cite{peres1995higher}. Suppose the dimension of the vector spaces of the three component systems are all $d$. Taking the normalization condition and the common phase factor taken into consideration, $2d^3-2$ real parameters are needed to determine the state of the composite system
    \begin{align}
        \lvert\psi\rangle=\sum_{j,k,l=1}^dA_{jkl}\lvert j\rangle\lvert k\rangle\lvert l\rangle,
    \end{align}
    where $\lvert j\rangle$, $\lvert k\rangle$ and $\lvert l\rangle$ are any fixed orthonormal bases for systems $A$, $B$ and $C$, respectively.
    To rewrite the state into the form
    \begin{align}
        \lvert\psi\rangle=\sum_{i=1}^d\lvert i_A\rangle\lvert i_B\rangle\lvert i_C\rangle,
    \end{align}
    where $\lambda_i$ are real numbers, and $\lvert i_A\rangle$, $\lvert i_B\rangle$, $\lvert i_C\rangle$ are orthonormal bases of the respective systems, we need take unitary transforms,
    \begin{align}
        \lvert i_A\rangle=&\sum_{j=1}^dU_{ij}\lvert j\rangle,\\
        \lvert i_B\rangle=&\sum_{k=1}^dV_{ik}\lvert k\rangle,\\
        \lvert i_C\rangle=&\sum_{l=1}^dW_{il}\lvert l\rangle.
    \end{align}
    Each $d\times d$ unitary matrix has $d(d-1)$ real parameters, so we only have $3d(d-1)+2d-2=3d^2-d-2$ real parameters in the rewriting process, which are not enough to solve the problem in general.

    Therefore, there must be quantum state $\lvert\psi\rangle$ of such systems which can not be written in the form
    \begin{align}
        \lvert\psi\rangle=\sum_i\lambda_i\lvert i_A\rangle\lvert i_B\rangle\lvert i_C\rangle.
    \end{align}

    One of the examples is a quantum system consists of three qubits, $A$, $B$, and $C$, whose state is
    \begin{align}
        \lvert\psi\rangle=\frac{\lvert 001\rangle+\lvert 010\rangle+\lvert 100\rangle}{\sqrt{3}}.
    \end{align}
\end{sol}

\begin{exe}
    Prove that a state $\lvert\psi\rangle$ of a composite system $AB$ is a product state if and only if it has Schmidt number $1$. Prove that $\lvert\psi\rangle$ is a product state if and only if $\rho^A$ (and thus $\rho^B$) are pure states.
\end{exe}
\begin{pf}
    \emph{Sufficiency}: If the state $\lvert\psi\rangle$ has Schmidt number $1$, it can be written as
    \begin{align}
        \lvert\psi\rangle=\lambda_1\lvert 1_A\rangle\lvert 1_B\rangle,
    \end{align}
    where the normalization condition requires that $\abs{\lambda_1}=1$ and we can actually absorb the phase factor of $\lambda_1$ into $\lvert 1_A\rangle$ or $\lvert 1_B\rangle$, so the phase $\lvert\psi\rangle$ is a product state.\\
    \emph{Necessity}: If the state $\lvert\psi\rangle$ is a product state, it can be written as the product of the state of $A$ and the state of $B$,
    \begin{align}
        \lvert\psi\rangle=\lvert 1_A\rangle\lvert 1_B\rangle,
    \end{align}
    so its Schmidt number is $1$.

    \emph{Sufficiency}: If $\rho^A$ (and thus $\rho^B$) are pure states, they can be written as
    \begin{align}
        \rho^A=&\lvert 1_A\rangle\langle 1_A\rvert,\\
        \rho^B=&\lvert 1_B\rangle\langle 1_B\rvert,\\
        \Longrightarrow\rho=&\rho^A\otimes\rho^B=\lvert 1_A\rangle\lvert 1_B\rangle\langle 1_A\rvert\langle 1_B\rvert=\lvert\psi\rangle\langle\psi\rvert,
    \end{align}
    so $\lvert\psi\rangle=\lvert 1_A\rangle\lvert 1_B\rangle$ is a product state.\\
    \emph{Necessity}: If $\lvert\psi\rangle$ is a product state, it can be written as
    \begin{align}
        \lvert\psi\rangle=\lvert 1_A\rangle\lvert 1_B\rangle,
    \end{align}
    and its density matrix is
    \begin{align}
        \rho=\lvert\psi\rangle\langle\psi\rvert=\lvert 1_A\rangle\lvert 1_B\rangle\langle 1_A\rvert\langle 1_B\rvert,
    \end{align}
    so
    \begin{align}
        \rho^A=&\tr_B(\rho)=\lvert 1_A\rangle\langle 1_A\rvert,\\
        \rho^B=&\tr_A(\rho)=\lvert 1_B\rangle\langle 1_B\rvert,
    \end{align}
    are pure states.
\end{pf}

\begin{exe}
    Consider a composite system consisting of two qubits. Find the Schmidt decomposition of the states
    \begin{align}
        \frac{\lvert 00\rangle+\lvert 11\rangle}{\sqrt{2}};\quad\frac{\lvert 00\rangle+\lvert 01\rangle+\lvert 10\rangle+\lvert 11\rangle}{2};\quad\text{and}\quad\frac{\lvert 00\rangle+\lvert 01\rangle+\lvert 10\rangle}{\sqrt{3}}.
    \end{align}
\end{exe}
\begin{pf}
    State $\frac{\lvert 00\rangle+\lvert 11\rangle}{\sqrt{2}}$ is already in the form of Schmidt decomposition,
    \begin{align}
        \frac{\lvert 00\rangle+\lvert 11\rangle}{\sqrt{2}}=\frac{1}{\sqrt{2}}\lvert 0\rangle\lvert 0\rangle+\frac{1}{\sqrt{2}}\lvert 1\rangle\lvert 1\rangle.
    \end{align}
    For state $\frac{\lvert 00\rangle+\lvert 01\rangle+\lvert 10\rangle+\lvert 11\rangle}{2}$, by singular decomposition, we have
    \begin{align}
        a=\frac{1}{2}\left[\begin{matrix}
            1&1\\
            1&1
        \end{matrix}\right]=\frac{1}{\sqrt{2}}\left[\begin{matrix}
            -1&1\\
            1&1
        \end{matrix}\right]\left[\begin{matrix}
            0&0\\
            0&1
        \end{matrix}\right]\frac{1}{\sqrt{2}}\left[\begin{matrix}
            -1&1\\
            1&1
        \end{matrix}\right]=udv,
    \end{align}
    where
    \begin{align}
        u=v=&\frac{1}{\sqrt{2}}\left[\begin{matrix}
            -1&1\\
            1&1
        \end{matrix}\right],\\
        d=&\left[\begin{matrix}
            0&0\\
            0&1
        \end{matrix}\right].
    \end{align}
    Hence we define
    \begin{align}
        \lvert 1_A\rangle=&u_{11}\lvert 0\rangle+u_{12}\lvert 1\rangle=\frac{1}{\sqrt{2}}(-\lvert 0\rangle+\lvert 1\rangle),\\
        \lvert 2_A\rangle=&u_{21}\lvert 1\rangle+u_{22}\lvert 1\rangle=\frac{1}{\sqrt{2}}(\lvert 0\rangle+\lvert 1\rangle),\\
        \lvert 1_B\rangle=&v_{11}\lvert 0\rangle+v_{12}\lvert 1\rangle=\frac{1}{\sqrt{2}}(-\lvert 0\rangle+\lvert 1\rangle),\\
        \lvert 2_B\rangle=&v_{21}\lvert 1\rangle+v_{22}\lvert 1\rangle=\frac{1}{\sqrt{2}}(\lvert 0\rangle+\lvert 1\rangle),
    \end{align}
    and have Schmidt decomposition
    \begin{align}
        \frac{\lvert 00\rangle+\lvert 01\rangle+\lvert 10\rangle+\lvert 11\rangle}{2}=\sum_{i=1}^2d_{ii}\lvert i_A\rangle\lvert i_B\rangle=\frac{1}{\sqrt{2}}(\lvert 0\rangle+\lvert 1\rangle)\otimes\frac{1}{\sqrt{2}}(\lvert 0\rangle+\lvert 1\rangle).
    \end{align}
    For state $\frac{\lvert 00\rangle+\lvert 01\rangle+\lvert 10\rangle}{\sqrt{3}}$, by singular decomposition,
    \begin{align}
        a=\frac{1}{\sqrt{3}}\left[\begin{matrix}
            1&1\\
            1&0
        \end{matrix}\right]=\left[\begin{matrix}
            -\sqrt{\frac{5-\sqrt{5}}{10}}&\sqrt{\frac{5+\sqrt{5}}{10}}\\
            \sqrt{\frac{5+\sqrt{5}}{10}}&\sqrt{\frac{5-\sqrt{5}}{10}}
        \end{matrix}\right]\left[\begin{matrix}
            \frac{1-\sqrt{5}}{2\sqrt{3}}&0\\
            0&\frac{1+\sqrt{5}}{2\sqrt{3}}
        \end{matrix}\right]\left[\begin{matrix}
            -\sqrt{\frac{5-\sqrt{5}}{10}}&\sqrt{\frac{5+\sqrt{5}}{10}}\\
            \sqrt{\frac{5+\sqrt{5}}{10}}&\sqrt{\frac{5-\sqrt{5}}{10}}
        \end{matrix}\right]=udv,
    \end{align}
    where
    \begin{align}
        u=v=&\left[\begin{matrix}
            -\sqrt{\frac{5-\sqrt{5}}{10}}&\sqrt{\frac{5+\sqrt{5}}{10}}\\
            \sqrt{\frac{5+\sqrt{5}}{10}}&\sqrt{\frac{5-\sqrt{5}}{10}}
        \end{matrix}\right],\\
        d=&\left[\begin{matrix}
            \frac{1-\sqrt{5}}{2\sqrt{3}}&0\\
            0&\frac{1+\sqrt{5}}{2\sqrt{3}}
        \end{matrix}\right].
    \end{align}
    Hence we define
    \begin{align}
        \lvert 1_A\rangle=&u_{11}\lvert 0\rangle+u_{12}\lvert 1\rangle=-\sqrt{\frac{5-\sqrt{5}}{10}}\lvert 0\rangle+\sqrt{\frac{5+\sqrt{5}}{10}}\lvert 1\rangle,\\
        \lvert 2_A\rangle=&u_{21}\lvert 0\rangle+u_{22}\lvert 1\rangle=\sqrt{\frac{5+\sqrt{5}}{10}}\lvert 0\rangle+\sqrt{\frac{5-\sqrt{5}}{10}}\lvert 1\rangle,\\
        \lvert 1_B\rangle=&v_{11}\lvert 0\rangle+v_{12}\lvert 1\rangle=-\sqrt{\frac{5-\sqrt{5}}{10}}\lvert 0\rangle+\sqrt{\frac{5+\sqrt{5}}{10}}\lvert 1\rangle,\\
        \lvert 2_B\rangle=&v_{21}\lvert 0\rangle+v_{22}\lvert 1\rangle=\sqrt{\frac{5+\sqrt{5}}{10}}\lvert 0\rangle+\sqrt{\frac{5-\sqrt{5}}{10}}\lvert 1\rangle,
    \end{align}
    and have Schmidt decomposition
    \begin{align}
        \notag\frac{\lvert 00\rangle+\lvert 01\rangle+\lvert 10\rangle}{\sqrt{3}}=&\frac{1-\sqrt{5}}{2\sqrt{3}}\left(-\sqrt{\frac{5-\sqrt{5}}{10}}\lvert 0\rangle+\sqrt{\frac{5+\sqrt{5}}{10}}\lvert 1\rangle\right)\otimes\left(-\sqrt{\frac{5-\sqrt{5}}{10}}\lvert 0\rangle+\sqrt{\frac{5+\sqrt{5}}{10}}\lvert 1\rangle\right)\\
        &+\frac{1+\sqrt{5}}{2\sqrt{3}}\left(\sqrt{\frac{5+\sqrt{5}}{10}}\lvert 0\rangle+\sqrt{\frac{5-\sqrt{5}}{10}}\lvert 1\rangle\right)\otimes\left(\sqrt{\frac{5+\sqrt{5}}{10}}\lvert 0\rangle+\sqrt{\frac{5-\sqrt{5}}{10}}\lvert 1\rangle\right).
    \end{align}
\end{pf}

\begin{exe}
    Suppose $\lvert\psi\rangle$ and $\lvert\varphi\rangle$ are two pure states of a composite quantum system with components $A$ and $B$, with identical Schmidt coefficients. Show that there are unitary transformations $U$ on system $A$ and $V$ on system $B$ such that $\lvert\psi\rangle=(U\otimes V)\lvert\varphi\rangle$.
\end{exe}
\begin{pf}
    Suppose the two pure states are respectively
    \begin{align}
        \lvert\psi\rangle=&\sum_i\lambda_i\lvert\psi_i^A\rangle\lvert\psi_i^B\rangle,\\
        \lvert\varphi\rangle=&\sum_i\lambda_i\lvert\varphi_i^A\rangle\lvert\varphi_i^B\rangle.
    \end{align}
    Define the two unitary matrices as
    \begin{align}
        U=&\sum_i\lvert\psi_i^A\rangle\langle\varphi_i^A\rvert,\\
        V=&\sum_i\lvert\psi_i^B\rangle\langle\varphi_i^B\rvert.
    \end{align}
    Then
    \begin{align}
        \notag(U\otimes V)\lvert\varphi\rangle=&\sum_j\lvert\psi_j^A\rangle\langle\varphi_j^A\rvert\sum_k\lvert\psi_k^B\rangle\langle\varphi_k^B\rvert\sum_i\lambda_i\lvert\varphi_i^A\rangle\lvert\varphi_i^B\rangle=\sum_{i,j,k}\lambda_i\lvert\psi_j^A\rangle\langle\varphi_j^A\vert\varphi_i^A\rangle\lvert\psi_k^B\rangle\langle\varphi_k^A\vert\varphi_i^B\rangle\\
        =&\sum_{i,j,k}\delta_{ij}\delta_{ik}\lambda_i\lvert\psi_j^A\rangle\lvert\psi_k^B\rangle=\sum_i\lambda_i\lvert\psi_i^A\rangle\lvert\psi_i^B\rangle=\lvert\psi\rangle.
    \end{align}
\end{pf}

\begin{exe}[Freedom in purifications]
    Let $\lvert AR_1\rangle$ and $\lvert AR_2\rangle$ be two purifications of a state $\rho^A$ to a composite system $AR$. Prove that there exists a unitary transformation $U_R$ acting on system $R$ such that $\lvert AR_1\rangle=(I_A\otimes U_R)\lvert AR_2\rangle$.
\end{exe}
\begin{pf}
    Let the Schmidt decomposition of $\lvert AR_1\rangle$ and $\lvert AR_2\rangle$ be respectively
    \begin{align}
        \lvert AR_1\rangle=&\sum_i\sqrt{p_i}\lvert\psi_i^A\rangle\lvert\psi_i^R\rangle,\\
        \lvert AR_2\rangle=&\sum_i\sqrt{q_i}\lvert\varphi_i^A\rangle\lvert\varphi_i^R\rangle.
    \end{align}
    Since $\lvert AR_1\rangle$ and $\lvert AR_2\rangle$ are two purifications of state $\rho^A$,
    \begin{gather}
        \rho^A=\tr_R(\lvert AR_1\rangle\langle AR_1\rvert)=\tr_R(\lvert AR_2\rangle\langle AR_2\rvert),\\
        \Longrightarrow\sum_ip_i\lvert\psi_i^A\rangle\langle\psi_i^A\rvert=\sum_iq_i\lvert\varphi_i^A\rangle\langle\varphi_i^A\rvert.
    \end{gather}
    Since both $\lvert\psi_i^A\rangle$ and $\lvert\varphi_i^A\rangle$ are orthogonal bases of system $A$ and eigenvectors of $\rho^A$, without loss of generality, we can set
    \begin{gather}
        \sqrt{p_i}=\sqrt{q_i}\equiv\lambda_i,\\
        \lvert\psi_i^A\rangle=\lvert\varphi_i^A\rangle\equiv\lvert i^A\rangle.
    \end{gather}
    In this way,
    \begin{align}
        \lvert AR_1\rangle=&\sum_i\lambda_i\lvert i^A\rangle\lvert\psi_i^R\rangle,\\
        \lvert AR_2\rangle=&\sum_i\lambda_i\lvert i^A\rangle\lvert\varphi_i^R\rangle
    \end{align}
    Define the unitary transformation as
    \begin{align}
        U_R=\sum_i\lvert\psi_i^R\rangle\langle\varphi_i^R\rvert.
    \end{align}
    Then
    \begin{align}
        \notag(I_A\otimes U_R)\lvert AR_2\rangle=&\left(I_A\otimes\sum_j\lvert\psi_i^R\rangle\langle\varphi_i^R\rvert\right)\sum_i\lambda_i\lvert i^A\rangle\lvert\varphi_i^R\rangle=\sum_{i,j}\lambda_i\lvert i^A\rangle\lvert\psi_j^R\rangle\langle\varphi_j^R\vert\varphi_i^R\rangle=\sum_{i,j}\delta_{ij}\lambda_i\lvert i^A\rangle\lvert\psi_i^R\rangle=\sum_i\lambda_i\lvert i^A\rangle\lvert\psi_i^R\rangle\\
        =&\lvert AR_2\rangle.
    \end{align}
\end{pf}

\begin{exe}
    Suppose $\{p_i,\lvert\psi_i\rangle\}$ is an ensemble of states generating a density matrix $\rho=\sum_ip_i\lvert\psi_i\rangle\langle\psi_i\rvert$ for a quantum system $A$. Introduce a system $R$ with orthonormal basis $\lvert i\rangle$.
    \begin{itemize}
        \item[(1)] Show that $\sum_i\sqrt{p_i}\lvert\psi_i\rangle\lvert i\rangle$ is a purification of $\rho$.
        \item[(2)] Suppose we measure $R$ in the basis $\lvert i\rangle$, obtaining outcome $i$. With what probability do we obtain the result $i$, and what is the corresponding state of system $A$.
        \item[(3)] Let $\lvert AR\rangle$ be any purification of $\rho$ to the system $AR$. Show that there exists an orthonormal basis $\lvert i\rangle$ in which $R$ can be measured such that the corresponding post-measurement state for system $A$ is $\lvert\psi_i\rangle$ with probability $p_i$.
    \end{itemize}
\end{exe}
\begin{sol}
    \begin{itemize}
        \item[(1)] Since
        \begin{align}
            \tr_R\left[\left(\sum_i\sqrt{p_i}\lvert\psi_i\rangle\lvert i\rangle\right)\left(\sum_j\sqrt{p_j^*}\langle\psi_j\rvert\langle j\rvert\right)\right]=\sum_ip_i\lvert\psi_i\rangle\langle\psi_i\rvert=\rho,
        \end{align}
        $\sum_i\sqrt{p_i}\lvert\psi_i\rangle\lvert i\rangle$ is a purification of $\rho$.
        \item[(2)] The probability of obtaining $i$ is
        \begin{align}
            \notag p(i)=&\tr\left[(I^A\otimes M_i^R)^\dagger(I_A\otimes M_i^R)\left(\sum_j\sqrt{p_j}\lvert\psi_j\rangle\lvert j\rangle\right)\left(\sum_k\sqrt{p_k^*}\langle\psi_k\rvert\langle k\rvert\right)\right]\\
            \notag=&\tr\left[\sum_{j,k}\sqrt{p_jp_k^*}\lvert\psi_j\rangle\langle\psi_k\rvert\otimes\lvert i\rangle\langle i\vert i\rangle\langle i\vert j\rangle\langle k\rvert\right]=\tr\left[\sum_{j,k}\delta_{ij}\sqrt{p_jp_k^*}\lvert\psi_j\rangle\langle\psi_k\rvert\otimes\lvert i\rangle\langle k\rvert\right]\\
            =&\tr\left[\sum_k\sqrt{p_ip_k^*}\lvert\psi_i\rangle\langle\psi_k\rvert\otimes\lvert i\rangle\langle k\rvert\right]=\abs{p_i}.
        \end{align}
        The state of the joint system after measurement is
        \begin{align}
            \notag\frac{(I^A\otimes M_i^R)^{\dagger}\left(\sum_j\sqrt{p_j}\lvert\psi_j\rangle\lvert j\rangle\right)\left(\sum_k\sqrt{p_k^*}\langle\psi_k\rvert\langle k\rvert\right)(I_A\otimes M_i^R)}{p(i)}=&\frac{\sum_{j,k}\sqrt{p_jp_k^*}\lvert\psi_j\rangle\langle\psi_k\rvert\lvert i\rangle\langle i\vert j\rangle\langle k\vert i\rangle\langle i\rvert}{\abs{p_i}}\\
            \notag=&\frac{\sum_{j,k}\delta_{ij}\delta_{ik}\sqrt{p_jp_k^*}\lvert\psi_j\rangle\langle\psi_k\rvert\otimes\lvert i\rangle\langle i\rvert}{\abs{p_i}}\\
            =&\frac{\abs{p_i}\lvert\psi_i\rangle\langle\psi_i\rvert\otimes\lvert i\rangle\langle i\rvert}{\abs{p_i}}=\lvert\psi_j\rangle\langle\psi_k\rvert\otimes\lvert i\rangle\langle i\rvert.
        \end{align}
        The corresponding state of system $A$ is
        \begin{align}
            \tr_R(\lvert\psi_j\rangle\langle\psi_k\rvert\otimes\lvert i\rangle\langle i\rvert)=\lvert\psi_i\rangle\langle\psi_i\rvert,
        \end{align}
        i.e. $\lvert\psi_i\rangle$.
        \item[(3)] Suppose the Schmidt decomposition of the purification $\lvert AR\rangle$ is
        \begin{align}
            \lvert AR\rangle=\sum_i\sqrt{q_i}\lvert\varphi_i^A\rangle\lvert\varphi_i^R\rangle.
        \end{align}
        According to Theorem 2.6, there exists a unitary matrix $u_{ij}$ such that
        \begin{align}
            \sqrt{q_i}\lvert\varphi_i^A\rangle=\sum_{j}u_{ij}\sqrt{p_j}\lvert\psi_i\rangle.
        \end{align}
        In this way,
        \begin{align}
            \lvert AR\rangle=\sum_i\left(\sum_{j}u_{ij}\sqrt{p_j}\lvert\psi_i\rangle\right)\lvert\varphi_i^R\rangle=\sum_j\sqrt{p_j}\lvert\psi_j\rangle\left(\sum_iu_{ij}\lvert\varphi_i^R\rangle\right)=\sum_j\sqrt{p_j}\lvert\psi_j\rangle\lvert j\rangle=\sum_i\sqrt{p_i}\lvert\psi_i\rangle\lvert i\rangle,
        \end{align}
        where
        \begin{align}
            \lvert i\rangle=\sum_ju_{ji}\lvert\varphi_j^R\rangle.
        \end{align}
        Therefore, there exists an orthonormal basis $\lvert i\rangle$ in which $R$ can be measured such that the corresponding post-measurement state for system $A$ is $\lvert\psi_i\rangle$ with probability $p_i$.
    \end{itemize}
\end{sol}

\section{EPR and the Bell Inequality}

\begin{prob}[Functions of the Pauli matrices]
    Let $f(\cdot)$ be any function from complex numbers to complex numbers. Let $\vec{n}$ be a normalized vector in three dimensions, and let $\theta$ be real. Show that
    \begin{align}
        \label{P2.1}
        f(\theta\vec{n}\cdot\vec{\sigma})=\frac{f(\theta)+f(-\theta)}{2}I+\frac{f(\theta)-f(-\theta)}{2}\vec{n}\cdot\vec{\sigma}.
    \end{align}
\end{prob}
\begin{pf}
    The left side of equation \eqref{P2.1} is
    \begin{align}
        \notag f(\theta\vec{n}\cdot\vec{\sigma})=\sum_{k=0}^{\infty}\frac{1}{k!}f^{(k)}(0)(\theta\vec{n}\cdot\vec{\sigma})^k=\sum_{k=0}^{\infty}\frac{1}{(2k)!}(\theta\vec{n}\cdot\vec{\sigma})^{2k}+\sum_{k=0}^{\infty}\frac{1}{(2k+1)!}(\theta\vec{n}\cdot\vec{\sigma})^{2k+1}.
    \end{align}
    Note that
    \begin{align}
        (\vec{n}\cdot\vec{\sigma})^2=\left(\sum_{i=1}^3n_i\sigma_i\right)^2=\sum_{i,j=1}^3v_iv_j\sigma_i\sigma_j.
    \end{align}
    Using the anti commutation relation between the Pauli matrices,
    \begin{align}
        \{\sigma_i,\sigma_j\}=\sigma_i\sigma_j+\sigma_j\sigma_i=2\delta_{ij}I=\left\{\begin{array}{ll}
            2I,&i=j;\\
            0,&i\neq j,
        \end{array}\right.
    \end{align}
    we have
    \begin{align}
        (\vec{n}\cdot\vec{\sigma})^2=\sum_in_i^2I=I.
    \end{align}
    Hence the left side of equation \eqref{P2.1} can be written as
    \begin{align}
        f(\theta\vec{n}\cdot\vec{\sigma})=\sum_{k=0}^{\infty}\frac{1}{(2k)!}f^{(2k)}(0)\theta^{2k}+\sum_{k=0}^{\infty}\frac{1}{(2k+1)!}f^{(2k+1)}(0)\theta^{2k+1}\vec{n}\cdot\vec{\sigma}.
    \end{align}
    Since
    \begin{align}
        f(\theta)+f(-\theta)=&\sum_{k=0}^{\infty}\frac{1}{k!}f^{(k)}(0)\theta^k+\sum_{k=0}^{\infty}\frac{1}{k!}f^{(k)}(0)(-\theta)^k=\sum_{k=0}^{\infty}\frac{1}{(2k)!}f^{(2k)}(0)\theta^{2k},\\
        f(\theta)-f(-\theta)=&\sum_{k=0}^{\infty}\frac{1}{k!}f^{(k)}(0)\theta^k-\sum_{k=0}^{\infty}\frac{1}{k!}f^{(k)}(0)(-\theta)^k=\sum_{k=0}^{\infty}\frac{1}{(2k+1)!}f^{(2k+1)}(0)\theta^{2k+1},
    \end{align}
    the left side of equation \eqref{P2.1} equals its right side,
    \begin{align}
        f(\theta\vec{n}\cdot\vec{\sigma})=\frac{f(\theta)-f(-\theta)}{2}I+\frac{f(\theta)-f(-\theta)}{2}\vec{n}\cdot\vec{\sigma}.
    \end{align}
    Therefore, equation \eqref{P2.1} holds.
\end{pf}

\begin{prob}[Properties of the Schmidt number]
    Suppose $\lvert\psi\rangle$ is a pure state of a composite system with components $A$ and $B$.
    \begin{itemize}
        \item[(1)] Prove that the Schmidt number of $\lvert\psi\rangle$ is equal to the rank of the reduced density matrix $\rho_A\equiv\tr_B(\lvert\psi\rangle\langle\psi\rvert)$. (Note that the rank of a Hermitian operator is equal to the dimension of its support.)
        \item[(2)] Suppose $\lvert\psi\rangle=\sum_j\lvert\alpha_j\rangle\lvert\beta_j\rangle$ is a representation for $\lvert\psi\rangle$, where $\lvert\alpha_i\rangle$ and $\lvert\beta_j\rangle$ are (un-normalized) states for system $A$ and $B$, respectively. Prove that the number of terms in such a decomposition is greater than or equal to the Schmidt number of $\lvert\psi\rangle$, $\text{Sch}(\psi)$.
        \item[(3)] Suppose $\lvert\psi\rangle=\alpha\lvert\varphi\rangle+\beta\lvert\gamma\rangle$. Prove that
        \begin{align}
            \text{Sch}(\psi)\geq\abs{\text{Sch}(\varphi)-\text{Sch}(\gamma)}.
        \end{align}
    \end{itemize}
\end{prob}
\begin{pf}
    \begin{itemize}
        \item[(1)] Suppose the Schmidt decomposition of $\lvert\psi\rangle$ is
        \begin{align}
            \lvert\psi\rangle=\sum_{i=1}^{\text{Sch}(\psi)}\lambda_i\lvert\psi_i^A\rangle\lvert\psi_i^B\rangle.
        \end{align}
        The reduced density matrix is
        \begin{align}
            \notag\rho_A=&\tr_B(\lvert\psi\rangle\langle\psi\rvert)=\tr_B\left[\left(\sum_{i=1}^{\text{Sch}(\psi)}\lambda_i\lvert\psi_i^A\rangle\lvert\psi_i^B\rangle\right)\left(\sum_{j=1}^{\text{Sch}(\psi)}\lambda_j^*\langle\psi_j^A\rvert\langle\psi_j^B\rvert\right)\right]\\
            =&\tr_B\left(\sum_{i,j=1}^{\text{Sch}(\psi)}\lambda_i\lambda_j^*\lvert\psi_i^A\rangle\langle\psi_j^A\rvert\otimes\lvert\psi_i^B\rangle\langle\psi_j^B\rvert\right)=\sum_{i=1}^{\text{Sch}(\psi)}\abs{\lambda_i}^2\lvert\psi_i^A\rangle\langle\psi_i^A\rvert,
        \end{align}
        whose rank is equal to the Schmidt number $\text{Sch}(\psi)$.
        \item[(2)] Suppose the number of terms in the decomposition, $\lvert\psi\rangle=\sum_j\lvert\alpha_j\rangle\lvert\beta_j\rangle$, $J$ is less than $\text{Sch}(\psi)$.
        The reduced density matrix is
        \begin{align}
            \notag\rho_A=&\tr_B(\lvert\psi\rangle\langle\psi\rvert)=\tr_B\left[\left(\sum_{j=1}^J\lvert\alpha_j\rangle\lvert\beta_j\rangle\right)\left(\sum_{k=1}^J\langle\alpha_k\rvert\langle\beta_k\rvert\right)\right]=\tr_B\left(\sum_{j,k=1}^J\lvert\alpha_j\rangle\lvert\beta_j\rangle\langle\alpha_k\rvert\langle\beta_k\rvert\right)\\
            =&\sum_{i=1}^{\text{Sch}(\psi)}\sum_{j,k=1}^J\lvert\alpha_j\rangle\langle\alpha_k\rvert\langle\psi_i^B\vert\beta_j\rangle\langle\beta_k\vert\psi_i^B\rangle=\sum_{j,k=1}^J\left(\sum_{i=1}^{\text{Sch}(\psi)}\langle\psi_i^B\vert\beta_j\rangle\langle\beta_k\vert\psi_i^B\rangle\right)\lvert\alpha_j\rangle\langle\alpha_k\rvert
        \end{align}
        Even if $\lvert\alpha_j\rangle$ are all independent, the rank of the reduced density matrix $\rho^A$ is only $J$, which is less than $\text{Sch}(\psi)$, and thus less than the rank of $\rho^A$.
        Therefore, the supposition is incorrect and the number of terms in such a decomposition is greater than or equal to the Schmidt number of $\lvert\psi\rangle$, $\text{Sch}(\psi)$.
        \item[(3)] Suppose the Schmidt decompositions of $\lvert\varphi\rangle$ and $\lvert\gamma\rangle$ are respectively
        \begin{align}
            \lvert\varphi\rangle=&\sum_{i=1}^{\text{Sch}(\varphi)}\lvert\varphi_i^A\rangle\lvert\varphi_i^B\rangle,\\
            \lvert\gamma\rangle=&\sum_{i=1}^{\text{Sch}(\gamma)}\lvert\gamma_i^A\rangle\lvert\gamma_i^B\rangle.
        \end{align}
        Without loss of generality, we assume $\text{Sch}(\varphi)\geq\text{Sch}(\gamma)$ and thus the original problem is converted to proving that $\text{Sch}(\varphi)+\text{Sch}(\gamma)\geq\text{Sch}(\psi)$. From
        \begin{align}
            \lvert\psi\rangle=\alpha\lvert\varphi\rangle+\beta\lvert\gamma\rangle,
        \end{align}
        we have
        \begin{align}
            \lvert\varphi\rangle=\frac{\beta}{\alpha}\lvert\gamma\rangle-\frac{1}{\alpha}\lvert\psi\rangle=\frac{\beta}{\alpha}\sum_{i=1}^{\text{Sch}(\gamma)}\lvert\gamma_i^A\rangle\lvert\gamma_i^B\rangle-\frac{1}{\alpha}\sum_{i=1}^{\text{Sch}(\psi)}\lvert\psi_i^A\rangle\lvert\psi_i^B\rangle=\sum_{i=1}^{\text{Sch}(\gamma)+\text{Sch}(\psi)}\delta_i\lvert a_i\rangle\lvert b_i\rangle,
        \end{align}
        where
        \begin{align}
            \delta_i=&\left\{\begin{array}{ll}
                \frac{\beta}{\alpha},&1\leq i\leq\text{Sch}(\gamma);\\
                -\frac{1}{\alpha},&\text{Sch}(\gamma)<i\leq\text{Sch}(\gamma)+\text{Sch}(\psi),
            \end{array}\right.\\
            a_i=&\left\{\begin{array}{ll}
                \gamma_i^A,&1\leq i\leq\text{Sch}(\gamma);\\
                \psi_i^A,&\text{Sch}(\gamma)<i\leq\text{Sch}(\gamma)+\text{Sch}(\psi),
            \end{array}\right.\\
            b_i=&\left\{\begin{array}{ll}
                \gamma_i^B,&1\leq i\leq\text{Sch}(\gamma);\\
                \psi_i^B,&\text{Sch}(\gamma)<i\leq\text{Sch}(\gamma)+\text{Sch}(\psi).
            \end{array}\right.
        \end{align}
        Using the conclusion obtained in (2), we have
        \begin{align}
            \text{Sch}(\gamma)+\text{Sch}(\psi)\geq\text{Sch}(\varphi).
        \end{align}
        Therefore,
        \begin{align}
            \text{Sch}(\psi)\geq\abs{\text{Sch}(\varphi)-\text{Sch}(\gamma)}.
        \end{align}
    \end{itemize}
\end{pf}

\begin{prob}[Tsirelson's inequality]
    Suppose $Q=\vec{q}\cdot\vec{\sigma}$, $R=\vec{r}\cdot\vec{\sigma}$, $S=\vec{s}\cdot\vec{\sigma}$, $T=\vec{t}\cdot\vec{\sigma}$, where $\vec{q}$, $\vec{r}$, $\vec{s}$ and $\vec{t}$ are real unit vectors in three dimensions. Show that
    \begin{align}
        (Q\otimes S+R\otimes S+R\otimes T-Q\otimes T)^2=4I+[Q,R]\otimes[S,T].
    \end{align}
    Use this result to prove that
    \begin{align}
        \langle Q\otimes S\rangle+\langle R\otimes S\rangle+\langle R\otimes T\rangle-\langle Q\otimes T\rangle\leq 2\sqrt{2},
    \end{align}
    so the violation of the Bell inequality found in Equation (2.230) is the maximum possible in quantum mechanics.
\end{prob}
\begin{pf}
    \begin{gather}
        Q=\vec{q}\cdot\vec{\sigma}=q_1\sigma_1+q_2\sigma_2+q_3\sigma_3=q_1\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]+q_2\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]+q_3\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]=\left[\begin{matrix}
            q_3&q_1-iq_2\\
            q_1+iq_2&-q_3
        \end{matrix}\right],\\
        \Longrightarrow Q^2=\left[\begin{matrix}
            q_3&q_1-iq_2\\
            q_1+iq_2&-q_3
        \end{matrix}\right]\left[\begin{matrix}
            q_3&q_1-iq_2\\
            q_1+iq_2&-q_3
        \end{matrix}\right]=\left[\begin{matrix}
            q_1^2+q_2^2+q_3^2&0\\
            0&q_1^2+q_2^2+q_3^2
        \end{matrix}\right]=\left[\begin{matrix}
            1&0\\
            0&1
        \end{matrix}\right]=I.
    \end{gather}
    Similarly,
    \begin{align}
        R^2=S^2=T^2=I.
    \end{align}
    Hence
    \begin{align}
        \notag(Q\otimes S+R\otimes S+R\otimes T-Q\otimes T)^2=&QQ\otimes SS+QR\otimes SS+QR\otimes ST-QQ\otimes ST\\
        \notag&+RQ\otimes SS+RR\otimes SS+RR\otimes ST-RQ\otimes ST\\
        \notag&+RQ\otimes TS+RR\otimes TS+RR\otimes TT-RQ\otimes TT\\
        \notag&-QQ\otimes TS-QR\otimes TS-QR\otimes TT+QQ\otimes TT\\
        \notag=&I\otimes I+QR\otimes I+QR\otimes ST-I\otimes ST\\
        \notag&+RQ\otimes I+I\otimes I+I\otimes ST-RQ\otimes ST\\
        \notag&+RQ\otimes TS+I\otimes TS+I\otimes I-RQ\otimes I\\
        \notag&-I\otimes TS-QR\otimes TS-QR\otimes I+I\otimes I\\
        \notag=&4I+QR\otimes ST-RQ\otimes ST+RQ\otimes TS-QR\otimes TS\\
        =&4I+(QR-RQ)\otimes(ST-TS)=4I+[Q,R]\otimes[S,T].
    \end{align}
    The average of the above equation is
    \begin{align}
        \langle(Q\otimes S+R\otimes S+R\otimes T-Q\otimes T)^2\rangle=\langle 4I+[Q,R]\otimes[S,T]\rangle=4\langle I\rangle+\langle[Q,S]\rangle\otimes\langle[S,T]\rangle=4+\langle[Q,R]\rangle\otimes\langle[S,T]\rangle,
    \end{align}
    where
    \begin{align}
        \langle[Q,R]\rangle=\langle\psi\rvert[Q,R]\lvert\psi\rangle=\langle\psi\rvert QR-RQ\lvert\psi\rangle=\langle\psi\rvert QR\lvert\rangle-\langle\psi\rvert RQ\lvert\psi\rangle=2i\re[\langle\psi\rvert QR\lvert\psi\rangle]=2i\re[\langle QR\rangle].
    \end{align}
    Note that both $Q$ and $R$ are unitarity, so $QR$ is unitary. Since unitary operators preserve the inner products and thus lengths of vectors, $QR\lvert\psi\rangle$ is still a normalized vector and $\abs{\langle[Q,R]\rangle}\leq 1\Longrightarrow-1\leq\re[\langle QR\rangle]\leq 1$.
    Similarly,
    \begin{align}
        \langle[S,T]\rangle=2i\re[\langle ST\rangle].
    \end{align}
    and $-1\leq\re[\langle ST\rangle]\leq 1$.
    Therefore,
    \begin{align}
        \notag\langle Q\otimes S\rangle+\langle R\otimes S\rangle+\langle R\otimes T\rangle-\langle Q\otimes T\rangle\leq&\sqrt{\langle(Q\otimes S+R\otimes S+R\otimes T-Q\otimes T)^2\rangle}=\sqrt{4+\langle[Q,R]\rangle\otimes\langle[S,T]\rangle}\\
        =&\sqrt{4-4\re[\langle QR\rangle]\re[\langle ST\rangle]}\leq 2\sqrt{2}.
    \end{align}
\end{pf}

\ifx\allfiles\undefined
\bibliographystyle{plain}
\bibliography{References}
\end{document}
\fi