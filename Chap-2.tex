% !Tex program = pdflatex
% Chapter 2 - Introduction to quantum mechanics
\ifx\allfiles\undefined
\documentclass[en]{sol-man}
\begin{document}
\fi
\chapter{Introduction to quantum mechanics}

\section{Linear algebra}

\begin{exe}[Linear dependence: example]
    Show that $(1,-1)$, $(1,2)$ and $(2,1)$ are Linearly dependent.
\end{exe}
\begin{pf}
    Since
    \begin{align}
        (1,-1)+(1,2)-(2,1)=0,
    \end{align}
    these three vectors are linearly dependent.
\end{pf}

\begin{exe}[Matrix representations: example]
    Suppose $V$ is a vector space with basis vectors $\lvert 0\rangle$ and $\lvert 1\rangle$, and $A$ is a linear operator from $V$ to $V$ such that $A\lvert 0\rangle=\lvert 1\rangle$ and $A\lvert 1\rangle=\lvert 0\rangle$. Give a matrix representation for $A$, with respect to the input basis $\lvert 0\rangle$, $\lvert 1\rangle$, and the output basis $\lvert 0\rangle$ and $\lvert 1\rangle$. Find input and output bases which give rise to a different matrix representation of $A$.
\end{exe}
\begin{sol}
    The matrix representation for $A$ with respect to the input basis $\lvert 0\rangle$, $\lvert 1\rangle$ and the output basis $\lvert 0\rangle$ and $\lvert 1\rangle$ is
    \begin{align}
        \left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right].
    \end{align}

    Keep $\lvert 0\rangle$ and $\lvert 1\rangle$ as the input basis and choose $\lvert+\rangle=(\lvert 0\rangle+\lvert 1\rangle)/\sqrt{2}$ and $\lvert-\rangle=(\lvert 0\rangle-\lvert 1\rangle)/\sqrt{2}$ as the output basis, then $A$ can be regarded as a linear operator from $V$ to $V$ such that $A\lvert 0\rangle=(\lvert+\rangle-\lvert-\rangle)/\sqrt{2}$ and $A\lvert 1\rangle=(\lvert 0\rangle+\lvert 1\rangle)/\sqrt{2}$. In this way, the matrix representation for $A$ is
    \begin{align}
        \frac{1}{\sqrt{2}}\left[\begin{matrix}
            1&-1\\
            1&1
        \end{matrix}\right].
    \end{align}
\end{sol}

\begin{exe}[Matrix representation for operator products]
    Suppose $A$ is a linear operator from vector space $V$ to vector space $W$, and $B$ is a linear operator from vector spaces $W$ to vector space $X$. Let $\lvert v_i\rangle$, $\lvert w_j\rangle$, and $\lvert x_k\rangle$ be bases for the vector spaces $V$, $W$, and $X$, respectively. Show that the matrix representation for the linear transformation $BA$ is the matrix product of the matrix representations for $B$ and $A$, with respect to the appropriate bases.
\end{exe}
\begin{pf}
    Suppose the dimension of vector spaces $V$, $W$, and $W$ are $l$, $m$, $n$, respectively. Since $A$ is a linear operator from vector space $V$ to vector space $W$, for each $i$ in the range $1,\cdots,l$, there exist complex numbers $A_{1i}$ through $A_{mi}$ such that
    \begin{align}
        A\lvert v_i\rangle=\sum_jA_{ji}\lvert w_j\rangle,
    \end{align}
    where $A_{ji}$ is the entries of the matrix representation of the operator $A$.
    Since $B$ is a linear operator from vector space $W$ to vector space $X$, for each $j$ in the range $1,\cdots,m$, there exist complex numbers $A_{1j}$ through $A_{nj}$ such that
    \begin{align}
        B\lvert w_j\rangle=\sum_kB_{kj}\lvert x_k\rangle,
    \end{align}
    where $B_{kj}$ is the entries of the matrix representation of the operator $B$.
    Putting the above two equations together, we have
    \begin{align}
        BA\lvert v_i\rangle=B\sum_jA_{ji}\lvert w_j\rangle=\sum_jA_{ji}\sum_kB_{kj}\lvert x_k\rangle=\sum_k\left(\sum_jB_{kj}A_{ji}\right)\lvert x_k\rangle=\sum_k(BA)_{ki}\lvert x_k\rangle.
    \end{align}
    where $(AB)_{ki}$ is the entries of the matrix representation of the operator $BA$. Therefore, the matrix representation for the linear transformation $BA$ is the matrix product of the matrix representations for $B$ and $A$, with respect to the appropriate bases.
\end{pf}

\begin{exe}[Matrix representation for identity]
    Show that the identity operator on a vector space $V$ has a matrix representation which is one along the diagonal and zero everywhere else, if the matrix representation is taken with respect to the same input and output bases. The matrix is known as the \emph{identity matrix}.
\end{exe}
\begin{pf}
    Suppose $I$ is the identity operator on the vector space $V$ and choose $\lvert v_1\rangle,\cdots,\lvert v_m\rangle$ as both the input basis and the output basis for $V$. Then for each $j$ in the range $1,\cdots,m$, there exist complex numbers $I_{1j}$ through $I_{mj}$ such that
    \begin{gather}
        I\lvert v_j\rangle=\sum_iA_{ij}\lvert v_i\rangle=\lvert v_j\rangle,\\
        \Longrightarrow(A_{jj}-1)\lvert v_j\rangle+\sum_{i\neq j}A_{ij}\lvert v_i\rangle=0,
    \end{gather}
    where $A_{ij}$ is the entries of the matrix representation of the operator $I$. Due to linear independence of the basis, there must be
    \begin{align}
        v_{ij}=\left\{\begin{array}{ll}
            1,&i=j;\\
            0,&i\neq j,
        \end{array}\right.=\delta_{ij},\quad\forall i,j=1,\cdots,m.
    \end{align}
    i.e., the identity operator on the vector space $V$ has a matrix representation which is one along the diagonal and zero everywhere else.
\end{pf}

\begin{exe}
    Verify that $(\cdot,\cdot)$ just defined is an inner product on $\mathbb{C}^n$.
\end{exe}
\begin{pf}
    $(\cdot,\cdot)$ just defined satisfies the requirements that:
    \begin{itemize}
        \item[(1)] $(\cdot,\cdot)$ is linear in the second argument,
        \begin{align}
            \notag\left((y_1,\cdots,y_n),\sum_j\lambda_j(z_1^{(j)},\cdots,z_n^{(j)})\right)=&\left((y_1,\cdots,y_n),(\sum_j\lambda_jz_1^{(j)},\cdots,\sum_j\lambda_jz_n^{(j)})\right)=\sum_iy_i^*\sum_j\lambda_jz_i^{(j)}\\
            =&\sum_j\lambda_j\sum_iy_i^*z_i^{(j)}=\sum_j\lambda_j((y_1,\cdots,y_n),(z_1^{(j)},\cdots,z_n^{(j)})).
        \end{align}
        \item[(2)] 
        \begin{align}
            ((y_1,\cdots,y_n),(z_1,\cdots,z_n))=\sum_iy_i^*z_i=\left(\sum_iz_i^*y_i\right)^*=((z_1,\cdots,z_n),(y_1,\cdots,y_n))^*.
        \end{align}
        \item[(3)] 
        \begin{align}
            ((y_1,\cdots,y_n),(y_1,\cdots,y_n))=\sum_iy_i^*y_i=\sum_i\abs{y_i}^2\geq 0,
        \end{align}
        with equality if and only if $(y_1,\cdots,y_n)=0$.
    \end{itemize}
    Therefore, $(\cdot,\cdot)$ is an inner product on $\mathbb{C}^n$.
\end{pf}

\begin{exe}
    Show that any inner product $(\cdot,\cdot)$ is conjugate-linear in the first argument,
    \begin{align}
        \left(\sum_i\lambda_i\lvert w_i\rangle,\lvert v\rangle\right)=\sum_i\lambda_i^*(\lvert w_i\rangle,\lvert v\rangle).
    \end{align}
\end{exe}
\begin{pf}
    \begin{align}
        \left(\sum_i\lambda_i\lvert w_i\rangle,\lvert v\rangle\right)=\left(\lvert v\rangle,\sum_i\lambda_i\lvert w_i\rangle\right)^*=\left(\sum_i\lambda_i(\lvert v\rangle,\lvert w_i\rangle)\right)^*=\sum_i\lambda_i^*(\lvert w_i\rangle,\lvert v\rangle).
    \end{align}
    Therefore, any inner product $(\cdot,\cdot)$ is conjugate-linear in the first argument.
\end{pf}

\begin{exe}
    Verify that $\lvert w\rangle\equiv(1,1)$ and $\lvert v\rangle\equiv(1,-1)$ are orthogonal. What are the normalized forms of these vectors?
\end{exe}
\begin{sol}
    Since
    \begin{align}
        (\lvert w\rangle,\lvert v\rangle)=1\times 1+1\times(-1)=0,
    \end{align}
    these two vectors are orthogonal. The normalized form of $\lvert w\rangle$ and $\lvert v\rangle$ are
    \begin{align}
        \frac{\lvert w\rangle}{\norm{\lvert w\rangle}}=\frac{1}{\sqrt{2}}(1,1),
    \end{align}
    and
    \begin{align}
        \frac{\lvert v\rangle}{\norm{\lvert v\rangle}}=\frac{1}{\sqrt{2}}(1,-1),
    \end{align}
    respectively.
\end{sol}

\begin{exe}
    Prove that the Gram-Schmidt procedure produces an orthonormal basis for $V$.
\end{exe}
\begin{pf}
    Obviously, $\lvert v_1\rangle,\cdots,\lvert v_n\rangle$ are normalized, so we first prove that $\lvert v_1\rangle,\cdots,\lvert v_d\rangle$ are orthogonal with induction.
    For $k=1$,
    \begin{align}
        (\lvert v_1\rangle,\lvert v_{k+1}\rangle)=&(\lvert v_1,\lvert v_2\rangle\rangle)=\left(\lvert v_1\rangle,\frac{\lvert w_2\rangle-\langle v_1\vert w_2\rangle\lvert v_1\rangle}{\norm{\lvert v_1\rangle-\langle v_1\vert w_2\rangle\lvert v_1\rangle}}\right)=\frac{\langle v_1\vert w_2\rangle-\langle v_1\vert w_2\rangle\langle v_1\vert v_1\rangle}{\norm{\lvert v_1\rangle-\langle v_1\vert w_2\rangle\lvert v_1\rangle}}=\frac{\langle v_1\vert w_2\rangle-\langle v_1\vert w_2\rangle}{\norm{\lvert v_1\rangle-\langle v_1\vert w_2\rangle\lvert v_1\rangle}}=0,\\
    \end{align}
    so $\lvert v_1\rangle,\cdots,\lvert v_{k+1}\rangle$ are orthogonal for $k=1$.
    For $k\geq 2$, if $\lvert v_1\rangle,\cdots,\lvert v_k\rangle$ are orthogonal, then
    \begin{align}
        \notag(\lvert v_j\rangle,\lvert v_{k+1}\rangle)=&\left(\lvert v_j\rangle,\frac{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}{\norm{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}}\right)=\frac{\langle v_j\vert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\langle v_j\vert v_i\rangle}{\norm{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}}\\
        =&\frac{\langle v_j\vert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\delta_{ji}}{\norm{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}}=\frac{\langle v_j\vert w_{k+1}\rangle-\langle v_j\vert w_{k+1}\rangle}{\norm{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}}=0,\quad\forall j=1,\cdots,k,
    \end{align}
    so $\lvert v_1\rangle,\cdots,\lvert v_{k+1}\rangle$ are orthogonal for $k\geq 2$, such as $k=d-1$.
    Till now, we proved that $\lvert v_1\rangle,\cdots,\lvert v_d\rangle$ are orthonormal.

    We then prove that $\lvert v_1\rangle,\cdots,\lvert v_n\rangle$ are a basis for $V$.
    As proved above, $\lvert v_1\rangle,\cdots,\lvert v_d\rangle$ are orthonormal, and thus linear independent. Since $\lvert w_1\rangle,\cdots,\lvert w_d\rangle$ are a basis for $V$. Any vector $\lvert v\rangle$ in $V$ can be written as a linear combination of $\lvert w_1\rangle,\cdots,\lvert w_d\rangle$:
    \begin{align}
        \lvert v\rangle=\sum_{i=1}^da_i\lvert w_i\rangle.
    \end{align}
    Using
    \begin{align}
        \lvert v_1\rangle=\frac{\lvert w_1\rangle}{\norm{\lvert w_1\rangle}}\Longrightarrow \lvert w_1\rangle=\norm{\lvert w_1\rangle}\lvert v_1\rangle,\\
    \end{align}
    and
    \begin{align}
        \notag\lvert v_{k+1}\rangle=&\frac{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}{\norm{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}}\\
        \Longrightarrow&\lvert w_{k+1}\rangle=\norm{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}\lvert v_{k+1}\rangle+\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle,\quad\forall k=1,\cdots,d-1
    \end{align}
    we can rewrite $\lvert v\rangle$ as a linear combination of $\lvert v_1\rangle,\cdots,\lvert v_d\rangle$:
    \begin{align}
        \lvert v\rangle=a_1\norm{\lvert w_1\rangle}\lvert v_1\rangle+\sum_{k=1}^{d-1}a_{k+1}\left(\norm{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}\lvert v_{k+1}\rangle+\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle\right),
    \end{align}
    so $\lvert v_1\rangle,\cdots,\lvert v_d\rangle$ span and form a basis for $V$.

    Therefore, the Gram-Schmidt procedure produces an orthonormal basis for $V$.
\end{pf}

\begin{exe}[Pauli operators and the outer product]
    The Pauli matrices (Figure 2.2 on page 65) can be considered as operators with respect to an orthonormal basis $\lvert 0\rangle$, $\lvert 1\rangle$ for a two-dimensional Hilbert space. Express each of the Pauli operators in the outer product notion.
\end{exe}
\begin{sol}
    The Pauli operators in the outer product notion:
    \begin{align}
        \sigma_0=&\sum_{m,n=1}^2\langle m\rvert\sigma_0\lvert n\rangle\lvert m\rangle\langle n\rvert=\lvert 0\rangle\langle 0\rvert+\lvert 1\rangle\langle 1\rvert,\\
        \sigma_1=&\sum_{m,n=1}^2\langle m\rvert\sigma_1\lvert n\rangle\lvert m\rangle\langle n\rvert=\lvert 0\rangle\langle 1\rvert+\lvert 1\rangle\langle 0\rvert,\\
        \sigma_2=&\sum_{m,n=1}^2\langle m\rvert\sigma_2\lvert n\rangle\lvert m\rangle\langle n\rvert=-i\lvert 0\rangle\langle 1\rvert+i\lvert 1\rangle\langle 0\rvert,\\
        \sigma_3=&\sum_{m,n=1}^2\langle m\rvert\sigma_3\lvert n\rangle\lvert m\rangle\langle n\rvert=\lvert 0\rangle\langle 0\rvert-\lvert 1\rangle\langle 1\rvert.
    \end{align}
\end{sol}

\begin{exe}
    Suppose $\lvert v_i\rangle$ is an orthonormal basis for and inner product space $V$. What is the matrix representation for the operator $\lvert v_j\rangle\langle v_k\rvert$, with respect to the $\lvert v_i\rangle$ basis?
\end{exe}
\begin{sol}
    The matrix representation for the operator $\lvert v_j\rangle\langle v_k\rvert$ with respect to the $\lvert v_i\rangle$ basis:
    \begin{align}
        \lvert v_j\rangle\langle v_k\rvert=\begin{array}{cc}
            \begin{array}{ccccccc}
                \multicolumn{7}{c}{k\text{th column}} \\
                 &  &  & \downarrow &  &  & 
                \end{array} & \begin{array}{c}
                    \\
                    \\
                   \end{array} \\
            \left[\begin{array}{ccccccc}
                0 &  &  &  & \multicolumn{3}{c}{\multirow{3}{*}{\Huge0}} \\
                 & \ddots &  &  & \multicolumn{3}{c}{} \\
                 &  & 0 &  & \multicolumn{3}{c}{} \\
                 &  &  & 1 &  &  &  \\
                \multicolumn{3}{c}{\multirow{3}{*}{\Huge0}} &  & 0 &  &  \\
                \multicolumn{3}{c}{} &  &  & \ddots &  \\
                \multicolumn{3}{c}{} &  &  &  & 0
                \end{array}\right] & \begin{array}{c}
                    \\
                    \\
                    \\
                   \leftarrow\,j\text{th row} \\
                    \\
                    \\
                    \\
                   \end{array}
        \end{array}
    \end{align}
    (a matrix with all zeros expect a one at the $j$th row and $k$th column).
\end{sol}

\begin{exe}[Eigendecomposition of the Pauli matrices]
    Find the eigenvectors, eigenvalues, and diagonal representations of the Pauli matrices $X$, $Y$, and $Z$.
\end{exe}
\begin{sol}
    The characteristic equation of $X$
    \begin{align}
        \det\abs{X-\lambda I}=\abs{\begin{matrix}
            -\lambda&1\\
            1&-\lambda
        \end{matrix}}=\lambda^2-1=0,
    \end{align}
    gives the eigenvalues
    \begin{align}
        \lambda_1=1,\quad\lambda_2=-1.
    \end{align}
    The eigenequations of $X$
    \begin{align}
        X\lvert v_1\rangle=\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right]=\left[\begin{matrix}
            b_1\\
            a_1
        \end{matrix}\right]=\lambda_1\lvert v_1\rangle=\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right],\quad X\lvert v_2\rangle=\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]\left[\begin{matrix}
            a_2\\
            b_2
        \end{matrix}\right]=\left[\begin{matrix}
            b_2\\
            a_2
        \end{matrix}\right]=\lambda_2\lvert v_2\rangle=\left[\begin{matrix}
            -a_2\\
            -b_2
        \end{matrix}\right],
    \end{align}
    give the corresponding eigenvectors
    \begin{align}
        \lvert v_1\rangle=\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            1
        \end{matrix}\right]=\frac{1}{\sqrt{2}}(\lvert 0\rangle+\lvert 1\rangle),\quad\lvert v_2\rangle=\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            -1
        \end{matrix}\right]=\frac{1}{\sqrt{2}}(\lvert 0\rangle-\lvert 1\rangle).
    \end{align}
    The diagonal representation of $X$ is
    \begin{align}
        X=\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]=\frac{1}{\sqrt{2}}(\lvert 0\rangle+\lvert 1\rangle)\frac{1}{\sqrt{2}}(\langle 0\rvert+\langle 1\rvert)-\frac{1}{\sqrt{2}}(\lvert 0\rangle-\lvert 1\rangle)\frac{1}{\sqrt{2}}(\langle 0\rvert-\langle 1\rvert).
    \end{align}
    The characteristic equation of $Y$
    \begin{align}
        \det\abs{Y-\lambda I}=\abs{\begin{matrix}
            -\lambda&-i\\
            i&-\lambda
        \end{matrix}}=\lambda^2-1=0,
    \end{align}
    gives the eigenvalues
    \begin{align}
        \lambda_1=1,\quad\lambda_2=-1.
    \end{align}
    The eigenequations of $Y$
    \begin{align}
        Y\lvert v_1\rangle=\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right]=\left[\begin{matrix}
            -ib_1\\
            ia_1
        \end{matrix}\right]=\lambda_1\lvert v_1\rangle=\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right],\quad Y\lvert v_2\rangle=\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]\left[\begin{matrix}
            a_2\\
            b_2
        \end{matrix}\right]=\left[\begin{matrix}
            -ib_2\\
            ia_2
        \end{matrix}\right]=\lambda_2\lvert v_2\rangle=\left[\begin{matrix}
            -a_2\\
            -b_2
        \end{matrix}\right],
    \end{align}
    give the corresponding eigenvectors
    \begin{align}
        \lvert v_1\rangle=\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            i
        \end{matrix}\right]=\frac{1}{\sqrt{2}}(\lvert 0\rangle+i\lvert 1\rangle),\quad\lvert v_2\rangle=\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            -i
        \end{matrix}\right]=\frac{1}{\sqrt{2}}(\lvert 0\rangle-i\lvert 1\rangle).
    \end{align}
    The diagonal representation of $Y$ is
    \begin{align}
        Y=\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]=\frac{1}{\sqrt{2}}(\lvert 0\rangle-i\lvert 1\rangle)\frac{1}{\sqrt{2}}(\langle 0\rvert+i\langle 1\rvert)-\frac{1}{\sqrt{2}}(\lvert 0\rangle+i\lvert 1\rangle)\frac{1}{\sqrt{2}}(\langle 0\rvert-i\langle 1\rvert).
    \end{align}
    The characteristic equation of $Z$
    \begin{align}
        \det\abs{Z-\lambda I}=\abs{\begin{matrix}
            1-\lambda&0\\
            0&-1-\lambda
        \end{matrix}}=\lambda^2-1=0,
    \end{align}
    gives the eigenvalues
    \begin{align}
        \lambda_1=1,\quad\lambda_2=-1.
    \end{align}
    The eigenequations of $Z$
    \begin{align}
        Z\lvert v_1\rangle=\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right]=\left[\begin{matrix}
            a_1\\
            -b_1
        \end{matrix}\right]=\lambda_1\lvert v_1\rangle=\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right],\quad Z\lvert v_2\rangle=\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]\left[\begin{matrix}
            a_2\\
            -b_2
        \end{matrix}\right]=\lambda_2\lvert v_2\rangle=\left[\begin{matrix}
            -a_2\\
            -b_2
        \end{matrix}\right],
    \end{align}
    give the corresponding eigenvectors
    \begin{align}
        \lvert v_1\rangle=\left[\begin{matrix}
            1\\
            0
        \end{matrix}\right]=\lvert 0\rangle,\quad\lvert v_2\rangle=\left[\begin{matrix}
            0\\
            1
        \end{matrix}\right].
    \end{align}
    The diagonal representation of $Z$ is
    \begin{align}
        Z=\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]=\lvert 0\rangle\langle 0\rvert-\lvert 1\rangle\langle 1\rvert.
    \end{align}
\end{sol}

\begin{exe}
    Prove that the matrix
    \begin{align}
        \left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]
    \end{align}
    is not diagonalizable.
\end{exe}
\begin{pf}
    The matrix is not normal,
    \begin{align}
        \left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]\left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]^{\dagger}=\left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]\left[\begin{matrix}
            1&1\\
            0&1
        \end{matrix}\right]=\left[\begin{matrix}
            1&1\\
            1&2
        \end{matrix}\right]\neq\left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]^{\dagger}\left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]=\left[\begin{matrix}
            1&1\\
            0&1
        \end{matrix}\right]\left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]=\left[\begin{matrix}
            2&1\\
            1&1
        \end{matrix}\right],
    \end{align}
    so it is nor diagonalizable.
\end{pf}

\begin{exe}
    If $\lvert w\rangle$ and $\lvert v\rangle$ are any two vectors, show that $(\lvert w\rangle\langle v\rvert)^{\dagger}=\lvert v\rangle\langle w\rvert$.
\end{exe}
\begin{pf}
    \begin{align}
        (\lvert w\rangle\langle v\rvert)^{\dagger}=\langle v\rvert^{\dagger}\lvert w\rangle^{\dagger}=\lvert v\rangle\langle w\rvert.
    \end{align}
\end{pf}

\begin{exe}[Anti-linearity of the adjoint]
    Show tha the adjoint operation is anti-linear,
    \begin{align}
        \left(\sum_ia_iA_i\right)^{\dagger}=\sum_ia_i^*A_i^{\dagger}.
    \end{align}
\end{exe}
\begin{pf}
    For two arbitrary vectors $\lvert v\rangle$ and $\lvert w\rangle$,
    \begin{align}
        \left(\left(\sum_ia_iA_i\right)^{\dagger}\lvert v\rangle,\lvert w\rangle\right)=\left(\lvert v\rangle,\sum_ia_iA_i\lvert w\rangle\right)=\sum_ia_i(\lvert v\rangle,A_i\lvert w\rangle)=\sum_ia_i(A_i^{\dagger}\lvert v\rangle,\lvert w\rangle)=\left(\sum_ia_i^*A_i^{\dagger}\lvert v\rangle,\lvert w\rangle\right).
    \end{align}
    Due to the arbitrariness of $\lvert v\rangle$ and $\lvert w\rangle$,
    \begin{align}
        \left(\sum_ia_iA_i\right)^{\dagger}=\sum_ia_i^*A_i^{\dagger}.
    \end{align}
\end{pf}

\begin{exe}
    Show that $(A^{\dagger})^{\dagger}=A$.
\end{exe}
\begin{pf}
    For two arbitrary vectors $\lvert v\rangle$ and $\lvert w\rangle$,
    \begin{align}
        ((A^{\dagger})^{\dagger}\lvert v\rangle,\lvert w\rangle)=(\lvert v\rangle,A^{\dagger}\lvert w\rangle)=(A^{\dagger}\lvert w\rangle,\lvert v\rangle)^*=(\lvert w\rangle,A\lvert v\rangle)^*=[(A\lvert v\rangle,\lvert w\rangle)^*]^*=(A\lvert v\rangle,\lvert w\rangle).
    \end{align}
    Due to the arbitrariness of $\lvert v\rangle$ and $\lvert w\rangle$,
    \begin{align}
        (A^{\dagger})^{\dagger}=A.
    \end{align}
\end{pf}

\begin{exe}
    Show that any projector $P$ satisfies the equation $P^2=P$.
\end{exe}
\begin{pf}
    For any orthonormal basis $\lvert 1\rangle,\cdots,\lvert k\rangle$ for $W$,
    \begin{align}
        P=\sum_{i=1}^k\lvert i\rangle\langle i\rvert,
    \end{align}
    and then
    \begin{align}
        P^2=\sum_{i=1}^k\lvert i\rangle\langle i\rvert\sum_{j=1}^k\lvert j\rangle\langle j\rvert=\sum_{i=1}^k\sum_{j=1}^k\lvert i\rangle\langle i\vert j\rangle\langle j\rvert=\sum_{i=1}^k\sum_{j=1}^k\lvert i\rangle\delta_{ij}\langle j\rvert=\sum_{i=1}^k\lvert i\rangle\langle i\rvert=P.
    \end{align}
\end{pf}

\begin{exe}
    Show that a normal matrix is Hermitian if and only if it has real eigenvalues.
\end{exe}
\begin{pf}
    \emph{Sufficiency}: If normal matrix $A$ has real eigenvalues $\lambda_1,\cdots,\lambda_n$ with corresponding eigenvectors $\lvert 1\rangle,\cdots,\lvert n\rangle$. It can be written as
    \begin{align}
        A=\sum_{i=1}^n\lambda_i\lvert i\rangle\langle i\rvert.
    \end{align}
    Since $\lambda_1,\cdots,\lambda_n$ are real,
    \begin{align}
        A^{\dagger}=\sum_{i=1}^n\lambda_i^{\dagger}\lvert i\rangle\langle i\rvert=\sum_{i=1}^n\lambda_i\lvert i\rangle\langle i\rvert=A.
    \end{align}
    Therefore, $A$ is Hermitian.

    \emph{Necessity}: Suppose normal and Hermitian matrix $A$ has eigenvalues $\lambda_1,\cdots,\lambda_n$ with corresponding eigenvectors $\lvert 1\rangle,\cdots,\lvert n\rangle$. For any $i=1,\cdots,n$,
    \begin{align}
        \notag\lambda_i=\lambda_i(\lvert i\rangle,\lvert i\rangle)=(\lvert i\rangle,\lambda_i\lvert i\rangle)=&(\lvert i\rangle,A\lvert i\rangle)\\
        =&(A^{\dagger}\lvert i\rangle,\lvert i\rangle)=(\lvert i\rangle,A^{\dagger}\lvert i\rangle)^*=((A^{\dagger})^{\dagger}\lvert i\rangle,\lvert i\rangle)=(A\lvert i\rangle,\lvert i\rangle)=(\lambda_i\lvert i\rangle,\lvert i\rangle)=\lambda_i^*(\lvert i\rangle,\lvert i\rangle)=\lambda_i^*.
    \end{align}
    Therefore, all the eigenvalues $\lambda_1,\cdots,\lambda_n$ are real.
\end{pf}

\begin{exe}
    Show that all eigenvalues of a unitary matrix has modulus $1$, that is can be written in the form $e^{i\theta}$ for some real $\theta$.
\end{exe}
\begin{pf}
    Suppose unitary matrix $A$ has eigenvalues $\lambda_1,\cdots,\lambda_n$ with corresponding eigenvectors $\lvert 1\rangle,\cdots,\lvert n\rangle$. The eigenequations of $A$ are
    \begin{align}
        A\lvert i\rangle=\lambda_i\lvert i\rangle,\quad\forall i=1,\cdots,n.
    \end{align}
    Taking Hermitian conjugate of the above equations,
    \begin{align}
        \langle i\rvert A^{\dagger}=(A\lvert i\rangle)^{\dagger}=\lambda_i^*\langle i\rvert,\quad\forall i=1,\cdots,n.
    \end{align}
    Since $A$ is unitary, for any $i=1,\cdots,n$,
    \begin{gather}
        1=\langle i\vert i\rangle=\langle i\rvert I\lvert i\rangle=\langle i\rvert A^{\dagger}A\lvert i\rangle=\abs{\lambda_i}^2\langle i\vert i\rangle=\abs{\lambda_i}^2,\\
        \Longrightarrow\abs{\lambda_i}=1.
    \end{gather}
    Therefore, all eigenvalues of a unitary matrix has modulus $1$.
\end{pf}

\begin{exe}[Pauli matrices: Hermitian and unitary]
    Show that the Pauli matrices are Hermitian and unitary.
\end{exe}
\begin{pf}
    Since
    \begin{align}
        \sigma_0^{\dagger}=I^{\dagger}=I=\sigma_0,
    \end{align}
    and
    \begin{align}
        \sigma_0^{\dagger}\sigma_0=I^{\dagger}I=II=I,
    \end{align}
    $\sigma_0$ is Hermitian and unitary.
    Since
    \begin{align}
        \sigma_1^{\dagger}=\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]=\sigma_1,
    \end{align}
    and
    \begin{align}
        \sigma_1^{\dagger}\sigma_1=\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]=\left[\begin{matrix}
            1&0\\
            0&1
        \end{matrix}\right]=I,
    \end{align}
    $\sigma_1$ is Hermitian and unitary.
    Since
    \begin{align}
        \sigma_2^{\dagger}=\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]=\sigma_2,
    \end{align}
    and
    \begin{align}
        \sigma_2^{\dagger}\sigma_2=\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]=\left[\begin{matrix}
            1&0\\
            0&1
        \end{matrix}\right]=I,
    \end{align}
    $\sigma_2$ is Hermitian and unitary.
    Since
    \begin{align}
        \sigma_3^{\dagger}=\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]=\sigma_3,
    \end{align}
    and
    \begin{align}
        \sigma_3^{\dagger}\sigma_3=\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]=\left[\begin{matrix}
            1&0\\
            0&1
        \end{matrix}\right]=I,
    \end{align}
    $\sigma_3$ is Hermitian and unitary.
\end{pf}

\begin{exe}[Basis changes]
    Suppose $A'$ and $A''$ are matrix representations of an operator $A$ on a vector space $V$ with respect to two different orthonormal bases, $\lvert v_i\rangle$ and $\lvert w_i\rangle$. Then the elements of $A'$ and $A''$ are $A_{ij}'=\langle v_i\rvert A\lvert v_j\rangle$ and $A_{ij}''=\langle w_i\rvert A\lvert w_j\rangle$. characterize the relationship between $A'$ and $A''$.
\end{exe}
\begin{sol}
    Define $U=\sum_i\lvert w_i\rangle\langle v_i\rvert$ so that $\lvert w_i\rangle=U\lvert v_i\rangle$ and $\langle w_i\rvert=\langle v_i\rvert U^{\dagger}$ $\forall i$. Since $\lvert v_i\rangle$ and $\lvert w_i\rangle$ are two bases for $V$ and
    \begin{align}
        A_{ij}''=\langle w_i\rvert A''\lvert w_j\rangle=\langle v_i\rvert U^{\dagger}A'U\lvert v_j\rangle,
    \end{align}
    the relationship between $A'$ and $A''$ is
    \begin{align}
        A''=U^{\dagger}A'U.
    \end{align}
\end{sol}

\begin{exe}
    Repeat the proof of the spectral decomposition in Box 2.2 for the case when $M$ is Hermitian, simplifying the proof wherever possible.
\end{exe}
\begin{pf}
    \emph{Forward}: Suppose vector space $V$ is $d$-dimensional.
    The case $d=1$ is trivial.
    For the case $d\geq 2$, let $\lambda$ be an eigenvalue of $M$, $P$ the projector onto the $\lambda$ eigenspace, and $Q$ the projector onto the orthogonal component. The $M=IMI=(P+Q)M(P+Q)=PMP+PMQ+QMP+QMQ=PMP+QMQ$, where $PMP=\lambda P$, i.e., $PMP$ is diagonal, and $QMQQM^{\dagger}Q=QM^{\dagger}QQMQ$, i.e., $QMQ$ is normal. By induction, $QMQ$ is diagonal with respect to some orthonormal basis for the subspace $Q$. It follows that $M=PMP+QMQ$ is diagonal with respect to some orthonormal basis for the total vector space.

    \emph{Converse} holds only if all the eigenvalues of $M$ are real. Suppose $M$ is diagonalizable with respect to an orthonormal basis $\lvert i\rangle$ for $V$, i.e.,
    \begin{align}
        M=\sum_i\lambda_i\lvert i\rangle\langle i\rvert,
    \end{align}
    where $\lambda_i$ are the eigenvalues of $M$.
    Since $M$ is Hermitian, all $\lambda_i$ are real,
    \begin{align}
        M^{\dagger}=\left(\sum_i\lambda_i\lvert i\rangle\langle i\rvert\right)^{\dagger}=\sum_i\lambda_i^*\lvert i\rangle\langle i\rvert=\sum_i\lambda_i\lvert i\rangle\langle i\rvert,
    \end{align}
    so $M$ is Hermitian.
\end{pf}

\begin{exe}
    Prove that two eigenvectors of a Hermitian operators with different eigenvalues are necessarily orthogonal.
\end{exe}
\begin{pf}
    Suppose Hermitian operator $A$ has two eigenvectors $\lvert v_1\rangle$ and $\lvert v_2\rangle$ corresponding to two different eigenvalues $\lambda_1$ and $\lambda_2$, i.e.,
    \begin{align}
        A\lvert v_1\rangle=\lambda_1\lvert v_1\rangle,\\
        A\lvert v_2\rangle=\lambda_2\lvert v_2\rangle.
    \end{align}
    Then,
    \begin{align}
        \langle v_1\rvert A\lvert v_2\rangle=\lambda_1\langle v_1\vert v_2\rangle=\lambda_2\langle v_1\vert v_2\rangle.
    \end{align}
    Since $\lambda_1\neq\lambda_2$, the above equation holds only if
    \begin{align}
        \langle v_1\vert v_2\rangle=0,
    \end{align}
    i.e., $\lvert v_1\rangle$ and $\lvert v_2\rangle$ are orthogonal.
\end{pf}

\begin{exe}
    Show that the eigenvalues of a projector $P$ are all either $0$ or $1$.
\end{exe}
\begin{pf}
    Suppose $\lvert 1\rangle,\cdots,\lvert k\rangle$ is an orthonormal basis for the subspace $P$ and $\lvert 1\rangle,\cdots,\lvert n\rangle$ is an orthonormal basis for the total vector space, where $k\leq n$. It is easy to see that $\lvert 1\rangle,\cdots,\lvert n\rangle$ are eigenvectors of projector $P=\sum_{i=1}^k\lvert i\rangle\langle i\rvert$:
    \begin{align}
        P\lvert j\rangle=\sum_{i=1}^k\lvert i\rangle\langle i\vert j\rangle=\sum_{i=1}^k\lvert i\rangle\delta_{ij}=\left\{\begin{array}{ll}
            \lvert j\rangle,&\text{if }j=1,\cdots,k;\\
            0,&\text{if }j=k+1,\cdots,n.
        \end{array}\right.
    \end{align}
    Therefore, the eigenvalues of a projector $P$ are all either $0$ or $1$.
\end{pf}

\begin{exe}[Hermiticity of positive operators]
    Show that a positive operator is necessarily Hermitian. (\emph{Hint}: Show that an arbitrary operator $A$ can be written $A=B+iC$ where $B$ and $C$ are Hermitian.)
\end{exe}
\begin{pf}
    An arbitrary positive operator $A$ can be written $A=B+iC$ where all the entries of $B=\frac{A+A^{\dagger}}{2}$ and $C=\frac{A-A^{\dagger}}{2i}$ are Hermitian. Since $A$ is a positive operator, for any vector $\lvert v\rangle$,
    \begin{align}
        \langle v\rvert A\lvert v\rangle=\langle v\rvert(B+iC)\lvert v\rangle=\langle v\rvert B\lvert v\rangle+i\langle v\rvert C\lvert v\rangle\geq 0,
    \end{align}
    and is real. In this way, $\langle v\rvert C\lvert v\rangle$ can only be either purely imaginary or zero. Since $C$ is Hermitian, it is diagonalizable and has a diagonal representation
    \begin{align}
        C=\sum_i\lambda_i\lvert i\rangle\langle i\rvert,
    \end{align}
    where $\lambda_i$ are the eigenvalues of $C$ and real, and $\lvert i\rangle$ is an orthonormal basis. Any vector $\lvert v\rangle$ can be written as a linear combination of $\lvert i\rangle$,
    \begin{align}
        \lvert v\rangle=\sum_ia_i\lvert i\rangle.
    \end{align}
    Hence,
    \begin{align}
        \langle v\rvert C\lvert v\rangle=\sum_ia_i^*\langle i\rvert \sum_j\lambda_j\lvert j\rangle\langle j\rvert\sum_ka_k\lvert k\rangle=\sum_{i,j,k}\lambda_ja_i^*a_k\langle i\vert j\rangle\langle j\vert k\rangle=\sum_{ijk}\lambda_ja_i^*a_k\delta_{ij}\delta_{jk}=\sum_i\lambda_i\abs{a_i}^2
    \end{align}
    can not be purely imaginary and only be zero. Therefore, $A=B$ and is Hermitian.
\end{pf}

\begin{exe}
    Show that for any operator $A$, $A^{\dagger}A$ is positive.
\end{exe}
\begin{pf}
    For any vector $\lvert v\rangle$,
    \begin{align}
        (\lvert v\rangle,A^{\dagger}A\lvert v\rangle)=((A^{\dagger}A)^{\dagger}\lvert v\rangle,\lvert v\rangle)=(A^{\dagger}A\lvert v\rangle,\lvert v\rangle)=(\lvert v\rangle,A^{\dagger}A\lvert v\rangle)^*,
    \end{align}
    so $(\lvert v\rangle,A^{\dagger}A\lvert v\rangle)$ is real. Besides,
    \begin{align}
        (\lvert v\rangle,A^{\dagger}A\lvert v\rangle)=((A^{\dagger})^{\dagger}\lvert v\rangle,A\lvert v\rangle)=(A\lvert v\rangle,A\lvert v\rangle)\geq 0.
    \end{align}
    Therefore, for any operator $A$, $A^{\dagger}A$ is positive.
\end{pf}

\begin{exe}
    Let $\lvert\psi\rangle=(\lvert 0\rangle+\lvert 1\rangle)/\sqrt{2}$. Write out $\lvert\psi\rangle^{\otimes 2}$ and $\lvert\psi\rangle^{\otimes 3}$ explicitly, both in terms of tensor products like $\lvert 0\rangle\lvert 1\rangle$, and using the Kronecker product.
\end{exe}
\begin{sol}
    \begin{align}
        \lvert\psi\rangle^{\otimes 2}=&\frac{1}{\sqrt{2}}(\lvert 0\rangle+\lvert 1\rangle)\otimes\frac{1}{\sqrt{2}}(\lvert 0\rangle+\lvert 1\rangle)=\frac{1}{2}(\lvert 0\rangle\lvert 0\rangle+\lvert 0\rangle\lvert 1\rangle+\lvert 1\rangle\lvert 0\rangle+\lvert 1\rangle\lvert 1\rangle)\\
        =&\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            1
        \end{matrix}\right]\otimes\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            1
        \end{matrix}\right]=\frac{1}{2}\left[\begin{matrix}
            1\\
            1\\
            1\\
            1
        \end{matrix}\right],\\
        \notag\lvert\psi\rangle^{\otimes 3}=&\frac{1}{\sqrt{2}}(\lvert 0\rangle+\lvert 1\rangle)\otimes\frac{1}{\sqrt{2}}(\lvert 0\rangle+\lvert 1\rangle)\otimes\frac{1}{\sqrt{2}}(\lvert 0\rangle+\lvert 1\rangle)=\frac{1}{2\sqrt{2}}(\lvert 0\rangle\lvert 0\rangle\lvert 0\rangle+\lvert 0\rangle\lvert 0\rangle\lvert 1\rangle+\lvert 0\rangle\lvert 1\rangle\lvert 0\rangle+\lvert 0\rangle\lvert 1\rangle\lvert 1\rangle\\
        &+\lvert 1\rangle\lvert 0\rangle\lvert 0\rangle+\lvert 1\rangle\lvert 0\rangle\lvert 1\rangle+\lvert 1\rangle\lvert 1\rangle\lvert 0\rangle+\lvert 1\rangle\lvert 1\rangle\lvert 1\rangle)\\
        =&\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            1
        \end{matrix}\right]\otimes\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            1
        \end{matrix}\right]\otimes\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            1
        \end{matrix}\right]=\frac{1}{2\sqrt{2}}\left[\begin{matrix}
            1\\
            1\\
            1\\
            1\\
            1\\
            1\\
            1\\
            1
        \end{matrix}\right].
    \end{align}
\end{sol}

\begin{exe}
    Calculate the matrix representation of the tensor products of the Pauli operators (a) $X$ and $Z$; (b) $I$ and $X$; (c) $X$ and $I$. Is the tensor product commutative?
\end{exe}
\begin{sol}
    \begin{itemize}
        \item[(a)] The matrix representation of the tensor product of $X$ and $Z$:
        \begin{align}
            X\otimes Z=\left[\begin{matrix}
                0&1\\
                1&0
            \end{matrix}\right]\otimes\left[\begin{matrix}
                1&0\\
                0&-1
            \end{matrix}\right]=\left[\begin{matrix}
                0&0&1&0\\
                0&0&0&-1\\
                1&0&0&0\\
                0&-1&0&0
            \end{matrix}\right].
        \end{align}
        The matrix representation of the tensor product of $Z$ and $X$:
        \begin{align}
            Z\otimes X=\left[\begin{matrix}
                1&0\\
                0&-1
            \end{matrix}\right]\otimes\left[\begin{matrix}
                0&1\\
                1&0
            \end{matrix}\right]=\left[\begin{matrix}
                0&1&0&0\\
                1&0&0&0\\
                0&0&0&-1\\
                0&0&-1&0
            \end{matrix}\right].
        \end{align}
        Therefore, the tensor product $X\otimes Z$ and $Z\otimes X$ are not commutative.
        \item[(b)] The matrix representation of the tensor product of $I$ and $X$:
        \begin{align}
            I\otimes X=\left[\begin{matrix}
                1&0\\
                0&1
            \end{matrix}\right]\otimes\left[\begin{matrix}
                0&1\\
                1&0
            \end{matrix}\right]=\left[\begin{matrix}
                0&1&0&0\\
                1&0&0&0\\
                0&0&0&1\\
                0&0&1&0
            \end{matrix}\right].
        \end{align}
        \item[(c)] The matrix representation of the tensor product of $X$ and $I$:
        \begin{align}
            X\otimes I=\left[\begin{matrix}
                0&1\\
                1&0
            \end{matrix}\right]\otimes\left[\begin{matrix}
                1&0\\
                0&1
            \end{matrix}\right]=\left[\begin{matrix}
                0&0&1&0\\
                0&0&0&1\\
                1&0&0&0\\
                0&1&0&0
            \end{matrix}\right].
        \end{align}
        Therefore, the tensor product $I\otimes X$ and $X\otimes I$ are not commutative.
    \end{itemize}
\end{sol}

\begin{exe}
    Show that the transpose, complex conjugate, and joint operation distribute over the tensor product,
    \begin{align}
        (A\otimes B)^*=A^*\otimes B^*;\quad(A\otimes B)^T=A^T\otimes B^T;\quad(A\otimes B)^{\dagger}=A^{\dagger}\otimes B^{\dagger}.
    \end{align}
\end{exe}
\begin{pf}
    Suppose $A$ is a $m$ by $n$ matrix, and $B$ is a $p$ by $q$ matrix.
    \begin{itemize}
        \item[(a)] 
        \begin{align}
            (A\otimes B)^*=\left[\begin{matrix}
                A_{11}B&A_{12}B&\cdots&A_{1n}B\\
                A_{21}B&A_{22}B&\cdots&A_{2n}B\\
                \vdots&\vdots&\vdots&\vdots\\
                A_{m1}B&A_{m2}B&\cdots&A_{mn}B
            \end{matrix}\right]^*=\left[\begin{matrix}
                A_{11}^*B^*&A_{12}^*B^*&\cdots&A_{1n}^*B^*\\
                A_{21}^*B^*&A_{22}^*B^*&\cdots&A_{2n}^*B^*\\
                \vdots&\vdots&\vdots&\vdots\\
                A_{m1}^*B^*&A_{m2}^*B^*&\cdots&A_{mn}^*B^*
            \end{matrix}\right]=A^*\otimes B^*.
        \end{align}
        \item[(b)] 
        \begin{align}
            (A\otimes B)^T=\left[\begin{matrix}
                A_{11}B&A_{12}B&\cdots&A_{1n}B\\
                A_{21}B&A_{22}B&\cdots&A_{2n}B\\
                \vdots&\vdots&\vdots&\vdots\\
                A_{m1}B&A_{m2}B&\cdots&A_{mn}B
            \end{matrix}\right]^T=\left[\begin{matrix}
                A_{11}B^T&A_{21}B^T&\cdots&A_{m1}B^T\\
                A_{12}B^T&A_{22}B^T&\cdots&A_{m2}B^T\\
                \vdots&\vdots&\vdots&\vdots\\
                A_{1n}B^T&A_{2n}B^T&\cdots&A_{mn}B^T
            \end{matrix}\right]=A^T\otimes B^T.
        \end{align}
        \item[(c)] 
        \begin{align}
            (A\otimes B)^{\dagger}=[(A\otimes B)^*]^T=(A^*\otimes B^*)^T=(A^*)^T\otimes(B^*)^T=A^{\dagger}\otimes B^{\dagger}.
        \end{align}
    \end{itemize}
\end{pf}

\begin{exe}
    Show that the tensor product of two unitary operators is unitary.
\end{exe}
\begin{pf}
    Suppose $A$ and $B$ are two unitary operators,
    \begin{align}
        A^{\dagger}A=&I,\\
        B^{\dagger}B=&I.
    \end{align}
    Using the conclusion obtain in the previous exercise,
    \begin{align}
        (A\otimes B)^{\dagger}(A\otimes B)=(A^{\dagger}\otimes B^{\dagger})(A\otimes B)=(A^{\dagger}A)\otimes(B^{\dagger}B)=I\otimes I=I,
    \end{align}
    so the tensor product of two unitary operators is unitary.
\end{pf}

\begin{exe}
    Show that the tensor product of two Hermitian operators is Hermitian.
\end{exe}
\begin{pf}
    Suppose $A$ and $B$ are two Hermitian operators,
    \begin{align}
        A^{\dagger}=&A,\\
        B^{\dagger}=&B.
    \end{align}
    Using the conclusion obtained in Exercise 2.28,
    \begin{align}
        (A\otimes B)^{\dagger}=A^{\dagger}\otimes B^{\dagger}=A\otimes B,
    \end{align}
    so the tensor product of two Hermitian operators is Hermitian.
\end{pf}

\begin{exe}
    Show that the tensor product of two positive operators is positive.
\end{exe}
\begin{pf}
    Suppose $A$ and $B$ are positive operators on vector spaces $V$ and $W$ respectively. For any vector $\lvert v\rangle\in V$ and $\lvert w\rangle\in W$,
    \begin{align}
        (\lvert v\rangle,A\lvert v\rangle)\geq&0,\\
        (\lvert w\rangle,B\lvert w\rangle)\geq&0,
    \end{align}
    so $A\otimes B$ is a positive operator,
    \begin{align}
        (\lvert v\rangle\otimes\lvert w\rangle,(A\otimes B)(\lvert v\rangle\otimes\lvert w\rangle))=(\lvert v\rangle\otimes\lvert w\rangle,(A\lvert v\rangle)\otimes(B\lvert w\rangle))=(\lvert v\rangle,A\lvert v\rangle)(\lvert w\rangle,B\lvert w\rangle)\geq 0.
    \end{align}
    Therefore, the tensor product of two positive operators is positive.
\end{pf}

\begin{exe}
    Show that the tensor product of two projectors is a projector.
\end{exe}
\begin{pf}
    Suppose $\lvert i\rangle_V$ is an orthonormal basis for vector space $V$, and $\lvert j\rangle_W$ is an orthonormal basis for vector space $W$. The projector onto $V$ is
    \begin{align}
        P_V=\sum_i\lvert i\rangle_V\langle i\lvert_V,
    \end{align}
    and the projector onto $W$ is
    \begin{align}
        P_W=\sum_j\lvert j\rangle_W\langle j\lvert_W.
    \end{align}
    Their tensor product is
    \begin{align}
        P_V\otimes P_W=\left(\sum_i\lvert i\rangle_V\langle i\lvert_V\right)\otimes\left(\sum_j\lvert j\rangle_W\langle j\lvert_W\right)=\sum_{i,j}(\lvert i\rangle_V\otimes\lvert j\rangle_W)(\langle i\rvert_V\otimes\langle j\rvert_W).
    \end{align}
    Since both $\lvert i\rangle_V$ and $\lvert j\rangle_W$ are independent, $\lvert i\rangle\otimes\lvert j\rangle$ are independent. Since both $\lvert i\rangle_V$ and $\lvert j\rangle_W$ are orthonormal, $\lvert i\rangle\otimes\lvert j\rangle$ are orthonormal,
    \begin{align}
        (\vert i\rangle_V\otimes\lvert j\rangle_W,\lvert k\rangle_V\otimes\lvert l\rangle_W)=(\lvert i\rangle_V,\lvert k\rangle_V)(\lvert j\rangle_W,\lvert l\rangle_W)=\delta_{ik}\delta_{jl}.
    \end{align}
    Since any vector $\lvert v\rangle\in V$ can be written as a linear combination of $\lvert i\rangle_V$,
    \begin{align}
        \lvert v\rangle=\sum_ia_i\lvert i\rangle_V,
    \end{align}
    and any vector $\lvert w\rangle\in W$ can be written as a linear combination of $\lvert j\rangle_W$,
    \begin{align}
        \lvert w\rangle=\sum_jb_j\lvert j\rangle_W,
    \end{align}
    the tensor product $\lvert v\otimes\lvert w\rangle\in V\otimes W$ can be written as a linear combination of $\lvert i\rangle_V\otimes\lvert j\rangle_W$,
    \begin{align}
        \lvert v\rangle\otimes\lvert w\rangle=\left(\sum_ia_i\lvert i\rangle_V\right)\otimes\left(\sum_jb_j\lvert j\rangle_W\right)=\sum_{i,j}a_ib_j\lvert i\rangle_V\otimes\lvert j\rangle_W.
    \end{align}
    Hence $\lvert i\rangle_V\otimes\lvert j\rangle_W$ is an orthonormal basis for $V\otimes W$ and $P_V\otimes P_W$ is a projector onto $V\otimes W$. Therefore, the tensor product of two projectors is a projector.
\end{pf}

\begin{exe}
    The Hadamard operator on one qubit may be written as
    \begin{align}
        H=\frac{1}{\sqrt{2}}\left[(\lvert 0\rangle+\lvert 1\rangle)\langle 0\rvert+(\lvert 0\rangle-\lvert 1\rangle)\langle 1\rvert\right].
    \end{align}
    Show explicitly that the Hadamard transform on $n$ qubits, $H^{\otimes n}$, may be written as
    \begin{align}
        H^{\otimes n}=\frac{1}{\sqrt{2^n}}\sum_{x,y}(-1)^{x\cdot y}\lvert x\rangle\langle y\rvert.
    \end{align}
    Write out an explicit matrix representation for $H^{\otimes 2}$.
\end{exe}
\begin{sol}
    Hadamard transform on one qubit may be written as
    \begin{align}
        H\lvert 0\rangle=\frac{1}{\sqrt{2}}(\lvert 0\rangle\langle 0\rvert+\lvert 1\rangle\langle 0\rvert+\lvert 0\rangle\langle 1\rvert-\lvert 1\rangle\langle 1\rvert)=\frac{1}{\sqrt{2}}\sum_{x,y=0}^1\lvert x\rangle\langle y\rvert.
    \end{align}
    Thus Hadamard transform on $n$ qubits may be written as
    \begin{align}
        \notag H^{\otimes n}=&\left(\frac{1}{\sqrt{2}}\sum_{x_1,y_1=0}^1\lvert x_1\rangle\langle y_1\rvert\right)\otimes\cdots\otimes\left(\frac{1}{\sqrt{2}}\sum_{x_n,y_n=0}^1\lvert x_n\rangle\langle y_n\rvert\right)=\frac{1}{\sqrt{2^n}}\sum_{\substack{x_1,\cdots,x_n\\y_1,\cdots,y_n}=0}^1\lvert x_1,\cdots,x_n\rangle\langle y_1,\cdots,y_n\rvert\\
        =&\frac{1}{\sqrt{2^n}}\sum_{x,y}(-1)^{x\cdot y}\lvert x\rangle\langle y\rvert.
    \end{align}
    The matrix representation for $H^{\otimes 2}$ is
    \begin{align}
        H^{\otimes 2}=\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1&1\\
            1&-1
        \end{matrix}\right]\otimes\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1&1\\
            1&-1
        \end{matrix}\right]=\frac{1}{2}\left[\begin{matrix}
            1&1&1&1\\
            1&-1&1&-1\\
            1&1&-1&-1\\
            1&-1&-1&1
        \end{matrix}\right].
    \end{align}
\end{sol}

\begin{exe}
    Find the square root and logarithm of the matrix
    \begin{align}
        \left[\begin{matrix}
            4&3\\
            3&4
        \end{matrix}\right].
    \end{align}
\end{exe}
\begin{sol}
    The square root of the matrix is
    \begin{align}
        \left[\begin{matrix}
            2&\sqrt{3}\\
            \sqrt{3}&2
        \end{matrix}\right].
    \end{align}
    The logarithm of the matrix is
    \begin{align}
        \left[\begin{matrix}
            \ln 4&\ln 3\\
            \ln 3&\ln 4
        \end{matrix}\right].
    \end{align}
\end{sol}

\begin{exe}[Exponential of the Pauli matrices]
    Let $\vec{v}$ be any real, three dimensional unit vector and $\theta$ a real number. Prove that
    \begin{align}
        \label{E3.35}
        \exp(i\theta\vec{v}\cdot\vec{\sigma})=\cos(\theta)I+i\sin(\theta)\vec{v}\cdot\vec{\sigma},
    \end{align}
    where $\vec{v}\cdot\vec{\sigma}=\sum_{i=1}^3v_i\sigma_i$. This exercise is generalized in Problem 2.1 on page 117.
\end{exe}
\begin{pf}
    The left side of equation \eqref{E3.35} is
    \begin{align}
        \exp(i\theta\vec{v}\cdot\vec{\sigma})=\sum_{n=0}^{\infty}\frac{1}{n!}(i\theta\vec{v}\cdot\vec{\sigma})^k=\sum_{n=1}^{\infty}\frac{(-1)^n}{(2n)!}(\theta\vec{v}\cdot\vec{\sigma})^{2n}+\sum_{n=0}^{\infty}\frac{i(-1)^n}{(2n+1)!}(\theta\vec{v}\cdot\vec{\sigma})^{2n+1}.
    \end{align}
    Note that
    \begin{align}
        (\vec{v}\cdot\vec{\sigma})^2=\left(\sum_{i=1}^3v_i\sigma_i\right)^2=\sum_{i,j=1}^3v_iv_j\sigma_i\sigma_j.
    \end{align}
    Due to the anti-commutation relation between the Pauli matrices,
    \begin{align}
        \{\sigma_i,\sigma_j\}=\sigma_i\sigma_j+\sigma_j\sigma_i=2\delta_{ij}I=\left\{\begin{array}{ll}
            2I,&i=j;\\
            0,&i\neq j,
        \end{array}\right.
    \end{align}
    we have
    \begin{align}
        (\vec{v}\cdot\vec{\sigma})^2=\sum_iv_i^2I=I.
    \end{align}
    Hence the left side of equation \eqref{E3.35} can be written as
    \begin{align}
        \exp(i\theta\vec{v}\cdot\vec{\sigma})=\sum_{n=1}^{\infty}\frac{(-1)^n}{(2n)!}\theta^{2n}+\sum_{n=0}^{\infty}\frac{i(-1)^n}{(2n+1)}\theta^{2n+1}\vec{\sigma}=\cos(\theta)I+i\sin(\theta)\vec{\sigma},
    \end{align}
    which equals the right side of equation \eqref{E3.35}. Therefore, equation \eqref{E3.35} holds.
\end{pf}

\begin{exe}
    Show that the Pauli matrices except for $I$ have trace zero.
\end{exe}
\begin{pf}
    The trace of $I$ is
    \begin{align}
        \tr(I)=1+1=2.
    \end{align}
    The trace of $X$ is
    \begin{align}
        \tr(X)=0+0=0.
    \end{align}
    The trace of $Y$ is
    \begin{align}
        \tr(Y)=0+0=0.
    \end{align}
    The trace of $Z$ is
    \begin{align}
        \tr(Z)=1+(-1)=0.
    \end{align}
    Therefore, the Pauli matrices except for $I$ have trace zero.
\end{pf}

\begin{exe}[Cyclic property of the trace]
    If $A$ and $B$ are two linear operators show that
    \begin{align}
        \tr(AB)=\tr(BA).
    \end{align}
\end{exe}
\begin{pf}
    \begin{align}
        \tr(AB)=\sum_i(AB)_{ii}=\sum_i\left(\sum_jA_{ij}B_{ji}\right)=\sum_j\left(\sum_iB_{ji}A_{ij}\right)=\sum_j(BA)_{jj}=\tr(BA).
    \end{align}
\end{pf}

\begin{exe}[Linearity of the trace]
    If $A$ and $B$ are two linear operators. show that
    \begin{align}
        \tr(A+B)=\tr(A)+\tr(B)
    \end{align}
    and if $z$ is an arbitrary complex number show that
    \begin{align}
        \tr(zA)=z\tr(A).
    \end{align}
\end{exe}
\begin{pf}
    \begin{align}
        \tr(A+B)=&\sum_i(A+B)_{ii}=\sum_iA_{ii}+\sum_iB_{ii}=\tr(A)+\tr(B).\\
        \tr(zA)=&\sum_i(zA)_{ii}=z\sum_iA_{ii}=z\tr(A).
    \end{align}
\end{pf}

\begin{exe}[The Hilbert-Schmidt inner product on operators]
    The set $L_V$ of linear operators on a Hilbert space $V$ is obviously a vector space -- the sum of two linear operators is a Linear operator, $zA$ is a linear operator if $A$ is a linear operator and $z$ is a complex number, and there is a zero element $0$. An important additional result is that the vector space $L_V$ can be given a natural inner product structure, turning it into a Hilbert space.
    \begin{itemize}
        \item[(1)] Show that the function $(\cdot,\cdot)$ on $L_V\times L_V$ defined by
        \begin{align}
            (A,B)\equiv\tr(A^{\dagger}B)
        \end{align}
        is an inner product function. This inner product is known as the \emph{Hilbert-Schmidt} or \emph{trace} inner product.
        \item[(2)] If $V$ has $d$ dimensions show that $L_V$ has dimension $d^2$.
        \item[(3)] Find an orthonormal basis of Hermitian matrices for the Hilbert space $L_V$.
    \end{itemize}
\end{exe}
\begin{sol}
    \begin{itemize}
        \item[(1)] The function $(\cdot,\cdot)$ on $L_V\times L_V$ satisfies the requirements that:
        \begin{itemize}
            \item[(a)] $(\cdot,\cdot)$ is linear in the second argument,
            \begin{align}
                \left(A,\sum_j\lambda_jB^{(j)}\right)=\tr\left(A^{\dagger}\sum_j\lambda_jB^{(j)}\right)=\tr\left(\sum_j\lambda_jA^{\dagger}B^{(j)}\right)=\sum_j\lambda_j\tr(A^{\dagger}B^{(j)})=\sum_j\lambda(A,B^{\dagger}).
            \end{align}
            \item[(b)] 
            \begin{align}
                \notag(A,B)=&\tr(A^{\dagger}B)=\sum_i(A^{\dagger}B)_{ii}=\sum_i\left[\sum_j(A^{\dagger})_{ij}B_{ji}\right]=\sum_i\left(\sum_jA_{ji}^*B_{ji}\right)=\left[\sum_i\left(\sum_jA_{ji}B_{ji}^*\right)\right]^*\\
                =&\left\{\sum_i\left[\sum_j(B^{\dagger})_{ij}A_{ji}\right]\right\}^*=\left[\sum_i(B^{\dagger}A)_{ii}\right]^*=[\tr(B^{\dagger}A)]^*=(B,A)^*.
            \end{align}
            \item[(c)] 
            \begin{align}
                (A,A)=\tr(A^{\dagger}A)=\sum_i(A^{\dagger}A)_{ii}=\sum_i\left[\sum_j(A^{\dagger})_{ij}A_{ji}\right]=\sum_i\left(\sum_jA_{ji}^*A_{ji}\right)=\sum_{i,j}\abs{A_{ji}}^2\geq 0,
            \end{align}
            with equality if and only if $A=0$.
        \end{itemize}
        Therefore, the function $(\cdot,\cdot)$ defined on $L_V\times L_V$ is an inner product function.
        \item[(2)] Suppose $\lvert 1\rangle,\cdot,\lvert d\rangle$ form an orthonormal basis for $V$. Since
        \begin{itemize}
            \item[(a)] any operator $A$ in $V$ can be written as
            \begin{align}
                A=\sum_{i,j}A_{ij}\lvert i\rangle\langle j\rvert,
            \end{align}
            and
            \item[(b)] $\lvert i\rangle\langle j\rvert$ are orthonormal,
            \begin{align}
                (\lvert i\rangle\langle j\rvert,\lvert m\rangle\langle n\rvert)=\tr[(\lvert i\rangle\langle j\rvert)^{\dagger}\lvert m\rangle\langle n\rvert]=\tr(\lvert j\rangle\langle i\vert m\rangle\langle n\rvert)=\sum_k\langle k\rvert j\rangle\langle i\vert m\rangle\langle n\vert k\rangle=\sum_k\delta_{kj}\delta_{im}\delta_{nk}=\delta_{im}\delta_{jn},
            \end{align}
            and thus linearly independent, $\lvert i\rangle\langle j\rvert$ is an orthonormal basis for $L_V$. Since this basis has $d^2$ elements, $L_V$ has dimension $d^2$.
        \end{itemize}
        \item[(3)] $\left\{\lvert i\rangle\langle i\rvert,\frac{\lvert i\rangle\langle j\rvert+\lvert j\rangle\langle i\rvert}{\sqrt{2}},\frac{\lvert i\rangle\langle j\rvert-\lvert j\rangle\langle i\rvert}{i\sqrt{2}};\forall 1\leq i<j\leq d\right\}$ is an orthonormal basis of Hermitian matrices for the Hilbert space $L_V$. Here is the reason:
        \begin{itemize}
            \item[(a)] For any Hermitian matrices $A$, since
            \begin{align}
                A^{\dagger}=A\Longrightarrow A_{ji}^*=A_{ji},\quad\forall i,j=1,\cdots,d,
            \end{align}
            and $A_{ii}$ is real $\forall i$, $A$ can be written as
            \begin{align}
                \notag A=&\sum_{i,j}A_{ij}\lvert i\rangle\langle j\rvert=\sum_iA_{ii}\lvert i\rangle\langle i\rvert+\sum_{i=2}^d\sum_{j=1}^{i-1}A_{ij}\lvert i\rangle\langle j\rvert+\sum_{j=2}^d\sum_i^{j-1}A_{ij}\lvert i\rangle\langle j\rvert\\
                \notag=&\sum_iA_{ii}\lvert i\rangle\langle i\rvert+\sum_{i=2}^d\sum_{j=1}^{i-1}A_{ji}^*\lvert i\rangle\langle j\rvert+\sum_{j=2}^d\sum_i^{j-1}A_{ij}\lvert i\rangle\langle j\rvert\\
                \notag=&\sum_iA_{ii}\lvert i\rangle\langle i\rvert+\sum_{j=2}^d\sum_{i=1}^{j-1}A_{ij}^*\lvert j\rangle\langle i\rvert+\sum_{j=2}^d\sum_i^{j-1}A_{ij}\lvert i\rangle\langle j\rvert\\
                \notag=&\sum_iA_{ii}\lvert i\rangle\langle i\rvert+\sum_{j=2}^d\sum_{i=1}^{j-1}\frac{A_{ij}+A_{ij}^*}{\sqrt{2}}\frac{\lvert i\rangle\langle j\rvert+\lvert j\rangle\langle i\rvert}{\sqrt{2}}+\sum_{j=2}^d\sum_i^{j-1}\frac{A_{ij}-A_{ij}^*}{-i\sqrt{2}}\frac{\lvert i\rangle\langle j\rvert-\lvert j\rangle\langle i\rvert}{i\sqrt{2}}.
            \end{align}
            \item[(b)] Elements in $\left\{\lvert i\rangle\langle i\rvert,\frac{\lvert i\rangle\langle j\rvert+\lvert j\rangle\langle i\rvert}{\sqrt{2}},\frac{\lvert i\rangle\langle j\rvert-\lvert j\rangle\langle i\rvert}{i\sqrt{2}};\forall 1\leq i<j\leq d\right\}$ are orthonormal and thus linearly independent,
            \begin{align}
                \notag\left(\lvert i\rangle\langle i\rvert,\frac{\lvert m\rangle\langle n\rvert+\lvert n\rangle\langle m\rvert}{\sqrt{2}}\right)=&\tr\left[(\lvert i\rangle\langle i\rvert)^{\dagger}\left(\frac{\lvert m\rangle\langle n\rvert+\lvert n\rangle\langle m\rvert}{\sqrt{2}}\right)\right]=\frac{1}{\sqrt{2}}\tr\left[\lvert i\rangle\langle i\rvert(\lvert m\rangle\langle n\rvert+\lvert n\rangle\langle m\rvert)\right]\\
                \notag=&\frac{1}{\sqrt{2}}\tr(\lvert i\rangle\langle i\vert m\rangle\langle n\rvert+\lvert i\rangle\langle i\vert n\rangle\langle m\rvert)\\
                \notag=&\frac{1}{\sqrt{2}}\sum_k(\langle k\vert i\rangle\langle i\vert m\rangle\langle n\vert k\rangle+\langle k\vert i\rangle\langle i\vert n\rangle\langle m\vert k\rangle)\\
                \notag=&\frac{1}{\sqrt{2}}\sum_k(\delta_{ki}\delta_{im}\delta_{nk}+\delta_{ki}\delta_{in}\delta_{mk})\\
                =&\sqrt{2}\delta_{im}\delta_{in}=0,\quad\forall i=1,\cdots,d,1\leq m<n\leq d,\\
                \notag\left(\lvert i\rangle\langle i\rvert,\frac{\lvert m\rangle\langle n\rvert-\lvert n\rangle\langle m\rvert}{i\sqrt{2}}\right)=&\tr\left[(\lvert i\rangle\langle i\rvert)^{\dagger}\left(\frac{\lvert m\rangle\langle n\rvert+\lvert n\rangle\langle m\rvert}{i\sqrt{2}}\right)\right]=\frac{1}{i\sqrt{2}}\tr\left[\lvert i\rangle\langle i\rvert(\lvert m\rangle\langle n\rvert+\lvert n\rangle\langle m\rvert)\right]\\
                \notag=&\frac{1}{i\sqrt{2}}\tr(\lvert i\rangle\langle i\vert m\rangle\langle n\rvert-\lvert i\rangle\langle i\vert n\rangle\langle m\rvert)\\
                \notag=&\frac{1}{i\sqrt{2}}\sum_k(\langle k\vert i\rangle\langle i\vert m\rangle\langle n\vert k\rangle-\langle k\vert i\rangle\langle i\vert n\rangle\langle m\vert k\rangle)\\
                \notag=&\frac{1}{i\sqrt{2}}\sum_k(\delta_{ki}\delta_{im}\delta_{nk}-\delta_{ki}\delta_{in}\delta_{mk})\\
                =&0,\quad\forall i=1,\cdots,d,1\leq m<n\leq d,\\
                \notag\left(\frac{\lvert i\rangle\langle j\rvert+\lvert j\rangle\langle i\rvert}{\sqrt{2}},\frac{\lvert m\rangle\langle n\rvert-\lvert n\rangle\langle m\rvert}{i\sqrt{2}}\right)=&\tr\left[\left(\frac{\lvert i\rangle\langle j\rvert+\lvert j\rangle\langle i\rvert}{\sqrt{2}}\right)^{\dagger}\left(\frac{\lvert m\rangle\langle n\rvert-\lvert n\rangle\langle m\rvert}{i\sqrt{2}}\right)\right]\\
                \notag=&\frac{1}{2i}\tr\left[(\lvert j\rangle\langle i\rvert+\lvert i\rangle\langle j\rvert)(\lvert m\rangle\langle n\rvert-\lvert n\rangle\langle m\rvert)\right]\\
                \notag=&\frac{1}{2i}\tr(\lvert j\rangle\langle i\vert m\rangle\langle n\rvert-\lvert j\rangle\langle i\vert n\rangle\langle m\rvert+\lvert i\rangle\langle j\vert m\rangle\langle n\rvert-\lvert i\rangle\langle j\vert n\rangle\langle m\rvert)\\
                \notag=&\frac{1}{2i}\sum_k(\langle k\vert j\rangle\langle i\vert m\rangle\langle n\vert k\rangle-\langle k\vert j\rangle\langle i\vert n\rangle\langle m\vert k\rangle\\
                \notag&+\langle k\vert i\rangle\langle j\vert m\rangle\langle n\vert k\rangle-\langle k\vert i\rangle\langle j\vert n\rangle\langle m\vert k\rangle)\\
                \notag=&\frac{1}{2i}\sum_k(\delta_{kj}\delta_{im}\delta_{nk}-\delta_{kj}\delta_{in}\delta_{mk}+\delta_{ki}\delta_{jm}\delta_{nk}-\delta_{ki}\delta_{jn}\delta_{mk})\\
                =&0,\quad\forall 1\leq i<j\leq d,\cdots,d,1\leq m<n\leq d,\\
                \notag(\lvert i\rangle\langle i\rvert,\lvert j\rangle\langle j\rvert)=&\tr[(\lvert i\rangle\langle i\rvert)^{\dagger}\lvert j\rangle\langle j\rvert]=\tr(\lvert i\rangle\langle i\vert j\rangle\langle j\rvert)=\sum_k\langle k\vert i\rangle\langle i\vert j\rangle\langle j\vert k\rangle=\sum_k\delta_{ki}\delta_{ij}\delta_{jk}\\
                =&\delta_{ij},\quad\forall i,j=1,\cdots,d,\\
                \notag\left(\frac{\lvert i\rangle\langle j\rvert+\lvert i\rangle\langle j\rvert}{\sqrt{2}},\frac{\lvert m\rangle\langle n\rvert+\lvert n\rangle\langle m\rvert}{\sqrt{2}}\right)=&\tr\left[\left(\frac{\lvert i\rangle\langle j\rvert+\lvert j\rangle\langle i\rvert}{\sqrt{2}}\right)^{\dagger}\frac{\lvert m\rangle\langle n\rvert+\lvert n\rangle\langle m\rvert}{\sqrt{2}}\right]\\
                \notag=&\frac{1}{2}\tr[(\lvert j\rangle\langle i\rvert+\lvert i\rangle\langle j\rvert)(\lvert m\rangle\langle m\rvert+\lvert n\rangle\langle m\rvert)]\\
                \notag=&\frac{1}{2}\tr(\lvert j\rangle\langle i\vert m\rangle\langle n\rvert+\lvert j\rangle\langle i\vert n\rangle\langle m\rvert+\lvert i\rangle\langle j\vert m\rangle\langle n\rvert+\lvert i\rangle\langle j\vert n\rangle\langle m\rvert)\\
                \notag=&\frac{1}{2}\sum_k(\langle k\vert j\rangle\langle i\vert m\rangle\langle n\vert k\rangle+\langle k\vert j\rangle\langle i\vert n\rangle\langle m\vert k\rangle\\
                \notag&+\langle k\vert i\rangle\langle j\vert m\rangle\langle n\vert k\rangle+\langle k\vert i\rangle\langle j\vert n\rangle\langle m\vert k\rangle)\\
                \notag=&\frac{1}{2}\sum_k(\delta_{kj}\delta_{im}\delta_{nk}+\delta_{kj}\delta_{in}\delta_{mk}+\delta_{ki}\delta_{jm}\delta_{nk}+\delta_{ki}\delta_{jn}\delta_{mk})\\
                =&\delta_{im}\delta_{jn}+\delta_{in}\delta_{jm},\quad\forall 1\leq i<j\leq d,1\leq m<n\leq d,\\
                \notag\left(\frac{\lvert i\rangle\langle j\rvert-\lvert j\rangle\langle i\rvert}{i\sqrt{2}},\frac{\lvert m\rangle\langle n\rvert-\lvert n\rangle\langle m\rvert}{i\sqrt{2}}\right)=&\tr\left[\left(\frac{\lvert i\rangle\langle j\rvert-\lvert j\rangle\langle i\rvert}{i\sqrt{2}}\right)^{\dagger}\frac{\lvert m\rangle\langle n\rvert-\lvert n\rangle\langle m\rvert}{i\sqrt{2}}\right]\\
                \notag=&\frac{1}{2}\tr\left[(\lvert j\rangle\langle i\rvert-\lvert i\rangle\langle j\rvert)(\lvert m\rangle\langle n\rvert-\lvert n\rangle\langle m\rvert)\right]\\
                \notag=&\frac{1}{2}\tr(\lvert j\rangle\langle i\vert m\rangle\langle n\rvert-\lvert j\rangle\langle i\vert n\rangle\langle m\rvert-\lvert i\rangle\langle j\vert m\rangle\langle n\rvert+\lvert j\rangle\langle i\vert n\rangle\langle m\rvert)\\
                \notag=&\frac{1}{2}\sum_k(\langle k\vert j\rangle\langle i\vert m\rangle\langle n\vert k\rangle-\langle k\vert j\rangle\langle i\vert n\rangle\langle m\vert k\rangle\\
                \notag&-\langle k\vert i\rangle\langle j\vert m\rangle\langle n\vert k\rangle+\langle k\vert i\rangle\langle j\vert n\rangle\langle m\vert k\rangle)\\
                \notag=&\frac{1}{2}\sum_k(\delta_{kj}\delta_{im}\delta_{nk}-\delta_{kj}\delta_{in}\delta_{mk}-\delta_{ki}\delta_{jm}\delta_{nk}+\delta_{ki}\delta_{jn}\delta_{mk})\\
                =&\delta_{im}\delta_{jn}-\delta_{in}\delta_{jm}=\delta_{im}\delta_{jn},\quad\forall 1\leq i<j\leq d,1\leq m<n\leq d.
            \end{align}
            and thus linearly independent.
        \end{itemize}
        Therefore, $\left\{\lvert i\rangle\langle i\rvert,\frac{\lvert i\rangle\langle j\rvert+\lvert j\rangle\langle i\rvert}{\sqrt{2}},\frac{\lvert i\rangle\langle j\rvert-\lvert j\rangle\langle i\rvert}{i\sqrt{2}};\forall 1\leq i<j\leq d\right\}$ is an orthonormal basis of Hermitian matrices for the Hilbert space $L_V$.
    \end{itemize}
\end{sol}

\begin{exe}[commutation relation for the Pauli matrices]
    Verify the commutation relations
    \begin{align}
        [X,Y]=2iZ;\quad[Y,Z]=2iX;\quad[Z,X]=2iY.
    \end{align}
    There is an elegant way of writing this using $\epsilon_{jkl}$, the alternative antisymmetric tensor on three indices for which $\epsilon_{jkl}=0$ except for $\epsilon_{123}=\epsilon_{231}=\epsilon_{312}=1$, and $\epsilon_{321}=\epsilon_{213}=\epsilon_{132}=-1$:
    \begin{align}
        [\sigma_j,\sigma_k]=2i\sum_{l=1}^3\epsilon_{jkl}\sigma_l.
    \end{align}
\end{exe}
\begin{pf}
    The commutation relations for the Pauli matrices are
    \begin{align}
        [X,Y]=&XY-YX=\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]-\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]=\left[\begin{matrix}
            i&0\\
            0&-i
        \end{matrix}\right]-\left[\begin{matrix}
            -i&0\\
            0&i
        \end{matrix}\right]=2i\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]=2iZ,\\
        [Y,Z]=&YZ-ZY=\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]-\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]=\left[\begin{matrix}
            0&i\\
            i&0
        \end{matrix}\right]-\left[\begin{matrix}
            0&-i\\
            -i&0
        \end{matrix}\right]=2i\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]=2iX,\\
        [Z,X]=&ZX-XZ=\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]-\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]=\left[\begin{matrix}
            0&1\\
            -1&0
        \end{matrix}\right]-\left[\begin{matrix}
            0&-1\\
            1&0
        \end{matrix}\right]=2i\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]=2iY,
    \end{align}
    and
    \begin{align}
        [\sigma_j,\sigma_j]=&\sigma_l\sigma_l-\sigma_l\sigma_l=0,\quad\forall j=1,2,3,\\
        [\sigma_j,\sigma_k]=&\sigma_j\sigma_k-\sigma_k\sigma_j=-[\sigma_k,\sigma_j],\quad\forall j,k=1,2,3.
    \end{align}
    Therefore, in general,
    \begin{align}
        [\sigma_j,\sigma_k]=2i\sum_{l=1}^3\epsilon_{jkl}\sigma_l.
    \end{align}
\end{pf}

\begin{exe}[Anti-commutation relation for the Pauli matrices]
    Verify the anti-commutation relations
    \begin{align}
        \{\sigma_i,\sigma_j\}=0
    \end{align}
    where $i\neq j$ are both chosen from the set $1,2,3$. Also verify that ($i=0,1,2,3$)
    \begin{align}
        \sigma_i^2=I.
    \end{align}
\end{exe}
\begin{pf}
    \begin{align}
        \{\sigma_1,\sigma_2\}=&\sigma_1\sigma_2+\sigma_2\sigma_1=\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]+\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]=\left[\begin{matrix}
            i&0\\
            0&-i
        \end{matrix}\right]+\left[\begin{matrix}
            -i&0\\
            0&i
        \end{matrix}\right]=0,\\
        \{\sigma_2,\sigma_3\}=&\sigma_2\sigma_3+\sigma_3\sigma_2=\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]+\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]=\left[\begin{matrix}
            0&i\\
            i&0
        \end{matrix}\right]+\left[\begin{matrix}
            0&-i\\
            -i&0
        \end{matrix}\right]=0,\\
        \{\sigma_3,\sigma_1\}=&\sigma_3\sigma_1+\sigma_1\sigma_3=\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]+\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]=\left[\begin{matrix}
            0&1\\
            -1&0
        \end{matrix}\right]+\left[\begin{matrix}
            0&-1\\
            1&0
        \end{matrix}\right]=0,\\
        \sigma_0^2=&I^2=I,\\
        \sigma_1^2=&\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]=\left[\begin{matrix}
            1&0\\
            0&1
        \end{matrix}\right]=I,\\
        \sigma_2^2=&\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]=\left[\begin{matrix}
            1&0\\
            0&1
        \end{matrix}\right]=I,\\
        \sigma_3^2=&\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]=\left[\begin{matrix}
            1&0\\
            0&1
        \end{matrix}\right]=I.
    \end{align}
    Therefore, in general,
    \begin{align}
        \{\sigma_i,\sigma_j\}=0,\quad\forall i,j=1,2,3,\text{ and }i\neq j,
    \end{align}
    and
    \begin{align}
        \sigma_i^2=I,\quad\forall i=0,1,2,3.
    \end{align}
\end{pf}

\begin{exe}
    Verify that
    \begin{align}
        AB=\frac{[A,B]+\{A,B\}}{2}.
    \end{align}
\end{exe}
\begin{pf}
    \begin{align}
        \frac{[A,B]+\{A,B\}}{2}=\frac{AB-BA+AB+BA}{2}=AB.
    \end{align}
\end{pf}

\begin{exe}
    Show that for $j,k=1,2,3$,
    \begin{align}
        \sigma_j\sigma_k=\delta_{jk}I+i\sum_{l=1}^3\epsilon_{jkl}\sigma_l.
    \end{align}
\end{exe}
\begin{pf}
    As obtained in exercise 2.40, 2.41 and 2.42,
    \begin{align}
        [\sigma_j,\sigma_k]=&2i\sum_{l=1}^3\epsilon_{jkl}\sigma_l,\quad\forall j,k=1,2,3,\\
        \{\sigma_j,\sigma_k\}=&2\delta_{jk}I,\quad\forall j,k=1,2,3,\\
        AB=&\frac{[A,B]+\{A,B\}}{2},
    \end{align}
    so
    \begin{align}
        \sigma_j\sigma_k=\frac{[\sigma_j,\sigma_k]+\{\sigma_j,\sigma_k\}}{2}=\frac{2i\sum_{l=1}^3\epsilon_{jkl}\sigma_l+2\delta_{jk}I}{2}=\delta_{jk}I+i\sum_{l=1}^3\epsilon_{jkl}\sigma_l,\quad\forall j,k=1,2,3.
    \end{align}
\end{pf}

\ifx\allfiles\undefined
\end{document}
\fi