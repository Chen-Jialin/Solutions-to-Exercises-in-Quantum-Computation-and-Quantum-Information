% !Tex program = pdflatex
% Chapter 2 - Introduction to quantum mechanics
\ifx\allfiles\undefined
\documentclass[en]{sol-man}
\begin{document}
\fi
\chapter{Introduction to quantum mechanics}

\section{Linear algebra}

\begin{exe}[Linear dependence: example]
    Show that $(1,-1)$, $(1,2)$ and $(2,1)$ are Linearly dependent.
\end{exe}
\begin{pf}
    Since
    \begin{align}
        (1,-1)+(1,2)-(2,1)=0,
    \end{align}
    these three vectors are linearly dependent.
\end{pf}

\begin{exe}[Matrix representations: example]
    Suppose $V$ is a vector space with basis vectors $\lvert 0\rangle$ and $\lvert 1\rangle$, and $A$ is a linear operator from $V$ to $V$ such that $A\lvert 0\rangle=\lvert 1\rangle$ and $A\lvert 1\rangle=\lvert 0\rangle$. Give a matrix representation for $A$, with respect to the input basis $\lvert 0\rangle$, $\lvert 1\rangle$, and the output basis $\lvert 0\rangle$ and $\lvert 1\rangle$. Find input and output bases which give rise to a different matrix representation of $A$.
\end{exe}
\begin{sol}
    The matrix representation for $A$ with respect to the input basis $\lvert 0\rangle$, $\lvert 1\rangle$ and the output basis $\lvert 0\rangle$ and $\lvert 1\rangle$ is
    \begin{align}
        \left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right].
    \end{align}

    Keep $\lvert 0\rangle$ and $\lvert 1\rangle$ as the input basis and choose $\lvert+\rangle=(\lvert 0\rangle+\lvert 1\rangle)/\sqrt{2}$ and $\lvert-\rangle=(\lvert 0\rangle-\lvert 1\rangle)/\sqrt{2}$ as the output basis, then $A$ can be regarded as a linear operator from $V$ to $V$ such that $A\lvert 0\rangle=(\lvert+\rangle-\lvert-\rangle)/\sqrt{2}$ and $A\lvert 1\rangle=(\lvert 0\rangle+\lvert 1\rangle)/\sqrt{2}$. In this way, the matrix representation for $A$ is
    \begin{align}
        \frac{1}{\sqrt{2}}\left[\begin{matrix}
            1&-1\\
            1&1
        \end{matrix}\right].
    \end{align}
\end{sol}

\begin{exe}[Matrix representation for operator products]
    Suppose $A$ is a linear operator from vector space $V$ to vector space $W$, and $B$ is a linear operator from vector spaces $W$ to vector space $X$. Let $\lvert v_i\rangle$, $\lvert w_j\rangle$, and $\lvert x_k\rangle$ be bases for the vector spaces $V$, $W$, and $X$, respectively. Show that the matrix representation for the linear transformation $BA$ is the matrix product of the matrix representations for $B$ and $A$, with respect to the appropriate bases.
\end{exe}
\begin{pf}
    Suppose the dimension of vector spaces $V$, $W$, and $W$ are $l$, $m$, $n$, respectively. Since $A$ is a linear operator from vector space $V$ to vector space $W$, for each $i$ in the range $1,\cdots,l$, there exist complex numbers $A_{1i}$ through $A_{mi}$ such that
    \begin{align}
        A\lvert v_i\rangle=\sum_jA_{ji}\lvert w_j\rangle,
    \end{align}
    where $A_{ji}$ is the entries of the matrix representation of the operator $A$.
    Since $B$ is a linear operator from vector space $W$ to vector space $X$, for each $j$ in the range $1,\cdots,m$, there exist complex numbers $A_{1j}$ through $A_{nj}$ such that
    \begin{align}
        B\lvert w_j\rangle=\sum_kB_{kj}\lvert x_k\rangle,
    \end{align}
    where $B_{kj}$ is the entries of the matrix representation of the operator $B$.
    Putting the above two equations together, we have
    \begin{align}
        BA\lvert v_i\rangle=B\sum_jA_{ji}\lvert w_j\rangle=\sum_jA_{ji}\sum_kB_{kj}\lvert x_k\rangle=\sum_k\left(\sum_jB_{kj}A_{ji}\right)\lvert x_k\rangle=\sum_k(BA)_{ki}\lvert x_k\rangle.
    \end{align}
    where $(AB)_{ki}$ is the entries of the matrix representation of the operator $BA$. Therefore, the matrix representation for the linear transformation $BA$ is the matrix product of the matrix representations for $B$ and $A$, with respect to the appropriate bases.
\end{pf}

\begin{exe}[Matrix representation for identity]
    Show that the identity operator on a vector space $V$ has a matrix representation which is one along the diagonal and zero everywhere else, if the matrix representation is taken with respect to the same input and output bases. The matrix is known as the \emph{identity matrix}.
\end{exe}
\begin{pf}
    Suppose $I$ is the identity operator on the vector space $V$ and choose $\lvert v_1\rangle,\cdots,\lvert v_m\rangle$ as both the input basis and the output basis for $V$. Then for each $j$ in the range $1,\cdots,m$, there exist complex numbers $I_{1j}$ through $I_{mj}$ such that
    \begin{gather}
        I\lvert v_j\rangle=\sum_iA_{ij}\lvert v_i\rangle=\lvert v_j\rangle,\\
        \Longrightarrow(A_{jj}-1)\lvert v_j\rangle+\sum_{i\neq j}A_{ij}\lvert v_i\rangle=0,
    \end{gather}
    where $A_{ij}$ is the entries of the matrix representation of the operator $I$. Due to linear independence of the basis, there must be
    \begin{align}
        v_{ij}=\left\{\begin{array}{ll}
            1,&i=j;\\
            0,&i\neq j,
        \end{array}\right.=\delta_{ij},\quad\forall i,j=1,\cdots,m.
    \end{align}
    i.e., the identity operator on the vector space $V$ has a matrix representation which is one along the diagonal and zero everywhere else.
\end{pf}

\begin{exe}
    Verify that $(\cdot,\cdot)$ just defined is an inner product on $\mathbb{C}^n$.
\end{exe}
\begin{pf}
    $(\cdot,\cdot)$ just defined satisfies the requirements that:
    \begin{itemize}
        \item[(1)] $(\cdot,\cdot)$ is linear in the second argument,
        \begin{align}
            \notag\left((y_1,\cdots,y_n),\sum_j\lambda_j(z_1^{(j)},\cdots,z_n^{(j)})\right)=&\left((y_1,\cdots,y_n),(\sum_j\lambda_jz_1^{(j)},\cdots,\sum_j\lambda_jz_n^{(j)})\right)=\sum_iy_i^*\sum_j\lambda_jz_i^{(j)}\\
            =&\sum_j\lambda_j\sum_iy_i^*z_i^{(j)}=\sum_j\lambda_j((y_1,\cdots,y_n),(z_1^{(j)},\cdots,z_n^{(j)})).
        \end{align}
        \item[(2)] 
        \begin{align}
            ((y_1,\cdots,y_n),(z_1,\cdots,z_n))=\sum_iy_i^*z_i=\left(\sum_iz_i^*y_i\right)^*=((z_1,\cdots,z_n),(y_1,\cdots,y_n))^*.
        \end{align}
        \item[(3)] 
        \begin{align}
            ((y_1,\cdots,y_n),(y_1,\cdots,y_n))=\sum_iy_i^*y_i=\sum_i\abs{y_i}^2\geq 0,
        \end{align}
        with equality if and only if $(y_1,\cdots,y_n)=0$.
    \end{itemize}
    Therefore, $(\cdot,\cdot)$ is an inner product on $\mathbb{C}^n$.
\end{pf}

\begin{exe}
    Show that any inner product $(\cdot,\cdot)$ is conjugate-linear in the first argument,
    \begin{align}
        \left(\sum_i\lambda_i\lvert w_i\rangle,\lvert v\rangle\right)=\sum_i\lambda_i^*(\lvert w_i\rangle,\lvert v\rangle).
    \end{align}
\end{exe}
\begin{sol}
    \begin{align}
        \left(\sum_i\lambda_i\lvert w_i\rangle,\lvert v\rangle\right)=\left(\lvert v\rangle,\sum_i\lambda_i\lvert w_i\rangle\right)^*=\left(\sum_i\lambda_i(\lvert v\rangle,\lvert w_i\rangle)\right)^*=\sum_i\lambda_i^*(\lvert w_i\rangle,\lvert v\rangle).
    \end{align}
    Therefore, any inner product $(\cdot,\cdot)$ is conjugate-linear in the first argument.
\end{sol}

\begin{exe}
    Verify that $\lvert w\rangle\equiv(1,1)$ and $\lvert v\rangle\equiv(1,-1)$ are orthogonal. What are the normalized forms of these vectors?
\end{exe}
\begin{sol}
    Since
    \begin{align}
        (\lvert w\rangle,\lvert v\rangle)=1\times 1+1\times(-1)=0,
    \end{align}
    these two vectors are orthogonal. The normalized form of $\lvert w\rangle$ and $\lvert v\rangle$ are
    \begin{align}
        \frac{\lvert w\rangle}{\norm{\lvert w\rangle}}=\frac{1}{\sqrt{2}}(1,1),
    \end{align}
    and
    \begin{align}
        \frac{\lvert v\rangle}{\norm{\lvert v\rangle}}=\frac{1}{\sqrt{2}}(1,-1),
    \end{align}
    respectively.
\end{sol}

\begin{exe}
    Prove that the Gram-Schmidt procedure produces an orthonormal basis for $V$.
\end{exe}
\begin{sol}
    Obviously, $\lvert v_1\rangle,\cdots,\lvert v_n\rangle$ are normalized, so we first prove that $\lvert v_1\rangle,\cdots,\lvert v_d\rangle$ are orthogonal with induction.
    For $k=1$,
    \begin{align}
        (\lvert v_1\rangle,\lvert v_{k+1}\rangle)=&(\lvert v_1,\lvert v_2\rangle\rangle)=\left(\lvert v_1\rangle,\frac{\lvert w_2\rangle-\langle v_1\vert w_2\rangle\lvert v_1\rangle}{\norm{\lvert v_1\rangle-\langle v_1\vert w_2\rangle\lvert v_1\rangle}}\right)=\frac{\langle v_1\vert w_2\rangle-\langle v_1\vert w_2\rangle\langle v_1\vert v_1\rangle}{\norm{\lvert v_1\rangle-\langle v_1\vert w_2\rangle\lvert v_1\rangle}}=\frac{\langle v_1\vert w_2\rangle-\langle v_1\vert w_2\rangle}{\norm{\lvert v_1\rangle-\langle v_1\vert w_2\rangle\lvert v_1\rangle}}=0,\\
    \end{align}
    so $\lvert v_1\rangle,\cdots,\lvert v_{k+1}\rangle$ are orthogonal for $k=1$.
    For $k\geq 2$, if $\lvert v_1\rangle,\cdots,\lvert v_k\rangle$ are orthogonal, then
    \begin{align}
        \notag(\lvert v_j\rangle,\lvert v_{k+1}\rangle)=&\left(\lvert v_j\rangle,\frac{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}{\norm{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}}\right)=\frac{\langle v_j\vert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\langle v_j\vert v_i\rangle}{\norm{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}}\\
        =&\frac{\langle v_j\vert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\delta_{ji}}{\norm{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}}=\frac{\langle v_j\vert w_{k+1}\rangle-\langle v_j\vert w_{k+1}\rangle}{\norm{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}}=0,\quad\forall j=1,\cdots,k,
    \end{align}
    so $\lvert v_1\rangle,\cdots,\lvert v_{k+1}\rangle$ are orthogonal for $k\geq 2$, such as $k=d-1$.
    Till now, we proved that $\lvert v_1\rangle,\cdots,\lvert v_d\rangle$ are orthonormal.

    We then prove that $\lvert v_1\rangle,\cdots,\lvert v_n\rangle$ are a basis for $V$.
    As proved above, $\lvert v_1\rangle,\cdots,\lvert v_d\rangle$ are orthonormal, and thus linear independent. Since $\lvert w_1\rangle,\cdots,\lvert w_d\rangle$ are a basis for $V$. Any vector $\lvert v\rangle$ in $V$ can be written as a linear combination of $\lvert w_1\rangle,\cdots,\lvert w_d\rangle$:
    \begin{align}
        \lvert v\rangle=\sum_{i=1}^da_i\lvert w_i\rangle.
    \end{align}
    Using
    \begin{align}
        \label{E2.8-w1-v1}
        \lvert v_1\rangle=\frac{\lvert w_1\rangle}{\norm{\lvert w_1\rangle}}\Longrightarrow \lvert w_1\rangle=\norm{\lvert w_1\rangle}\lvert v_1\rangle,\\
    \end{align}
    and
    \begin{align}
        \label{E2.8-w-v}
        \notag\lvert v_{k+1}\rangle=&\frac{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}{\norm{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}}\\
        \Longrightarrow&\lvert w_{k+1}\rangle=\norm{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}\lvert v_{k+1}\rangle+\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle,\quad\forall k=1,\cdots,d-1
    \end{align}
    we can rewrite $\lvert v\rangle$ as a linear combination of $\lvert v_1\rangle,\cdots,\lvert v_d\rangle$:
    \begin{align}
        \lvert v\rangle=a_1\norm{\lvert w_1\rangle}\lvert v_1\rangle+\sum_{k=1}^{d-1}a_{k+1}\left(\norm{\lvert w_{k+1}\rangle-\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle}\lvert v_{k+1}\rangle+\sum_{i=1}^k\langle v_i\vert w_{k+1}\rangle\lvert v_i\rangle\right),
    \end{align}
    so $\lvert v_1\rangle,\cdots,\lvert v_d\rangle$ span and form a basis for $V$.

    Therefore, the Gram-Schmidt procedure produces an orthonormal basis for $V$.
\end{sol}

\begin{exe}[Pauli operators and the outer product]
    The Pauli matrices (Figure 2.2 on page 65) can be considered as operators with respect to an orthonormal basis $\lvert 0\rangle$, $\lvert 1\rangle$ for a two-dimensional Hilbert space. Express each of the Pauli operators in the outer product notion.
\end{exe}
\begin{sol}
    The Pauli operators in the outer product notion:
    \begin{align}
        \sigma_0=&\sum_{m,n=1}^2\langle m\rvert\sigma_0\lvert n\rangle\lvert m\rangle\langle n\rvert=\lvert 0\rangle\langle 0\rvert+\lvert 1\rangle\langle 1\rvert,\\
        \sigma_1=&\sum_{m,n=1}^2\langle m\rvert\sigma_1\lvert n\rangle\lvert m\rangle\langle n\rvert=\lvert 0\rangle\langle 1\rvert+\lvert 1\rangle\langle 0\rvert,\\
        \sigma_2=&\sum_{m,n=1}^2\langle m\rvert\sigma_2\lvert n\rangle\lvert m\rangle\langle n\rvert=-i\lvert 0\rangle\langle 1\rvert+i\lvert 1\rangle\langle 0\rvert,\\
        \sigma_3=&\sum_{m,n=1}^2\langle m\rvert\sigma_3\lvert n\rangle\lvert m\rangle\langle n\rvert=\lvert 0\rangle\langle 0\rvert-\lvert 1\rangle\langle 1\rvert.
    \end{align}
\end{sol}

\begin{exe}
    Suppose $\lvert v_i\rangle$ is an orthonormal basis for and inner product space $V$. What is the matrix representation for the operator $\lvert v_j\rangle\langle v_k\rvert$, with respect to the $\lvert v_i\rangle$ basis?
\end{exe}
\begin{sol}
    The matrix representation for the operator $\lvert v_j\rangle\langle v_k\rvert$ with respect to the $\lvert v_i\rangle$ basis:
    \begin{align}
        \lvert v_j\rangle\langle v_k\rvert=\begin{array}{cc}
            \begin{array}{ccccccc}
                \multicolumn{7}{c}{k\text{th column}} \\
                 &  &  & \downarrow &  &  & 
                \end{array} & \begin{array}{c}
                    \\
                    \\
                   \end{array} \\
            \left[\begin{array}{ccccccc}
                0 &  &  &  & \multicolumn{3}{c}{\multirow{3}{*}{\Huge0}} \\
                 & \ddots &  &  & \multicolumn{3}{c}{} \\
                 &  & 0 &  & \multicolumn{3}{c}{} \\
                 &  &  & 1 &  &  &  \\
                \multicolumn{3}{c}{\multirow{3}{*}{\Huge0}} &  & 0 &  &  \\
                \multicolumn{3}{c}{} &  &  & \ddots &  \\
                \multicolumn{3}{c}{} &  &  &  & 0
                \end{array}\right] & \begin{array}{c}
                    \\
                    \\
                    \\
                   \leftarrow\,j\text{th row} \\
                    \\
                    \\
                    \\
                   \end{array}
        \end{array}
    \end{align}
    (a matrix with all zeros expect a one at the $j$th row and $k$th column).
\end{sol}

\begin{exe}[Eigendecomposition of the Pauli matrices]
    Find the eigenvectors, eigenvalues, and diagonal representations of the Pauli matrices $X$, $Y$, and $Z$.
\end{exe}
\begin{sol}
    The characteristic equation of $X$
    \begin{align}
        \det\abs{X-\lambda I}=\abs{\begin{matrix}
            -\lambda&1\\
            1&-\lambda
        \end{matrix}}=\lambda^2-1=0,
    \end{align}
    gives the eigenvalues
    \begin{align}
        \lambda_1=1,\quad\lambda_2=-1.
    \end{align}
    The eigenequations of $X$
    \begin{align}
        X\lvert v_1\rangle=\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right]=\left[\begin{matrix}
            b_1\\
            a_1
        \end{matrix}\right]=\lambda_1\lvert v_1\rangle=\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right],\quad X\lvert v_2\rangle=\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]\left[\begin{matrix}
            a_2\\
            b_2
        \end{matrix}\right]=\left[\begin{matrix}
            b_2\\
            a_2
        \end{matrix}\right]=\lambda_2\lvert v_2\rangle=\left[\begin{matrix}
            -a_2\\
            -b_2
        \end{matrix}\right],
    \end{align}
    give the corresponding eigenvectors
    \begin{align}
        \lvert v_1\rangle=\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            1
        \end{matrix}\right]=\frac{1}{\sqrt{2}}(\lvert 0\rangle+\lvert 1\rangle),\quad\lvert v_2\rangle=\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            -1
        \end{matrix}\right]=\frac{1}{\sqrt{2}}(\lvert 0\rangle-\lvert 1\rangle).
    \end{align}
    The diagonal representation of $X$ is
    \begin{align}
        X=\left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right]=\frac{1}{\sqrt{2}}(\lvert 0\rangle+\lvert 1\rangle)\frac{1}{\sqrt{2}}(\langle 0\rvert+\langle 1\rvert)-\frac{1}{\sqrt{2}}(\lvert 0\rangle-\lvert 1\rangle)\frac{1}{\sqrt{2}}(\langle 0\rvert-\langle 1\rvert).
    \end{align}
    The characteristic equation of $Y$
    \begin{align}
        \det\abs{Y-\lambda I}=\abs{\begin{matrix}
            -\lambda&-i\\
            i&-\lambda
        \end{matrix}}=\lambda^2-1=0,
    \end{align}
    gives the eigenvalues
    \begin{align}
        \lambda_1=1,\quad\lambda_2=-1.
    \end{align}
    The eigenequations of $Y$
    \begin{align}
        Y\lvert v_1\rangle=\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right]=\left[\begin{matrix}
            -ib_1\\
            ia_1
        \end{matrix}\right]=\lambda_1\lvert v_1\rangle=\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right],\quad Y\lvert v_2\rangle=\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]\left[\begin{matrix}
            a_2\\
            b_2
        \end{matrix}\right]=\left[\begin{matrix}
            -ib_2\\
            ia_2
        \end{matrix}\right]=\lambda_2\lvert v_2\rangle=\left[\begin{matrix}
            -a_2\\
            -b_2
        \end{matrix}\right],
    \end{align}
    give the corresponding eigenvectors
    \begin{align}
        \lvert v_1\rangle=\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            i
        \end{matrix}\right]=\frac{1}{\sqrt{2}}(\lvert 0\rangle+i\lvert 1\rangle),\quad\lvert v_2\rangle=\frac{1}{\sqrt{2}}\left[\begin{matrix}
            1\\
            -i
        \end{matrix}\right]=\frac{1}{\sqrt{2}}(\lvert 0\rangle-i\lvert 1\rangle).
    \end{align}
    The diagonal representation of $Y$ is
    \begin{align}
        Y=\left[\begin{matrix}
            0&-i\\
            i&0
        \end{matrix}\right]=\frac{1}{\sqrt{2}}(\lvert 0\rangle-i\lvert 1\rangle)\frac{1}{\sqrt{2}}(\langle 0\rvert+i\langle 1\rvert)-\frac{1}{\sqrt{2}}(\lvert 0\rangle+i\lvert 1\rangle)\frac{1}{\sqrt{2}}(\langle 0\rvert-i\langle 1\rvert).
    \end{align}
    The characteristic equation of $Z$
    \begin{align}
        \det\abs{Z-\lambda I}=\abs{\begin{matrix}
            1-\lambda&0\\
            0&-1-\lambda
        \end{matrix}}=\lambda^2-1=0,
    \end{align}
    gives the eigenvalues
    \begin{align}
        \lambda_1=1,\quad\lambda_2=-1.
    \end{align}
    The eigenequations of $Z$
    \begin{align}
        Z\lvert v_1\rangle=\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right]=\left[\begin{matrix}
            a_1\\
            -b_1
        \end{matrix}\right]=\lambda_1\lvert v_1\rangle=\left[\begin{matrix}
            a_1\\
            b_1
        \end{matrix}\right],\quad Z\lvert v_2\rangle=\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]\left[\begin{matrix}
            a_2\\
            -b_2
        \end{matrix}\right]=\lambda_2\lvert v_2\rangle=\left[\begin{matrix}
            -a_2\\
            -b_2
        \end{matrix}\right],
    \end{align}
    give the corresponding eigenvectors
    \begin{align}
        \lvert v_1\rangle=\left[\begin{matrix}
            1\\
            0
        \end{matrix}\right]=\lvert 0\rangle,\quad\lvert v_2\rangle=\left[\begin{matrix}
            0\\
            1
        \end{matrix}\right].
    \end{align}
    The diagonal representation of $Z$ is
    \begin{align}
        Z=\left[\begin{matrix}
            1&0\\
            0&-1
        \end{matrix}\right]=\lvert 0\rangle\langle 0\rvert-\lvert 1\rangle\langle 1\rvert.
    \end{align}
\end{sol}

\begin{exe}
    Prove that the matrix
    \begin{align}
        \left[\begin{matrix}
            1&0\\
            1&1
        \end{matrix}\right]
    \end{align}
    is not diagonalizable.
\end{exe}
\begin{pf}

\end{pf}

\ifx\allfiles\undefined
\end{document}
\fi