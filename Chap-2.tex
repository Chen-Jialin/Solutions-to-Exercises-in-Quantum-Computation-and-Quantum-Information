% !Tex program = pdflatex
% Chapter 2 - Introduction to quantum mechanics
\ifx\allfiles\undefined
\documentclass[en]{sol-man}
\begin{document}
\fi
\chapter{Introduction to quantum mechanics}

\section{Linear algebra}

\begin{exe}[Linear dependence: example]
    Show that $(1,-1)$, $(1,2)$ and $(2,1)$ are Linearly dependent.
\end{exe}
\begin{sol}
    Since
    \begin{align}
        (1,-1)+(1,2)-(2,1)=0,
    \end{align}
    these three vectors are linearly dependent.
\end{sol}

\begin{exe}[Matrix representations: example]
    Suppose $V$ is a vector space with basis vectors $\lvert 0\rangle$ and $\lvert 1\rangle$, and $A$ is a linear operator from $V$ to $V$ such that $A\lvert 0\rangle=\lvert 1\rangle$ and $A\lvert 1\rangle=\lvert 0\rangle$. Give a matrix representation for $A$, with respect to the input basis $\lvert 0\rangle$, $\lvert 1\rangle$, and the output basis $\lvert 0\rangle$ and $\lvert 1\rangle$. Find input and output bases which give rise to a different matrix representation of $A$.
\end{exe}
\begin{sol}
    The matrix representation for $A$ with respect to the input basis $\lvert 0\rangle$, $\lvert 1\rangle$ and the output basis $\lvert 0\rangle$ and $\lvert 1\rangle$ is
    \begin{align}
        \left[\begin{matrix}
            0&1\\
            1&0
        \end{matrix}\right].
    \end{align}

    Keep $\lvert 0\rangle$ and $\lvert 1\rangle$ as the input basis and choose $\lvert+\rangle=(\lvert 0\rangle+\lvert 1\rangle)/\sqrt{2}$ and $\lvert-\rangle=(\lvert 0\rangle-\lvert 1\rangle)/\sqrt{2}$ as the output basis, then $A$ can be regarded as a linear operator from $V$ to $V$ such that $A\lvert 0\rangle=(\lvert+\rangle-\lvert-\rangle)/\sqrt{2}$ and $A\lvert 1\rangle=(\lvert 0\rangle+\lvert 1\rangle)/\sqrt{2}$. In this way, the matrix representation for $A$ is
    \begin{align}
        \frac{1}{\sqrt{2}}\left[\begin{matrix}
            1&-1\\
            1&1
        \end{matrix}\right].
    \end{align}
\end{sol}

\begin{exe}[Matrix representation for operator products]
    Suppose $A$ is a linear operator from vector space $V$ to vector space $W$, and $B$ is a linear operator from vector spaces $W$ to vector space $X$. Let $\lvert v_i\rangle$, $\lvert w_j\rangle$, and $\lvert x_k\rangle$ be bases for the vector spaces $V$, $W$, and $X$, respectively. Show that the matrix representation for the linear transformation $BA$ is the matrix product of the matrix representations for $B$ and $A$, with respect to the appropriate bases.
\end{exe}
\begin{sol}
    Suppose the dimension of vector spaces $V$, $W$, and $W$ are $l$, $m$, $n$, respectively. Since $A$ is a linear operator from vector space $V$ to vector space $W$, for each $i$ in the range $1,\cdots,l$, there exist complex numbers $A_{1i}$ through $A_{mi}$ such that
    \begin{align}
        A\lvert v_i\rangle=\sum_jA_{ji}\lvert w_j\rangle,
    \end{align}
    where $A_{ji}$ is the entries of the matrix representation of the operator $A$. Since $B$ is a linear operator from vector space $W$ to vector space $X$, for each $j$ in the range $1,\cdots,m$, there exist complex numbers $A_{1j}$ through $A_{nj}$ such that
    \begin{align}
        B\lvert w_j\rangle=\sum_kB_{kj}\lvert x_k\rangle,
    \end{align}
    where $B_{kj}$ is the entries of the matrix representation of the operator $B$. Putting the above two equations together, we have
    \begin{align}
        BA\lvert v_i\rangle=B\sum_jA_{ji}\lvert w_j\rangle=\sum_jA_{ji}\sum_kB_{kj}\lvert x_k\rangle=\sum_k\left(\sum_jB_{kj}A_{ji}\right)\lvert x_k\rangle=\sum_k(BA)_{ki}\lvert x_k\rangle.
    \end{align}
    where $(AB)_{ki}$ is the entries of the matrix representation of the operator $BA$. Therefore, the matrix representation for the linear transformation $BA$ is the matrix product of the matrix representations for $B$ and $A$, with respect to the appropriate bases.
\end{sol}

\begin{exe}[Matrix representation for identity]
    Show that the identity operator on a vector space $V$ has a matrix representation which is one along the diagonal and zero everywhere else, if the matrix representation is taken with respect to the same input and output bases. The matrix is known as the \emph{identity matrix}.
\end{exe}
\begin{sol}
    Suppose $I$ is the identity operator on the vector space $V$ and choose $\lvert v_1\rangle,\cdots,\lvert v_m\rangle$ as both the input basis and the output basis for $V$. Then for each $j$ in the range $1,\cdots,m$, there exist complex numbers $I_{1j}$ through $I_{mj}$ such that
    \begin{gather}
        I\lvert v_j\rangle=\sum_iA_{ij}\lvert v_i\rangle=\lvert v_j\rangle,\\
        \Longrightarrow(A_{jj}-1)\lvert v_j\rangle+\sum_{i\neq j}A_{ij}\lvert v_i\rangle=0,
    \end{gather}
    where $A_{ij}$ is the entries of the matrix representation of the operator $I$. Due to linear independence of the basis, there must be
    \begin{align}
        v_{ij}=\left\{\begin{array}{ll}
            1,&i=j;\\
            0,&i\neq j,
        \end{array}\right.=\delta_{ij},\quad\forall i,j=1,\cdots,m.
    \end{align}
    i.e., the identity operator on the vector space $V$ has a matrix representation which is one along the diagonal and zero everywhere else.
\end{sol}

\begin{exe}
    Verify that $(\cdot,\cdot)$ just defined is an inner product on $\mathbb{C}^n$.
\end{exe}
\begin{sol}
    $(\cdot,\cdot)$ just defined satisfies the requirements that:
    \begin{itemize}
        \item[(1)] $(\cdot,\cdot)$ is linear in the second argument,
        \begin{align}
            \notag\left((y_1,\cdots,y_n),\sum_j\lambda_j(z_1^{(j)},\cdots,z_n^{(j)})\right)=&\left((y_1,\cdots,y_n),(\sum_j\lambda_jz_1^{(j)},\cdots,\sum_j\lambda_jz_n^{(j)})\right)=\sum_iy_i^*\sum_j\lambda_jz_i^{(j)}\\
            =&\sum_j\lambda_j\sum_iy_i^*z_i^{(j)}=\sum_j\lambda_j((y_1,\cdots,y_n),(z_1^{(j)},\cdots,z_n^{(j)})).
        \end{align}
        \item[(2)] 
        \begin{align}
            ((y_1,\cdots,y_n),(z_1,\cdots,z_n))=\sum_iy_i^*z_i=\left(\sum_iz_i^*y_i\right)^*=((z_1,\cdots,z_n),(y_1,\cdots,y_n))^*.
        \end{align}
        \item[(3)] 
        \begin{align}
            ((y_1,\cdots,y_n),(y_1,\cdots,y_n))=\sum_iy_i^*y_i=\sum_i\abs{y_i}^2\geq 0,
        \end{align}
        with equality if and only if $(y_1,\cdots,y_n)=0$.
    \end{itemize}
    Therefore, $(\cdot,\cdot)$ is an inner product on $\mathbb{C}^n$.
\end{sol}

\begin{exe}
    Show that any inner product $(\cdot,\cdot)$ is conjugate-linear in the first argument,
    \begin{align}
        \left(\sum_i\lambda_i\lvert w_i\rangle,\lvert v\rangle\right)=\sum_i\lambda_i^*(\lvert w_i\rangle,\lvert v\rangle).
    \end{align}
\end{exe}
\begin{sol}
    \begin{align}
        \left(\sum_i\lambda_i\lvert w_i\rangle,\lvert v\rangle\right)=\left(\lvert v\rangle,\sum_i\lambda_i\lvert w_i\rangle\right)^*=\left(\sum_i\lambda_i(\lvert v\rangle,\lvert w_i\rangle)\right)^*=\sum_i\lambda_i^*(\lvert w_i\rangle,\lvert v\rangle).
    \end{align}
    Therefore, any inner product $(\cdot,\cdot)$ is conjugate-linear in the first argument.
\end{sol}

\begin{exe}
    Verify that $\lvert w\rangle\equiv(1,1)$ and $\lvert v\rangle\equiv(1,-1)$ are orthogonal. What are the normalized forms of these vectors?
\end{exe}
\begin{sol}
    Since
    \begin{align}
        (\lvert w\rangle,\lvert v\rangle)=1\times 1+1\times(-1)=0,
    \end{align}
    these two vectors are orthogonal. The normalized form of $\lvert w\rangle$ and $\lvert v\rangle$ are
    \begin{align}
        \frac{\lvert w\rangle}{\norm{\lvert w\rangle}}=\frac{1}{\sqrt{2}}(1,1),
    \end{align}
    and
    \begin{align}
        \frac{\lvert v\rangle}{\norm{\lvert v\rangle}}=\frac{1}{\sqrt{2}}(1,-1),
    \end{align}
    respectively.
\end{sol}

\ifx\allfiles\undefined
\end{document}
\fi